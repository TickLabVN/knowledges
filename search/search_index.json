{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"Blog","text":""},{"location":"blog/git-workflow/","title":"Git workflow","text":"<p>For software development projects, the Git tool plays an important role in version management and supporting members to work effectively together. The following article is the theoretical and practical content shared at the training session and is also part of the software development process being applied at TickLab.</p>"},{"location":"blog/git-workflow/#the-main-branches","title":"The main branches","text":"<p>A repository must have at least these two main branches:</p> <ol> <li>main: where the source code is stable and has been released to production.</li> <li>staging: where the source code is latest, ready to be merged into the main branch for the next release.</li> </ol> <p>Note: All branches above are protected to prevent direct commit.</p> <p>Follow these steps:</p> <ol> <li>Clone a repository on github, example: https://github.com/ngyngcphu/mkdocs-tool</li> <li>Connect repository from local to remote:     <pre><code>git remote add origin &lt;remote_url&gt;\n</code></pre></li> <li>Push branch main up to remote:     <pre><code>git push origin main\n</code></pre></li> <li>Create branch staging from main:     <pre><code>git checkout -b staging main\n</code></pre></li> <li>Push branch develop up to remote:     <pre><code>git push origin staging\n</code></pre></li> <li>Set protection rules for 2 branches: main and staging at <code>Settings-&gt;Branches-&gt;Add branch protection rule</code>.</li> </ol> <p> </p> Branch main and branch staging"},{"location":"blog/git-workflow/#the-supporting-branches","title":"The supporting branches","text":"<p>Besides the main branches, there will be support branches so that team members can work in parallel, easily track features, prepare for release or quickly fix production and staging issues. These support branches will be deleted after using, including:</p> <ol> <li>Feature branches</li> <li>Hotfix branches</li> <li>Bugfix branches</li> <li>Test branches</li> <li>Release branches (Optional)</li> </ol> <p>Using the release branches may not be necessary. Instead, the TickFlow team used a tool that automatically generates releases based on conventional commits, that is release-please-action.</p>"},{"location":"blog/git-workflow/#conventional-commit-messages","title":"Conventional Commit Messages","text":""},{"location":"blog/git-workflow/#summary","title":"Summary","text":"<p>The commit message should be structured as follows: <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n</code></pre> The following structural elements:</p> <ol> <li>fix: a commit of the type <code>fix</code> patches a bug in your codebase.</li> <li>feat: a commit of the type <code>feat</code> introduces a new feature to the codebase</li> <li>types other than <code>fix:</code> and <code>feat:</code> are allowed, recommends <code>build:</code>, <code>chore:</code>, <code>ci:</code>, <code>docs:</code>, <code>style:</code>, <code>refactor:</code>, <code>perf:</code>, <code>test:</code>, and others (based on the Angular convention).</li> </ol> <p>A scope may be provided to a commit\u2019s type, to provide additional contextual information and is contained within parenthesis, e.g., <code>feat(parser): add ability to parse arrays</code>. For more detailed information, refer to Convention Commits.</p>"},{"location":"blog/git-workflow/#commitlint-tool","title":"Commitlint tool","text":"<p>Tool checks if your commit messages meet the conventional commit format, helps your team adhere to a commit convention.</p> <p>Follow these steps:</p> <ol> <li>Install:     <pre><code>yarn add -D @commitlint/{config-conventional,cli}\n</code></pre></li> <li>Configure:     <pre><code>echo \"module.exports = {extends: ['@commitlint/config-conventional']}\" &gt; commitlint.config.js\n</code></pre></li> <li>To lint commits before they are created you can use Husky's <code>commit-msg</code> hook:     <pre><code>yarn add -D husky\nyarn husky install\n</code></pre></li> <li>Add hook:     <pre><code>yarn husky add .husky/commit-msg  'npx --no -- commitlint --edit ${1}'\n</code></pre></li> <li>Test the hook:     </li> <li>Add script in <code>package.json</code> to automatically enable git hook after installing packages by <code>yarn</code>:     <pre><code>\"script\": {\n    \"prepare\": \"husky install\"\n},\n</code></pre></li> </ol>"},{"location":"blog/git-workflow/#feature-branches","title":"Feature branches","text":"<ul> <li>Branch off from: staging</li> <li>Merge back into: staging</li> <li>Branch naming convention: feature/**</li> </ul> <p>Feature branches are used to develop new features for the upcoming releases. Each feature will be a separate branch, created from the latest source code of staging, example: feature/project, feature/member,... After completing the features, features branch will be merged into staging and deleted.</p> <p>Follow these steps:</p> <ol> <li>Create a feature branch, ex: feature/part_1 from staging:     <pre><code>git checkout -b feature/part_1 staging\n</code></pre></li> <li>Make some changes to feature/part_1.</li> <li>Commit and push feature/part_1 up to remote:     <pre><code>git push origin feature/part_1\n</code></pre></li> </ol> <p> </p> Feature branches <p>After completing all the code in feature/part_1, make a Pull requests to staging. Then, add reviewers.</p> <p>Note: All conversations on code must be resolved before a pull request can be merged into a branch.</p> <p>After pull request has been merged into staging branch on remote. At local, checkout staging, delete feature/part_1 and pull latest code from remote. <pre><code>git checkout staging\ngit branch -D feature/part_1\ngit pull origin staging\n</code></pre></p> <p>Note: A pull request must not exceed 20 files changed. If the feature is too big (&gt; 20 files changed), it should be split into sub-branches, like that: sub-feature-part_1/search-project,...Then, make a pull request from sub-feature-part_1/search-project to feature/part_1, same as above.</p>"},{"location":"blog/git-workflow/#hotfix-and-bugfix-branches","title":"Hotfix and bugfix branches","text":""},{"location":"blog/git-workflow/#hotfix","title":"Hotfix","text":"<ul> <li>Branch off from: main</li> <li>Merge back into: main and staging</li> <li>Branch naming convention: hotfix/**</li> </ul> Hotfix branch Branches structure after merging hotfix"},{"location":"blog/git-workflow/#bugfix","title":"Bugfix","text":"<ul> <li>Branch off from: staging</li> <li>Merge back into: staging</li> <li>Branch naming convention: bugfix/**</li> </ul> Bugfix branch Branches structure after merging bugfix <p>Hotfix and bugfix branches are utilized to address issues that arise in a system. When a problem occurs, we gather information about how it happened (reproduction case) and promptly take corrective action to restore the system's functionality. This quick fix is applied in the hotfix branch. Following this, we investigate the underlying cause of the problem and take more comprehensive measures to prevent similar issues from occurring in the future. These longer-term solutions are implemented in the bugfix branch.</p>"},{"location":"blog/git-workflow/#test-branches","title":"Test branches","text":"<ul> <li>Branch off from: any branch</li> <li>Merge back into: none</li> <li>Branch naming convention: test/**</li> </ul> <p>If you come up with an idea, like a new method for coding or want to add a new tool to the project, you can try it out in a test branch. This lets you test the idea with safety and separation, reducing the potential impact on the main project.</p>"},{"location":"blog/git-workflow/#release","title":"Release","text":""},{"location":"blog/git-workflow/#github-action-workflows","title":"Github Action Workflows","text":"<p>A workflow is a configurable automated process that will run one or more jobs. Workflows are defined by a YAML file in the <code>.github/workflows</code> directory of your repository, can be triggered by events like <code>push</code>, <code>pull-requests</code> or on a <code>schedule</code>.</p> <p>Release workflow is defined in file <code>release.yml</code>.</p>"},{"location":"blog/git-workflow/#release-please-action-tool","title":"Release-please-action tool","text":"<p>A tool auto-release with Conventional Commit Message. </p> <p>Set up this action in <code>.github/workflows/release.yml</code>: <pre><code>release:\n    runs-on: ubuntu-latest\n    outputs:\n      build: ${{ steps.release.outputs.release_created }}\n      tag_name: ${{ steps.release.outputs.tag_name }}\n    steps:\n      - uses: google-github-actions/release-please-action@v3\n        id: release\n        with:\n          release-type: node\n          pull-request-header: 'Bot (:robot:) requested to create a new release on ${{ github.ref_name }}'\n</code></pre> Also, let's create a <code>build</code> job to build a package on github container registry (ghcr). This package is a docker image used to run docker-compose.</p> <p>Note: The release pull request generated by the github-action bot is not allowed to merged into <code>main</code> until the product is fully featured.</p>"},{"location":"blog/git-workflow/#github-project","title":"Github Project","text":"<p>GitHub is a platform for software developers to collaborate and manage not just their code but the overall project planning and task-tracking process through a feature called \u201cGitHub Project\u201d. A project provides various views: spreadsheet, task board, or roadmap integrated with your issues and pull requests, which help your management process always stay up-to-date with the developing process.</p>"},{"location":"blog/git-workflow/#setting-up-projects","title":"Setting up projects","text":"<ol> <li>We can navigate to the \u201cProjects\u201d tab from the header.  </li> <li>Create your first project by clicking on the \u201cNew project\u201d button.</li> <li>You can either \u201cStart from scratch\u201d or use the provided \u201cProject template\u201d (highly recommended) to get a grasp of this feature. </li> <li>Play around with different types of views that your team would be more comfortable with.</li> </ol>"},{"location":"blog/git-workflow/#creating-and-managing-tasklist","title":"Creating and managing tasklist","text":""},{"location":"blog/git-workflow/#getting-familiar-with-issues","title":"Getting familiar with issues","text":"<ol> <li> <p>A task before converting into issues is just a draft task that doesn\u2019t belong to any repository.  </p> </li> <li> <p>After converting them into issues, we would be able to assign tasks to team members and relate them to pull requests and that issue will reside inside a repository. </p> </li> <li> <p>Each issue has an ID. If you mention an issue in the description section of another issue, it then can be understood as a subtask. </p> </li> <li> <p>Use the checklist symbol before mentioning an issue ID so that it can display similar to the above. </p> </li> </ol>"},{"location":"blog/git-workflow/#manage-issue-via-cli","title":"Manage issue via CLI","text":"<p>GitHub also offers issues and project management directly from CLI. - Installation: GitHub CLI. - Command guideline: GitHub Issues || GitHub Project</p>"},{"location":"blog/git-workflow/#add-detailed-information-to-your-tasks","title":"Add detailed information to your tasks","text":"<p>GitHub provides various data types to add more information to your task:</p> <ul> <li>A date field to track target ship dates.</li> <li>A number field to track the complexity of a task.</li> <li>A single select field to track whether a task is a Low, Medium, or High priority.</li> <li>A text field to add a quick note.</li> <li>An iteration field to plan work week-by-week, including support for breaks. </li> </ul> <p>After converting into issues, you can also add repository information into your task:</p> <ul> <li>A member of your team to handle the task</li> <li>A milestone of your project</li> <li>A pull request </li> </ul> <p>Limitation: GitHub free plan would limit some features if you make your repository    private, some might be: - Can only assign 1 member to an issue - Can't track both private and public repository in 1 project</p>"},{"location":"blog/git-workflow-in-practice/","title":"Git workflow in practice","text":"<p>This is the sample project for Git workflow article. If you haven't already read it, it's recommended to give it a glance before attempting this project. Upon completion of this project, you will:</p> <ul> <li>Gain a clearer understanding of how Git workflow operates in practice.</li> <li>Be exposed to a simple debugging process.</li> </ul>"},{"location":"blog/git-workflow-in-practice/#setup-project","title":"Setup project","text":"<p>First, make sure you already have Git installed on your computer and a GitHub account. To check git is installed:</p> <pre><code>git --version\n</code></pre> <p>If this is the first time you use git, you need to set a git username and git email, you can remove the \u2014global flag if you want to set a user name and user email for a specific repository:</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre> <p>Check you already set the git user name and user email:</p> <pre><code>git config --global user.name\ngit config --global user.email\n</code></pre> <p>You need to create a repository in GitHub, let's give it a git-workflow-example name.</p> <p></p> <p>Tick at the \u201cAdd a README file\u201d option. Let all remaining setting is default then click the \"Create repository\" button.</p> <p></p> <p>Save the link to the repository, you will need it later, with this tutorial, we will use HTTPS.</p> <p>In the upper left, click on the branch button.</p> <p>Click on the View all branches.</p> <p>Click New branch, naming branch is staging, source is main.</p> <p>Go to Settings \u2192 Branches \u2192 Add rule.</p> <p>Create two sets of rules for main and staging, with the following rule chosen:</p> <ul> <li>Require a pull request before merging.</li> <li>Require status checks to pass before merging.<ul> <li>Require branches to be up to date before merging. </li> </ul> </li> </ul> <p>These rules make sure our project follows the git workflow.</p> <p>Open the terminal in your computer, clone the repository you have just created then change the directory to it:</p> <pre><code>git clone https://github.com/your-username/git-workflow-example.git\ncd git-workflow-example\n</code></pre>"},{"location":"blog/git-workflow-in-practice/#create-new-feature","title":"Create new feature","text":""},{"location":"blog/git-workflow-in-practice/#add-hello-world-line","title":"Add hello world line","text":"<p>In this example, the image you are given the task of adding \u201cHello World!\u201d to the README.md file</p> <p>Before starting, we need to have the staging branch in the local repository:</p> <pre><code>git switch staging\n</code></pre> <p>First, we need to create a new branch for this feature:</p> <pre><code>git checkout -b feature/adding-hello-world-line staging\n</code></pre> <p>Add a line to README.md:</p> <pre><code>printf \"\\n\" &gt;&gt; README.md\necho \"Hello World!\" &gt;&gt; README.md\n</code></pre> <p>Commit the current workspace to the local repository:</p> <pre><code>git add .\ngit commit -m \"feat(hello-world): add hello world file\"\n</code></pre> <p>Create a new branch and push your change to GitHub:</p> <pre><code>git push origin -u feature/adding-hello-world-line\n</code></pre> <p>This may ask you for credentials if this is the first time you are using Git, you can follow this to create a GitHub personal access token to identify.</p> <p>Notice: GitHub removed Support for password authentication on August 13, 2021.</p> <p>Before identifying, you can run this to avoid entering credentials multiple times:</p> <pre><code>git config credential.helper store\n</code></pre> <p>Go to your project in GitHub and create a Pull Request. Changing base branch to staging.</p> <p></p> <p>In practice, you will receive some reviews from others. For now, merge the new branch to staging then delete it.</p> <p></p>"},{"location":"blog/git-workflow-in-practice/#add-release-github-action","title":"Add release GitHub action","text":"<p>This time, you are asked to add GitHub action to release a new version whenever the staging branch is merged to the main branch.</p> <p>If you want to know more about GitHub action, try visiting this.</p> <p>Create a new feature branch:</p> <pre><code>git checkout staging\ngit pull\ngit checkout -b feature/add-release-action\n</code></pre> <p>The branch feature/adding-hello-world-line has been merged to staging, so you can delete it:</p> <pre><code>git branch -d feature/adding-hello-world-line\n</code></pre> <p>To create a GitHub action, first let\u2019s create a .github folder, create a workflows folder in .github:</p> <pre><code>mkdir .github &amp;&amp; cd $_\nmkdir workflows &amp;&amp; cd $_\n</code></pre> <p>In the workflows folder, create a file name release.yml and give the file permission to edit content:</p> <pre><code>touch release.yml\nchmod +w release.yml\n</code></pre> <p>You can use any editor you want, this tutorial will use vim to change file content.</p> <p>Open file with Vim:</p> <pre><code>vim release.yml\n</code></pre> <p>Paste the content below to release.yml:</p> <pre><code>name: release\non:\n  push:\n    branches:\n      - main\n    paths-ignore:\n      - '*.md'\nenv:\n  REGISTRY: ghcr.io\n  ORG_USERNAME: ${{ github.actor }}\n\npermissions:\n  contents: write\n  pull-requests: write\n  packages: write\n\njobs:\n  release:\n    runs-on: ubuntu-latest\n    outputs:\n      build: ${{ steps.release.outputs.release_created }}\n      tag_name: ${{ steps.release.outputs.tag_name }}\n    steps:\n      - uses: google-github-actions/release-please-action@v3\n        id: release\n        with:\n          release-type: simple\n          pull-request-header: 'Bot (:robot:) requested to create a new release on ${{ github.ref_name }}'\n</code></pre> <p>Entering :wq to save and close the file.</p> <p>Commit the change and push the new feature branch to the remote repo:</p> <pre><code>git add .\ngit commit -m \"feat(github action): implement github action for release\"\ngit push origin -u feature/add-release-action\n</code></pre> <p>Go to GitHub, create and merge a pull request of the new branch to staging.</p> <p></p> <p>To GitHub action can release a new version in your repository, you need to allow action can create and approve pull requests.</p> <p>Go to the settings of your repository. </p> <p>Go to Action &gt; General, and tick \u201cAllow GitHub Actions can create pull requests or submit approving pull request reviews\u201d at the end of the page. Save the setting.</p> <p></p> <p>Okay, now GitHub action will auto-release a new version of our app whenever any branch merges with the main branch.</p>"},{"location":"blog/git-workflow-in-practice/#release-new-version","title":"Release new version","text":"<p>It may need more features, and more processes to merge code from staging to main and release a new version. In this tutorial, simply create a pull request and merge staging to the main.</p> <p></p> <p>The GitHub action will create new pull request with information about our new release, and merge it to the main.</p> <p></p> <p>You have just finished creating a new version using the git workflow. The CHANGELOG.md file contains information on our releases.</p> <p></p>"},{"location":"blog/git-workflow-in-practice/#hotfix-and-bugfix","title":"Hotfix and Bugfix","text":""},{"location":"blog/git-workflow-in-practice/#about-the-problem","title":"About the problem","text":"<p>Image your co-worker\u2019s release a new version containing a script to automate adding a new line to the content of the README.md file.</p> <p>The expected render result is.</p> <p></p> <p>Let's see the repository.</p> <p></p> <p>Oop! The rendered result is different from what we expected.</p>"},{"location":"blog/git-workflow-in-practice/#fork-repository","title":"Fork repository","text":"<p>In this section, we will address this bug using a Git workflow. Since we don't have permission to directly modify this repository, we will fork it, make the necessary changes, push them, and create pull requests.</p> <p>Click the \u201cFork\u201d button in the upper right, and give it a new name git-workflow-sample to distinguish the one you created above. Let all remaining setting is the default.</p> <p>Clone the new repository we have just forked to make some changes in that code:</p> <pre><code>git clone https://github.com/your-user-name/git-workflow-sample.git\ncd git-workflow-sample\n</code></pre>"},{"location":"blog/git-workflow-in-practice/#hotfix-branch","title":"Hotfix branch","text":"<p>The first thing we need to do when our application exhibits incorrect behavior is to resolve the issue as quickly as possible, ensuring that users can continue using our service.</p> <p>See the CHANGELOG.md to know more details about your co-worker\u2019s release.</p> <p></p> <p>We will create a new hotfix branch, reverting back to version 1.0.0, to ensure that users can continue using our service before your co-worker's script is added.</p> <pre><code>git checkout -b 'hotfix/back-to-before-newline-script-is-added' main\n</code></pre> <p>See the history of commits:</p> <pre><code>git log\n</code></pre> <p></p> <p>We will revert the code to version 1.0.0 is released:</p> <pre><code>git revert -m 1 --no-commit 9cb11c..HEAD\n</code></pre> <p>Let's see the content of README.md:</p> <pre><code>cat README.md\n</code></pre> <p>Continue revert the code:</p> <pre><code>git revert --continue\n</code></pre> <p>An editor will show, by default, it will be nano, press Ctrl + O \u2192 Enter \u2192 Ctrl + X to save and out of the editor.</p> <p>Push our change to the remote repository and create a new pull request:</p> <pre><code>git push -u origin hotfix/back-to-before-newline-script-is-added\n</code></pre> <p>This will create a hotfix/back-to-before-newline-script-is-added branch in your remote repository and push your code to this branch.</p> <p>Create pull requests to the main and staging branches. Our user can continue using our service after your pull request is approved and merged.</p> <p></p>"},{"location":"blog/git-workflow-in-practice/#bugfix-branch","title":"Bugfix branch","text":"<p>After making a hotfix to keep our service still working, we need to make deep remediation to root the problem and avoid it happening again in the future.</p> <p>It seems the newline script is overwritten instead of appending the newline to it, let's see the script in detail and fix the problem.</p> <p>The image of the hotfix branch you have just created is approved and merged with the main and staging branch. Following the git workflow, you will create a bug fix branch from the staging branch, but for now, we make a new bugfix branch from the hotfix branch.</p> <p>Make sure you are currently in the hotfix branch:</p> <pre><code>git checkout 'hotfix/back-to-before-newline-script-is-added'\n</code></pre> <p>Create a new bugfix branch:</p> <pre><code>git checkout -b 'bugfix/let-newline-script-append-newline-to-README.md-file'\n</code></pre> <p>Revert branch to the script is done by your co-worker:</p> <pre><code>git revert --no-commit 4c5be7..HEAD\n</code></pre> <p>First, recover the content of README.md:</p> <pre><code>echo \"# git-workflow\n\nHello World!\" &gt; README.md\n</code></pre> <p>See the current content of README.md:</p> <pre><code>cat README.md\n</code></pre> <p></p> <p>Then see the content of newline.sh:</p> <pre><code>cat newline.sh\n</code></pre> <p></p> <p>In the \"append the new text to the file\" section, your co-worker used the '&gt;' operator, which overwrites the content of the file. Try changing it to the '&gt;&gt;' operator to see if it resolves the problem.</p> <p>Give newline.sh file permission to write and execute:</p> <pre><code>chmod +x+w newline.sh\n</code></pre> <p>Open file with Vim:</p> <pre><code>vim newline.sh\n</code></pre> <p>Enter i to access insert mode, change &gt; to &gt;&gt; operator</p> <p>Press the \u201cEsc\u201d then type :wq to save the change, and close the file.</p> <p>Execute the script and see if it works as we expect:</p> <pre><code>./newline.sh\n</code></pre> <p>See content of README.md:</p> <pre><code>cat README.md\n</code></pre> <p></p> <p>Nice, now the script works correctly.</p> <p>Commit the change, and push it to the remote repository.</p> <pre><code>git add .\ngit commit -m 'fix(newline.sh): replace &gt; by &gt;&gt; to let newline is append to exist README.md content'\ngit push -u origin bugfix/let-newline-script-append-newline-to-README.md-file\n</code></pre> <p>Create a pull request to merge your change to the staging branch</p> <p></p> <p>Good job! You have just completely debugged a problem with the git workflow.</p>"},{"location":"blog/set-up-front-end-development-environment-part-1/","title":"Set up Front-end development environment (Part 1)","text":"<p>B\u00e0i vi\u1ebft n\u00e0y d\u1ef1a tr\u00ean nh\u1eefng kinh nghi\u1ec7m m\u00e0 m\u00ecnh h\u1ecdc \u0111\u01b0\u1ee3c khi tham gia v\u00e0o m\u1ed9t d\u1ef1 \u00e1n t\u1ea1i TickLab v\u1edbi vai tr\u00f2 l\u00e0 Front-end developer. N\u1ed9i dung chia s\u1ebb bao g\u1ed3m vi\u1ec7c c\u00e0i \u0111\u1eb7t, c\u1ea5u h\u00ecnh m\u00f4i tr\u01b0\u1eddng ph\u00e1t tri\u1ec3n giao di\u1ec7n ng\u01b0\u1eddi d\u00f9ng tr\u00ean n\u1ec1n t\u1ea3ng web t\u1eeb l\u00fac project b\u1eaft \u0111\u1ea7u v\u00e0o giai \u0111o\u1ea1n hi\u1ec7n th\u1ef1c v\u00e0 c\u00e1ch l\u00e0m vi\u1ec7c nh\u00f3m hi\u1ec7u qu\u1ea3 gi\u1eefa c\u00e1c th\u00e0nh vi\u00ean trong team \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o ch\u1ea5t l\u01b0\u1ee3ng m\u00e3 ngu\u1ed3n. B\u1ea1n \u0111\u1ecdc t\u1ea1o issue tr\u00ean github repository c\u1ee7a m\u00ecnh n\u1ebfu c\u00f3 b\u1ea5t c\u1ee9 th\u1eafc m\u1eafc hay g\u00f3p \u00fd g\u00ec \u0111\u1ebfn b\u00e0i vi\u1ebft nh\u00e9.</p>"},{"location":"blog/set-up-front-end-development-environment-part-1/#1-so-luoc-ve-cac-cong-cu-su-dung","title":"1. S\u01a1 l\u01b0\u1ee3c v\u1ec1 c\u00e1c c\u00f4ng c\u1ee5 s\u1eed d\u1ee5ng","text":"<ul> <li>ReactJS l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n JavaScript, \u0111\u01b0\u1ee3c d\u00f9ng \u0111\u1ec3 x\u00e2y d\u1ef1ng giao di\u1ec7n ng\u01b0\u1eddi d\u00f9ng linh ho\u1ea1t v\u00e0 d\u1ec5 t\u00e1i s\u1eed d\u1ee5ng (https://react.dev/).</li> <li>TailwindCSS l\u00e0 m\u1ed9t framework CSS m\u1ea1nh m\u1ebd cho ph\u00e9p x\u00e2y d\u1ef1ng giao di\u1ec7n nhanh ch\u00f3ng v\u00e0 d\u1ec5 d\u00e0ng (https://tailwindcss.com/).</li> <li>Vite l\u00e0 m\u1ed9t front-end build-tool nhanh v\u00e0 hi\u1ec7u qu\u1ea3 gi\u00fap t\u0103ng t\u1ed1c quy tr\u00ecnh ph\u00e1t tri\u1ec3n (https://vitejs.dev/).</li> <li>Zustand l\u00e0 m\u1ed9t th\u01b0 vi\u1ec7n qu\u1ea3n l\u00fd tr\u1ea1ng th\u00e1i cho ReactJS, gi\u00fap vi\u1ec7c qu\u1ea3n l\u00fd tr\u1ea1ng th\u00e1i \u1ee9ng d\u1ee5ng m\u1ed9t c\u00e1ch \u0111\u01a1n gi\u1ea3n v\u00e0 d\u1ec5 d\u00e0ng (https://zustand-demo.pmnd.rs/).</li> <li>TypeScript l\u00e0 m\u1ed9t ng\u00f4n ng\u1eef l\u1eadp tr\u00ecnh ph\u1ed5 bi\u1ebfn, l\u00e0 m\u1ed9t phi\u00ean b\u1ea3n m\u1edf r\u1ed9ng c\u1ee7a JavaScript v\u1edbi h\u1ed7 tr\u1ee3 ki\u1ec3u d\u1eef li\u1ec7u t\u0129nh, gi\u00fap gi\u1ea3m l\u1ed7i v\u00e0 n\u00e2ng cao hi\u1ec7u su\u1ea5t ph\u00e1t tri\u1ec3n (https://www.typescriptlang.org/).</li> <li>ESLint, Prettier, Lint-staged, Husky: B\u1ed9 c\u00f4ng c\u1ee5 ki\u1ec3m tra v\u00e0 duy tr\u00ec ch\u1ea5t l\u01b0\u1ee3ng m\u00e3 ngu\u1ed3n trong qu\u00e1 tr\u00ecnh ph\u00e1t tri\u1ec3n ph\u1ea7n m\u1ec1m. C\u00e1c c\u00f4ng c\u1ee5 n\u00e0y gi\u00fap \u0111\u1ea3m b\u1ea3o m\u00e3 ngu\u1ed3n tu\u00e2n th\u1ee7 c\u00e1c quy t\u1eafc v\u00e0 ti\u00eau chu\u1ea9n vi\u1ebft m\u00e3, t\u1ea1o ra m\u00e3 d\u1ec5 \u0111\u1ecdc, d\u1ec5 b\u1ea3o tr\u00ec v\u00e0 d\u1ec5 c\u1ed9ng t\u00e1c.</li> </ul>"},{"location":"blog/set-up-front-end-development-environment-part-1/#2-bat-au-voi-mot-vi-du-on-gian","title":"2. B\u1eaft \u0111\u1ea7u v\u1edbi m\u1ed9t v\u00ed d\u1ee5 \u0111\u01a1n gi\u1ea3n","text":"<p>M\u1ee5c ti\u00eau c\u1ee7a b\u00e0i vi\u1ebft l\u00e0 gi\u1edbi thi\u1ec7u c\u00e1ch c\u00e0i \u0111\u1eb7t m\u00f4i tr\u01b0\u1eddng ph\u00e1t tri\u1ec3n giao di\u1ec7n ng\u01b0\u1eddi d\u00f9ng, kh\u00f4ng \u0111i s\u00e2u v\u00e0o vi\u1ec7c hi\u1ec7n th\u1ef1c s\u1ea3n ph\u1ea9m. C\u00e1c b\u1ea1n c\u00f3 th\u1ec3 t\u1ef1 m\u00ecnh ho\u00e0n thi\u1ec7n project nh\u1ecf n\u00e0y d\u1ef1a tr\u00ean template m\u00e0 ch\u00fang ta s\u1ebd x\u00e2y d\u1ef1ng.</p> <p>Gi\u1ea3 s\u1eed b\u1ea1n c\u00f3 m\u1ed9t y\u00eau c\u1ea7u x\u00e2y d\u1ef1ng giao di\u1ec7n c\u1ee7a m\u1ed9t \u1ee9ng d\u1ee5ng qu\u1ea3n l\u00fd tr\u00ean n\u1ec1n t\u1ea3ng web hi\u1ec3n th\u1ecb c\u00e1c project \u0111\u00e3 v\u00e0 \u0111ang \u0111\u01b0\u1ee3c l\u00e0m trong m\u1ed9t c\u00f4ng ty. Sau khi ph\u00e2n t\u00edch nghi\u1ec7p v\u1ee5, c\u00f3 \u0111\u01b0\u1ee3c c\u00e1c requirement nh\u01b0 sau:</p> <ul> <li>\u1ee8ng d\u1ee5ng g\u1ed3m 3 trang: trang \u0111\u0103ng nh\u1eadp, trang hi\u1ec3n th\u1ecb t\u1ed5ng quan t\u1ea5t c\u1ea3 c\u00e1c project v\u00e0 trang hi\u1ec3n th\u1ecb th\u00f4ng tin chi ti\u1ebft c\u1ee7a m\u1ed7i project.</li> <li>\u1ee8ng d\u1ee5ng ph\u1ea3i bao g\u1ed3m 4 thao t\u00e1c c\u01a1 b\u1ea3n: t\u1ea1o - xem - s\u1eeda - x\u00f3a.</li> <li>\u1ee8ng d\u1ee5ng ph\u1ea3i t\u01b0\u01a1ng th\u00edch tr\u00ean c\u00e1c m\u00e0n h\u00ecnh desktop, mobile.</li> </ul> <p>Ti\u1ebfp theo, b\u1ea1n c\u1ea7n t\u1ea1o m\u00f4i tr\u01b0\u1eddng ph\u00e1t tri\u1ec3n cho c\u00e1c developers trong team. \u1ede \u0111\u00e2y m\u00ecnh ch\u1ec9 gi\u1edbi thi\u1ec7u c\u00e1ch c\u00e0i \u0111\u1eb7t c\u00e1c c\u1ea5u h\u00ecnh c\u1ea7n thi\u1ebft v\u00e0 t\u1ea1o c\u1ea5u tr\u00fac th\u01b0 m\u1ee5c cho vi\u1ec7c ph\u00e1t tri\u1ec3n Front-end, c\u00e1c API t\u1eeb Back-end s\u1ebd \u0111\u01b0\u1ee3c mock b\u1eb1ng c\u00e1ch \u0111\u01a1n gi\u1ea3n tr\u1ea3 v\u1ec1 chu\u1ed7i string m\u00e0 kh\u00f4ng hi\u1ec7n th\u1ef1c chi ti\u1ebft. </p>"},{"location":"blog/set-up-front-end-development-environment-part-1/#3-cai-at-va-cau-hinh-moi-truong-phat-trien","title":"3. C\u00e0i \u0111\u1eb7t v\u00e0 c\u1ea5u h\u00ecnh m\u00f4i tr\u01b0\u1eddng ph\u00e1t tri\u1ec3n","text":""},{"location":"blog/set-up-front-end-development-environment-part-1/#31-cai-at","title":"3.1. C\u00e0i \u0111\u1eb7t","text":"<ul> <li> <p>V\u00ec ch\u00fang ta \u0111ang l\u00e0m vi\u1ec7c v\u1edbi Typescript n\u00ean c\u1ea7n ph\u1ea3i c\u00f3 m\u1ed9t runtime environment v\u00e0 m\u1ed9t package manager. M\u00ecnh ch\u1ecdn 2 c\u00f4ng c\u1ee5 ph\u1ed5 bi\u1ebfn nh\u1ea5t l\u00e0 node.js v\u00e0 yarn. C\u00e1c b\u1ea1n v\u00e0o trang ch\u1ee7 c\u1ee7a node.js (https://nodejs.org) \u0111\u1ec3 c\u00e0i \u0111\u1eb7t node, m\u1ed9t tr\u00ecnh qu\u1ea3n l\u00fd g\u00f3i npm c\u0169ng \u0111\u01b0\u1ee3c c\u00e0i k\u00e8m v\u1edbi node. B\u1ea1n c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng lu\u00f4n npm \u0111\u1ec3 thay th\u1ebf cho yarn nh\u01b0ng yarn s\u1eed d\u1ee5ng cache hi\u1ec7u qu\u1ea3 h\u01a1n \u0111\u1ec3 l\u01b0u tr\u1eef c\u00e1c g\u00f3i \u0111\u00e3 t\u1ea3i, gi\u00fap ti\u1ebft ki\u1ec7m th\u1eddi gian v\u00e0 b\u0103ng th\u00f4ng khi c\u00e0i \u0111\u1eb7t l\u1ea1i. C\u00e0i \u0111\u1eb7t yarn b\u1eb1ng command line:</p> <pre><code>npm install -g yarn\n</code></pre> </li> <li> <p>Ki\u1ec3m tra l\u1ea1i c\u00e1c c\u00f4ng c\u1ee5 \u0111\u00e3 c\u00e0i \u0111\u1eb7t:</p> <p></p> </li> <li> <p>Ti\u1ebfp theo, ch\u00fang ta c\u1ea7n m\u1ed9t base template t\u1eeb Vite, s\u1eed d\u1ee5ng command line sau:</p> <pre><code>yarn create vite react-template --template react-ts\n</code></pre> <p>Ta s\u1ebd c\u00f3 m\u1ed9t th\u01b0 m\u1ee5c react-template, x\u00f3a c\u00e1c file: App.css, .eslintrc.cjs. C\u1ea5u tr\u00fac th\u01b0 m\u1ee5c nh\u01b0 sau:</p> <p></p> </li> </ul>"},{"location":"blog/set-up-front-end-development-environment-part-1/#32-cau-hinh","title":"3.2. C\u1ea5u h\u00ecnh","text":"<p>Sau m\u1ed7i b\u01b0\u1edbc c\u1ea5u h\u00ecnh, c\u00e1c b\u1ea1n c\u00f3 th\u1ec3 xem l\u1ea1i c\u1ea5u tr\u00fac th\u01b0 m\u1ee5c t\u01b0\u01a1ng \u1ee9ng v\u1edbi t\u1eebng commit c\u1ee7a m\u00ecnh tr\u00ean github nh\u00e9.</p> <ol> <li> <p>C\u1ea5u h\u00ecnh CSS framework</p> <p>Ch\u00fang ta s\u1eed d\u1ee5ng TailwindCSS cho project. Tailwind \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf theo nguy\u00ean t\u1eafc utility-first, bao g\u1ed3m m\u1ed9t t\u1eadp h\u1ee3p c\u00e1c class element s\u1eb5n c\u00f3 gi\u00fap cho vi\u1ec7c ph\u00e1t tri\u1ec3n front-end nhanh v\u00e0 linh ho\u1ea1t. S\u1eed d\u1ee5ng Tailwind m\u1ed9t c\u00e1ch kh\u00e9o l\u00e9o, th\u1eadm ch\u00ed kh\u00f4ng c\u1ea7n ph\u1ea3i vi\u1ebft m\u1ed9t file css thu\u1ea7n n\u00e0o trong project c\u1ea3, c\u00e1c b\u01b0\u1edbc c\u00e0i \u0111\u1eb7t nh\u01b0 sau:</p> <ul> <li> <p>Install tailwindcss qua yarn v\u00e0 t\u1ea1o m\u1ed9t <code>tailwind.config.js</code> file (l\u01b0u \u00fd n\u00ean c\u00e0i tailwindcss \u1edf devDependencies c\u1ee7a project v\u00ec c\u00e1c class element trong Tailwind s\u1ebd \u0111\u01b0\u1ee3c generate th\u00e0nh c\u00e1c file CSS t\u0129nh tr\u01b0\u1edbc khi ch\u1ea1y tr\u00ean tr\u00ecnh duy\u1ec7t):</p> <pre><code>yarn add -D tailwindcss\nyarn tailwindcss init\n</code></pre> </li> <li> <p>Th\u00eam \u0111\u01b0\u1eddng d\u1eabn \u0111\u1ebfn c\u00e1c file s\u1eed d\u1ee5ng tailwind trong <code>tailwind.config.js</code> :</p> <pre><code>/** @type {import('tailwindcss').Config} */\nexport default {\n  content: ['./index.html', './src/**/*.{js,ts,jsx,tsx}'],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n}\n</code></pre> </li> <li> <p>Th\u00eam c\u00e1c ch\u1ec9 th\u1ecb <code>@tailwind</code> cho t\u1eebng layer c\u1ee7a Tailwind v\u00e0o t\u1ec7p <code>index.css</code> \u0111\u1ec3 k\u00edch ho\u1ea1t v\u00e0 s\u1eed d\u1ee5ng c\u00e1c l\u1edbp CSS \u0111\u00e3 \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a s\u1eb5n trong Tailwind CSS framework. C\u1ee5 th\u1ec3, c\u00f3 ba l\u1edbp ch\u00ednh c\u1ea7n th\u00eam ch\u1ec9 th\u1ecb <code>@tailwind</code>, tham kh\u1ea3o t\u1ea1i \u0111\u00e2y, l\u01b0u \u00fd x\u00f3a to\u00e0n b\u1ed9 n\u1ed9i dung c\u0169 trong file <code>index.css</code> v\u00e0 ch\u1ec9 c\u1ea7n th\u00eam 3 ch\u1ec9 th\u1ecb tailwind:</p> <pre><code>@tailwind base;\n@tailwind components;\n@tailwind utilities;\n</code></pre> </li> <li> <p>Th\u1eed generate file CSS t\u0129nh t\u1eeb 3 ch\u1ec9 th\u1ecb trong file <code>index.css</code>:</p> <pre><code>yarn tailwindcss -i ./src/index.css -o ./dist/output.css\n</code></pre> <p>Ta s\u1ebd th\u1ea5y m\u1ed9t folder <code>dist</code> xu\u1ea5t hi\u1ec7n ch\u1ee9a file <code>output.css</code> l\u00e0 m\u1ed9t file CSS t\u0129nh \u0111\u01b0\u1ee3c generate t\u1eeb tailwind. Ta import <code>dist/output.css</code> v\u00e0o file App.tsx, \u0111\u1ed3ng th\u1eddi x\u00f3a to\u00e0n b\u1ed9 n\u1ed9i dung c\u0169 v\u00e0 thay b\u1eb1ng \u0111o\u1ea1n code \u0111\u01a1n gi\u1ea3n sau:</p> <pre><code>import '../dist/output.css';\nfunction App() {\n  return (\n    &lt;h1 className=\"text-3xl font-bold underline\"&gt;\n      Hello world!\n    &lt;/h1&gt;\n  );\n}\n\nexport default App;\n</code></pre> <p>Th\u1ef1c thi command line <code>yarn dev</code>, m\u1eb7c \u0111\u1ecbnh yarn s\u1eed d\u1ee5ng port 5173 \u0111\u1ec3 host website c\u1ee7a ch\u00fang ta \u1edf local, m\u1edf tr\u00ecnh duy\u1ec7t web t\u1ea1i \u0111\u1ecba ch\u1ec9 <code>localhost:5173</code> v\u00e0 \u0111\u00e2y l\u00e0 k\u1ebft qu\u1ea3:</p> <p></p> <p>Ta nh\u1eadn th\u1ea5y qu\u00e1 tr\u00ecnh t\u1eeb l\u00fac vi\u1ebft c\u00e1c class tailwindcss \u0111\u1ebfn l\u00fac render ra \u0111\u01b0\u1ee3c UI kh\u00e1 th\u1ee7 c\u00f4ng: ph\u1ea3i generate ra file CSS t\u0129nh, sau \u0111\u00f3 import n\u00f3 v\u00e0o t\u1eebng file code c\u00f3 s\u1eed d\u1ee5ng tailwindcss. N\u1ebfu project ch\u00fang ta c\u00f3 t\u1ea7m 50 file code <code>.tsx</code> c\u00f3 s\u1eed d\u1ee5ng tailwindcss, ta ph\u1ea3i import 50 l\u1ea7n d\u00f2ng <code>import '../dist/output.tsx'</code>, nh\u01b0ng kh\u00f4ng ph\u1ea3i file n\u00e0o c\u0169ng d\u00f9ng to\u00e0n b\u1ed9 c\u00e1c class \u0111\u00e3 \u0111\u01b0\u1ee3c generate, \u0111i\u1ec1u n\u00e0y l\u00e0m ch\u1eadm qu\u00e1 tr\u00ecnh t\u1ea3i trang v\u00e0 t\u1ea1o ra c\u00e1c m\u00e3 CSS kh\u00f4ng c\u1ea7n thi\u1ebft. Th\u00eam v\u00e0o \u0111\u00f3, c\u00e1c m\u00e3 CSS do tailwindcss sinh ra c\u00f3 th\u1ec3 kh\u00f4ng t\u01b0\u01a1ng th\u00edch v\u1edbi m\u1ed9t v\u00e0i tr\u00ecnh duy\u1ec7t c\u0169 nh\u01b0 Safari, Internet Explorer,\u2026Ch\u00ednh v\u00ec v\u1eady, ta c\u1ea7n ph\u1ea3i s\u1eed d\u1ee5ng c\u00e1c CSS processor \u0111\u1ec3 t\u1ef1 \u0111\u1ed9ng v\u00e0 t\u1ed1i \u01b0u h\u00f3a qu\u00e1 tr\u00ecnh n\u00e0y, hai CSS processor \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0 postcss v\u00e0 autoprefixer. C\u00e0i \u0111\u1eb7t ch\u00fang \u1edf devDependencies :</p> <pre><code>yarn add -D postcss autoprefixer\n</code></pre> </li> <li> <p>Th\u00eam file <code>postcss.config.js</code> t\u1ea1i root folder v\u1edbi n\u1ed9i dung nh\u01b0 sau:</p> <pre><code>export default {\n  plugins: {\n    tailwindcss: {},\n    autoprefixer: {},\n  },\n}\n</code></pre> <p>Ta \u0111\u00e3 c\u1ea5u h\u00ecnh taiwindcss v\u00e0 autoprefixer l\u00e0 c\u00e1c plugin c\u1ee7a postcss, th\u00eam class <code>text-red-500</code> v\u00e0o \u0111o\u1ea1n code Hello world! \u1edf tr\u00ean, ch\u1ea1y l\u1ea1i l\u1ec7nh <code>yarn dev</code> \u0111\u1ec3 th\u1ea5y k\u1ebft qu\u1ea3:</p> <p></p> <p>Th\u1eadm ch\u00ed ch\u00fang ta kh\u00f4ng c\u1ea7n ch\u1ea1y l\u1ec7nh <code>yarn tailwindcss -i ./src/index.css -o ./dist/output.css</code> v\u00e0  <code>import '../dist/output.css'</code> \u1edf t\u1eebng file c\u00f3 s\u1eed d\u1ee5ng tailwindcss, vite s\u1ebd automate qu\u00e1 tr\u00ecnh n\u00e0y d\u1ef1a v\u00e0o file <code>postcss.config.js</code>, x\u00f3a b\u1ecf folder <code>dist</code> v\u00e0 b\u00e2y gi\u1edd ch\u1ec9 c\u1ea7n Ctrl+S sau m\u1ed7i l\u1ea7n c\u1eadp nh\u1eadt c\u00e1c class tailwindcss.</p> </li> </ul> </li> <li> <p>C\u1ea5u h\u00ecnh Code Quality Toolset (eslint, prettier, lint-staged, husky)</p> <p>Trong c\u00f9ng m\u1ed9t d\u1ef1 \u00e1n, c\u00e1c th\u00e0nh vi\u00ean trong nh\u00f3m c\u00f3 th\u1ec3 c\u00f3 c\u00e1ch vi\u1ebft m\u00e3 ri\u00eang. S\u1eed d\u1ee5ng b\u1ed9 c\u00f4ng c\u1ee5 n\u00e0y gi\u00fap \u0111\u1ea3m b\u1ea3o m\u1ecdi ng\u01b0\u1eddi vi\u1ebft m\u00e3 theo c\u00f9ng m\u1ed9t quy t\u1eafc, t\u1eeb \u0111\u1ecbnh d\u1ea1ng m\u00e3 \u0111\u1ebfn quy t\u1eafc vi\u1ebft m\u00e3. \u0110i\u1ec1u n\u00e0y l\u00e0m cho m\u00e3 ngu\u1ed3n d\u1ec5 \u0111\u1ecdc h\u01a1n v\u00e0 gi\u1ea3m nguy c\u01a1 c\u00f3 nh\u1eefng sai s\u00f3t \u0111\u01a1n gi\u1ea3n.</p> <ol> <li> <p>Prettier</p> <ul> <li> <p>L\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 \u0111\u1ecbnh d\u1ea1ng m\u00e3 t\u1ef1 \u0111\u1ed9ng v\u00e0 \u0111\u1ed3ng nh\u1ea5t h\u00f3a c\u00fa ph\u00e1p m\u00e3. Prettier gi\u00fap \u0111\u1ea3m b\u1ea3o m\u00e3 ngu\u1ed3n c\u00f3 c\u00f9ng m\u1ed9t c\u1ea5u tr\u00fac, d\u1ea5u c\u00e1ch v\u00e0 c\u00fa ph\u00e1p, t\u1ea1o ra m\u00e3 \u0111\u1eb9p v\u00e0 d\u1ec5 \u0111\u1ecdc, tham kh\u1ea3o t\u1ea1i \u0111\u00e2y. C\u00e0i \u0111\u1eb7t <code>prettier</code> \u1edf devDependencies:</p> <pre><code>yarn add -D --exact prettier\n</code></pre> </li> <li> <p>T\u1ea1o file <code>.prettierrc</code> \u0111\u1ec3 c\u1ea5u h\u00ecnh prettier v\u1edbi c\u00e1c rule c\u01a1 b\u1ea3n nh\u01b0 sau:</p> <pre><code>{\n    \"printWidth\": 100,      // ensure that lines of code are no longer than 100 characters.\n    \"singleQuote\": true,    // require Prettier to use single quotes (') instead of double quotes (\") to wrap around strings.\n    \"jsxSingleQuote\": true, // similar to the above, but applicable to JSX (JavaScript XML) as well.\n    \"semi\": true,           // require Prettier to add semicolons (;) at the end of each line of code.\n    \"tabWidth\": 2,          // specify that Prettier will use 2 spaces for each tab.\n    \"bracketSpacing\": true, // add a space after the opening curly brace '{' and before the closing curly brace '}'.\n    \"arrowParens\": \"always\",// require Prettier to always place parentheses around the parameter of an arrow function.\n    \"trailingComma\": \"none\" // specify that Prettier won't add a trailing comma in multi-line lists like arrays or objects.\n}\n</code></pre> </li> <li> <p>T\u1ea1o file <code>.prettierignore</code> \u0111\u1ec3 Prettier bi\u1ebft c\u00e1c file n\u00e0o kh\u00f4ng c\u1ea7n \u0111\u1ecbnh d\u1ea1ng:</p> <pre><code>dist\nnode_modules\npackage*.json\n*.lock\n*.yml\n*.md\n</code></pre> </li> <li> <p>Ta \u0111\u1ecbnh d\u1ea1ng file App.tsx b\u1eb1ng command line <code>yarn prettier ./src/App.tsx --write</code> ho\u1eb7c th\u00eam m\u1ed9t script command cho to\u00e0n b\u1ed9 project trong file package.json <code>\"format\": \"prettier '**/*.{tsx,ts,js,json,md,yml,yaml}' --write\"</code>.</p> </li> </ul> </li> <li> <p>ESLint</p> <ul> <li> <p>L\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 ki\u1ec3m tra l\u1ed7i m\u00e3 ngu\u1ed3n v\u00e0 quy t\u1eafc vi\u1ebft m\u00e3 trong JavaScript v\u00e0 TypeScript, gi\u00fap ph\u00e1t hi\u1ec7n v\u00e0 s\u1eeda c\u00e1c l\u1ed7i c\u00fa ph\u00e1p, logic v\u00e0 code convention trong m\u00e3 ngu\u1ed3n, tham kh\u1ea3o t\u1ea1i \u0111\u00e2y. C\u00e0i \u0111\u1eb7t <code>eslint</code> c\u00f9ng v\u1edbi <code>eslint-config-prettier</code> (d\u00f9ng \u0111\u1ec3 t\u1eaft c\u00e1c quy t\u1eafc kh\u00f4ng c\u1ea7n thi\u1ebft ho\u1eb7c xung \u0111\u1ed9t v\u1edbi prettier) v\u00e0 m\u1ed9t plugin c\u1ee7a eslint l\u00e0 <code>eslint-plugin-react-hooks</code> (d\u00f9ng khi s\u1eed d\u1ee5ng c\u00e1c react hooks v\u00e0 c\u1ea7n tu\u00e2n theo Rules of Hooks) v\u00e0  \u1edf devDependencies:</p> <pre><code>yarn add -D eslint eslint-config-prettier eslint-plugin-react-hooks\n</code></pre> </li> <li> <p>T\u1ea1o file <code>.eslintrc</code> \u0111\u1ec3 c\u1ea5u h\u00ecnh eslint, th\u00eam c\u00e1c rules, bi\u1ebfn m\u00f4i tr\u01b0\u1eddng v\u00e0 c\u00e1c plugins c\u01a1 b\u1ea3n sau:</p> <pre><code>{\n  \"parser\": \"@typescript-eslint/parser\",          // specify that the TypeScript ESLint parser should be used to analyze TypeScript code.\n  \"extends\": [\n    \"prettier\",                                   // integrate Prettier with ESLint for code formatting.\n    \"plugin:@typescript-eslint/recommended\",      // provide recommended rules for TypeScript code from the @typescript-eslint plugin.\n    \"plugin:react-hooks/recommended\"              // provide recommended rules for using React hooks.\n  ],\n  \"plugins\": [\"@typescript-eslint\"],              // configuration uses the @typescript-eslint plugin.\n  \"parserOptions\": {\n    \"ecmaVersion\": 2018,                          // specify that the code is written in ECMAScript 2018 (ES9).\n    \"sourceType\": \"module\",                       // indicate that the code is using modules (ES6 modules).\n    \"project\": \"./tsconfig.json\"                  // specify the TypeScript project configuration file to use.\n  },\n  \"env\": {\n    \"node\": true,                                 // indicate that the code will run in a Node.js environment.\n    \"es6\": true                                   // indicate that ES6 (ECMAScript 2015) features are supported.\n  },\n  \"rules\": {\n    \"@typescript-eslint/no-inferrable-types\": 0,  // turn off the rule that flags unnecessary type declarations in TypeScript.\n    \"@typescript-eslint/no-unused-vars\": 2,       // set the rule that enforces no-unused-vars to an error level.\n    \"@typescript-eslint/no-var-requires\": 0,      // turn off the rule that disallows the use of require in TypeScript files.\n    \"eqeqeq\": \"error\"                             // enforce strict equality (===) over loose equality (==) and sets it to an error level.\n  }\n}\n</code></pre> <p>V\u00ec c\u1ea5u h\u00ecnh project trong parserOptions n\u1eb1m \u1edf file <code>tsconfig.json</code> n\u00ean ta c\u00f3 nh\u1eefng thay \u0111\u1ed5i \u1edf file n\u00e0y \u0111\u1ec3 t\u01b0\u01a1ng th\u00edch v\u1edbi ESLint nh\u01b0 sau:</p> <ul> <li>B\u1ecf c\u00e1c Linting trong compilerOptions v\u00ec \u0111\u00e3 c\u00f3 ESLint.</li> <li> <p>Thay \u0111\u1ed5i \u0111\u01b0\u1eddng d\u1eabn file trong <code>include</code> \u0111\u1ec3 ch\u1ec9 \u0111\u1ecbnh Typescript Compiler ch\u1ec9 c\u1ea7n bi\u00ean d\u1ecbch c\u00e1c file sau:</p> <pre><code>\"include\": [\"src\", \"**/*.ts\", \"**/*.tsx\", \"**/*.js\", \"*.cjs\"]\n</code></pre> </li> <li> <p>Thay \u0111\u1ed5i t\u00ean c\u00e1c \u0111u\u00f4i <code>*.config.js</code> (tailwind.config.js, postcss.config.js) th\u00e0nh \u0111u\u00f4i <code>*.config.cjs</code> (tailwind.config.cjs, postcss.config.cjs) cho ph\u00f9 h\u1ee3p v\u1edbi c\u00e1c \u0111\u01b0\u1eddng d\u1eabn file trong m\u1ee5c <code>include</code> (\u0111\u1ed3ng ngh\u0129a v\u1edbi vi\u1ec7c thay \u0111\u1ed5i syntax t\u1eeb ESModule sang CommonJS).</p> </li> <li> <p>M\u1ed9t v\u00e0i thay \u0111\u1ed5i kh\u00e1c \u0111\u00e3 \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n trong compilerOptions nh\u01b0ng kh\u00f4ng tr\u00ecnh b\u00e0y c\u1ee5 th\u1ec3 t\u1ea1i \u0111\u00e2y, c\u00e1c b\u1ea1n xem t\u1ea1i commit n\u00e0y c\u1ee7a m\u00ecnh tr\u00ean github nh\u00e9.         - T\u1ea1o file <code>.eslintignore</code> \u0111\u1ec3 ESLint bi\u1ebft c\u00e1c file n\u00e0o kh\u00f4ng c\u1ea7n ki\u1ec3m tra:</p> <pre><code>dist\nnode_modules\n</code></pre> </li> </ul> </li> <li> <p>Ta ch\u1ea1y eslint file App.tsx b\u1eb1ng command line <code>yarn eslint ./src/App.tsx --fix</code> ho\u1eb7c th\u00eam m\u1ed9t script command cho to\u00e0n b\u1ed9 project trong file package.json <code>\"lint\": \"eslint '**/*.{tsx,ts,js}' --fix\"</code>.</p> </li> </ul> </li> <li> <p>Lint-staged</p> <ul> <li> <p>Hai tool \u1edf tr\u00ean c\u00f3 th\u1ec3 \u0111\u00e3 \u0111\u1ee7 d\u00f9ng cho ch\u00fang ta \u0111\u1ecbnh d\u1ea1ng code, ki\u1ec3m tra c\u00fa ph\u00e1p, ki\u1ec3m tra code convention nh\u01b0ng c\u00f3 m\u1ed9t nh\u01b0\u1ee3c \u0111i\u1ec3m. \u0110\u00f3 l\u00e0 ch\u00fang \u0111\u1ec1u ch\u1ea1y tr\u00ean to\u00e0n b\u1ed9 source code d\u00f9 ta ch\u1ec9 thay \u0111\u1ed5i duy nh\u1ea5t 1 file, g\u00e2y t\u1ed1n th\u1eddi gian v\u00e0 resource c\u1ee7a m\u00e1y. V\u00ec v\u1eady, ta d\u00f9ng th\u00eam m\u1ed9t tool n\u1eefa l\u00e0 lint-staged, tool n\u00e0y gi\u1edbi h\u1ea1n ph\u1ea1m vi Prettier v\u00e0 ESLint ch\u1ec9 tr\u00ean nh\u1eefng file n\u1eb1m trong v\u00f9ng staging c\u1ee7a git (nh\u1eefng file \u0111\u01b0\u1ee3c git add), tham kh\u1ea3o t\u1ea1i \u0111\u00e2y. C\u00e0i \u0111\u1eb7t <code>lint-staged</code> \u1edf devDependencies:</p> <pre><code>yarn add -D lint-staged\n</code></pre> </li> <li> <p>T\u1ea1o file <code>.lintstagedrc.json</code> \u0111\u1ec3 gi\u1edbi h\u1ea1n ph\u1ea1m vi c\u1ee7a prettier v\u00e0 eslint:</p> <pre><code>{\n  \"**/*.{ts,js,tsx}\": [\"eslint --fix\"],\n  \"**/*.{tsx,ts,js,json,md,yml,yaml}\": [\"prettier --write\"]\n}\n</code></pre> </li> <li> <p>Tr\u01b0\u1edbc khi ch\u1ea1y lint-staged, ta ph\u1ea3i \u0111\u01b0a c\u00e1c file \u0111\u00e3 thay \u0111\u1ed5i v\u00e0o staging c\u1ee7a git, k\u1ebft qu\u1ea3 nh\u01b0 sau:</p> <p></p> </li> </ul> </li> <li> <p>Husky</p> <ul> <li> <p>V\u1edbi lint-staged, ta c\u00f3 th\u1ec3 g\u1ed9p 2 l\u1ea7n ch\u1ea1y prettier v\u00e0 eslint ch\u1ec9 b\u1eb1ng m\u1ed9t command line duy nh\u1ea5t l\u00e0 <code>yarn lint-staged</code> nh\u01b0ng v\u1eabn c\u00f2n 1 v\u1ea5n \u0111\u1ec1: S\u1ebd ra sao n\u1ebfu c\u00f3 th\u00e0nh vi\u00ean n\u00e0o \u0111\u00f3 trong team qu\u00ean ch\u1ea1y <code>yarn lint-staged</code> tr\u01b0\u1edbc khi commit code? \u0110i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 ph\u00e1 v\u1ee1 code convention m\u00e0 team \u0111\u00e3 x\u00e2y d\u1ef1ng, khi\u1ebfn cho code tr\u1edf n\u00ean r\u1ed1i lo\u1ea1n, l\u00e0m ch\u1eadm th\u1eddi gian ph\u00e1t tri\u1ec3n s\u1ea3n ph\u1ea9m. Ch\u00ednh v\u00ec v\u1eady, husky ra \u0111\u1eddi nh\u01b0 l\u00e0 m\u1ed9t gi\u1ea3i ph\u00e1p cho v\u1ea5n \u0111\u1ec1 n\u00e0y. Husky l\u00e0 m\u1ed9t tool trigger c\u00e1c h\u00e0nh \u0111\u1ed9ng \u0111\u01b0\u1ee3c c\u00e0i \u0111\u1eb7t trong git hooks, tham kh\u1ea3o t\u1ea1i \u0111\u00e2y. Ta s\u1ebd d\u00f9ng husky \u0111\u1ec3 c\u1ea5u h\u00ecnh pre-commit git hook cho lint-staged, \u0111\u1ea3m b\u1ea3o ch\u1ec9 c\u1ea7n c\u00e1c th\u00e0nh vi\u00ean trong team commit code th\u00ec c\u00e2u l\u1ec7nh <code>yarn lint-staged</code> t\u1ef1 \u0111\u1ed9ng \u0111\u01b0\u1ee3c th\u1ef1c thi. C\u00e0i \u0111\u1eb7t <code>husky</code> \u1edf devDependencies v\u00e0 install git hook:</p> <pre><code>yarn add -D husky\nyarn husky install\n</code></pre> </li> <li> <p>B\u1ea1n s\u1ebd th\u1ea5y m\u1ed9t th\u01b0 m\u1ee5c <code>.husky</code> xu\u1ea5t hi\u1ec7n ch\u1ee9a 1 file t\u00ean l\u00e0 <code>husky.sh</code>, file n\u00e0y g\u1ed3m c\u00e1c \u0111o\u1ea1n m\u00e3 shell script m\u00e0 husky s\u1eed d\u1ee5ng \u0111\u1ec3 th\u1ef1c hi\u1ec7n c\u00e1c h\u00e0nh \u0111\u1ed9ng khi c\u00e1c git hooks \u0111\u01b0\u1ee3c k\u00edch ho\u1ea1t.</p> </li> <li> <p>T\u1ea1o m\u1ed9t pre-commit git hook \u0111\u1ec3 trigger c\u00e2u l\u1ec7nh <code>yarn lint-staged</code> m\u1ed7i l\u1ea7n tr\u01b0\u1edbc khi commit b\u1eb1ng command line sau:</p> <pre><code>yarn husky add .husky/pre-commit 'yarn lint-staged'\n</code></pre> <p>N\u1ebfu c\u00f3 b\u1ea5t c\u1ee9 l\u1ed7i g\u00ec vi ph\u1ea1m c\u00e1c lu\u1eadt c\u1ee7a prettier v\u00e0 eslint th\u00ec commit s\u1ebd b\u1ecb ch\u1eb7n, \u0111\u1ea3m b\u1ea3o ch\u1ec9 c\u00f3 nh\u1eefng \u0111o\u1ea1n code \u0111\u00fang quy t\u1eafc m\u1edbi \u0111\u01b0\u1ee3c push l\u00ean repository chung.</p> </li> <li> <p>Sau khi commit v\u00e0 push l\u00ean github, ch\u1ec9 c\u00f3 duy nh\u1ea5t file <code>pre-commit</code> \u0111\u01b0\u1ee3c push l\u00ean, file <code>husky.sh</code> \u0111\u00e3 \u0111\u01b0\u1ee3c git ignore. Ch\u00ednh v\u00ec th\u1ebf, ta s\u1ebd th\u00eam 1 script command <code>\"prepare\": \"husky install\"</code> trong package.json \u0111\u1ec3 t\u1ef1 \u0111\u1ed9ng kh\u1edfi t\u1ea1o <code>husky.sh</code> khi ch\u1ea1y l\u1ec7nh <code>yarn install</code> m\u00e0 kh\u00f4ng c\u1ea7n c\u00e1c th\u00e0nh vi\u00ean trong team ph\u1ea3i setup l\u1ea1i khi pull code v\u1ec1.</p> </li> </ul> </li> </ol> <p>Vi\u1ec7c b\u00e2y gi\u1edd c\u1ee7a c\u00e1c developer trong team l\u00e0 ch\u1ec9 c\u1ea7n code v\u00e0 commit, m\u1ecdi th\u1ee9 li\u00ean quan \u0111\u1ebfn coding convention v\u00e0 ki\u1ec3m tra c\u00e1c l\u1ed7i syntax \u0111\u00e3 \u0111\u01b0\u1ee3c automation b\u1edfi b\u1ed9 c\u00f4ng c\u1ee5 ESLint, Prettier, Lint-staged v\u00e0 Husky.</p> </li> <li> <p>C\u1ea5u h\u00ecnh m\u1ed9t th\u01b0 vi\u1ec7n UI k\u1ebft h\u1ee3p ReactJS v\u00e0 TailwindCSS</p> <p>L\u00fd do ch\u00fang ta c\u00f3 b\u01b0\u1edbc n\u00e0y b\u1edfi v\u00ec r\u1ea5t kh\u00f3 \u0111\u1ec3 c\u00e1c developer nh\u1edb v\u00e0 s\u1eed d\u1ee5ng m\u1ed9t h\u1ec7 th\u1ed1ng c\u00e1c class element kh\u1ed5ng l\u1ed3 trong TailwindCSS, c\u1ea7n ph\u1ea3i c\u00f3 m\u1ed9t wrapper library \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng d\u1ef1a tr\u00ean TailwindCSS v\u00e0 c\u00f3 h\u1ed7 tr\u1ee3 ReactJS, m\u00ecnh ch\u1ecdn th\u01b0 vi\u1ec7n Material Tailwind cho project n\u00e0y, tham kh\u1ea3o t\u1ea1i (https://www.material-tailwind.com/). C\u00e0i \u0111\u1eb7t <code>material-tailwind</code> \u1edf dependencies:</p> <pre><code>yarn add @material-tailwind/react\n</code></pre> <p>Sau khi c\u00e0i \u0111\u1eb7t, ta c\u1ea7n c\u1ea5u h\u00ecnh th\u00eam file <code>tailwind.config.cjs</code> v\u00e0 <code>main.tsx</code> \u0111\u1ec3 t\u01b0\u01a1ng th\u00edch v\u1edbi Material Tailwind, c\u00e1c b\u1ea1n xem c\u1ee5 th\u1ec3 t\u1ea1i commit n\u00e0y c\u1ee7a m\u00ecnh tr\u00ean github nh\u00e9.</p> </li> </ol>"},{"location":"blog/set-up-front-end-development-environment-part-1/#4-to-chuc-source-code","title":"4. T\u1ed5 ch\u1ee9c source code","text":"<p>\u0110\u1ed1i v\u1edbi m\u1ed9t project, vi\u1ec7c t\u1ed5 ch\u1ee9c source code m\u1ed9t c\u00e1ch t\u01b0\u1eddng minh v\u00e0 r\u00f5 r\u00e0ng l\u00e0 c\u1ef1c k\u1ef3 quan tr\u1ecdng, \u0111\u1eb7c bi\u1ec7t l\u00e0 khi k\u00edch c\u1ee1 project b\u1eaft \u0111\u1ea7u l\u1edbn d\u1ea7n v\u00e0 c\u00f3 th\u00eam nhi\u1ec1u nh\u00e2n s\u1ef1 tham gia v\u00e0o, ph\u1ea3i t\u1ed1i \u01b0u \u0111\u01b0\u1ee3c qu\u00e1 tr\u00ecnh \u0111\u1ecdc hi\u1ec3u c\u1ee7a c\u00e1c developer trong team v\u00e0 h\u1ea1n ch\u1ebf tr\u00f9ng l\u1eb7p m\u00e3 code. D\u01b0\u1edbi \u0111\u00e2y l\u00e0 m\u1ed9t \u201cpattern\u201d m\u00e0 team Engineer t\u1ea1i TickLab \u0111\u00e3 s\u1eed d\u1ee5ng trong qu\u00e1 tr\u00ecnh ph\u00e1t tri\u1ec3n s\u1ea3n ph\u1ea9m, \u0111\u01b0\u1ee3c minh h\u1ecda qua v\u00ed d\u1ee5 \u1edf m\u1ee5c 2.2.  </p> <ul> <li> <p>Ta b\u1eaft \u0111\u1ea7u v\u1edbi y\u00eau c\u1ea7u th\u1ee9 nh\u1ea5t: \u1ee8ng d\u1ee5ng g\u1ed3m trang \u0111\u0103ng nh\u1eadp, trang hi\u1ec3n th\u1ecb t\u1ed5ng quan t\u1ea5t c\u1ea3 c\u00e1c project v\u00e0 trang hi\u1ec3n th\u1ecb th\u00f4ng tin chi ti\u1ebft c\u1ee7a m\u1ed7i project. Do \u0111\u00f3, ta c\u1ea7n m\u1ed9t th\u01b0 m\u1ee5c <code>src/pages</code> ch\u1ee9a t\u1ea5t c\u1ea3 c\u00e1c trang UI ch\u00ednh. T\u1ea1o 3 trang trong th\u01b0 m\u1ee5c <code>pages</code>:</p> <ul> <li><code>AuthPage</code>: trang \u0111\u0103ng nh\u1eadp.</li> <li><code>ProjectGeneralPage</code>: trang hi\u1ec3n th\u1ecb t\u1ed5ng quan t\u1ea5t c\u1ea3 c\u00e1c project.</li> <li><code>ProjectDetailPage</code>: trang hi\u1ec3n th\u1ecb th\u00f4ng tin chi ti\u1ebft c\u1ee7a m\u1ed7i project.</li> </ul> <p>M\u1ed7i page s\u1ebd \u0111\u01b0\u1ee3c ph\u00e2n chia th\u00e0nh t\u1eebng component nh\u01b0 button, table, navigation bar, search bar, pagination bar,\u2026C\u00e1c component n\u00e0y c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u1ea1i nhi\u1ec1u l\u1ea7n trong m\u1ed9t page ho\u1eb7c \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng gi\u1eefa c\u00e1c page. M\u1ed9t th\u01b0 m\u1ee5c <code>src/components</code> ch\u1ecbu tr\u00e1ch nhi\u1ec7m l\u01b0u gi\u1eef c\u00e1c component \u0111\u01b0\u1ee3c t\u00e1i s\u1eed d\u1ee5ng.</p> <p>T\u00f9y v\u00e0o b\u1ea3n design UI/UX c\u1ee7a t\u1eebng project nh\u01b0ng th\u00f4ng th\u01b0\u1eddng layout c\u1ee7a trang \u0111\u0103ng nh\u1eadp s\u1ebd kh\u00e1c v\u1edbi layout c\u1ee7a c\u00e1c trang c\u00f2n l\u1ea1i. Do \u0111\u00f3, ta s\u1ebd t\u1ea1o th\u01b0 m\u1ee5c <code>src/layouts</code> bao g\u1ed3m 2 layout ch\u00ednh: <code>AuthLayout.tsx</code> v\u00e0 <code>AppLayout.tsx</code>.</p> <p>Ta t\u1ea1o th\u00eam 2 th\u01b0 m\u1ee5c <code>src/interfaces</code> v\u00e0 <code>src/types</code> \u0111\u1ec3 t\u1ea1o th\u00eam c\u00e1c c\u1ea5u tr\u00fac \u0111\u1ed1i t\u01b0\u1ee3ng v\u00e0 ki\u1ec3u d\u1eef li\u1ec7u t\u00f9y ch\u1ec9nh cho project, c\u00e0i \u0111\u1eb7t th\u00eam package <code>@types/node</code> \u0111\u1ec3 \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 c\u00e1c ki\u1ec3u d\u1eef li\u1ec7u trong Node.js.</p> </li> <li> <p>Ti\u1ebfp \u0111\u1ebfn v\u1edbi y\u00eau c\u1ea7u th\u1ee9 hai: \u1ee8ng d\u1ee5ng ph\u1ea3i bao g\u1ed3m 4 thao t\u00e1c c\u01a1 b\u1ea3n: t\u1ea1o - xem - s\u1eeda - x\u00f3a. Y\u00eau c\u1ea7u n\u00e0y \u0111\u00f2i h\u1ecfi \u1ee9ng d\u1ee5ng ph\u1ea3i t\u01b0\u01a1ng t\u00e1c v\u1edbi c\u00e1c APIs t\u1eeb ph\u00eda back-end. Ta s\u1ebd t\u1ea1o m\u1ed9t th\u01b0 m\u1ee5c t\u00ean l\u00e0 <code>src/services</code> gi\u1eef c\u00e1c l\u1eddi g\u1ecdi API. \u1ede b\u01b0\u1edbc n\u00e0y, ta c\u1ea7n l\u01b0u \u00fd v\u00e0i v\u1ea5n \u0111\u1ec1 sau:</p> <ul> <li>\u0110\u1eb7t URL c\u1ee7a server v\u00e0o m\u1ed9t file t\u00ean l\u00e0 <code>.env</code>. B\u1edfi trong m\u1ed9t d\u1ef1 \u00e1n th\u1ef1c t\u1ebf, ch\u00fang ta s\u1ebd l\u00e0m vi\u1ec7c v\u1edbi nhi\u1ec1u server kh\u00e1c nhau t\u00f9y thu\u1ed9c v\u00e0o t\u1eebng giai \u0111o\u1ea1n ph\u00e1t tri\u1ec3n, v\u00ed d\u1ee5 <code>http://localhost:8080</code> \u1edf m\u00f4i tr\u01b0\u1eddng Development, <code>https://staging.example.com</code> \u1edf m\u00f4i tr\u01b0\u1eddng Staging v\u00e0 <code>https://api.example.com</code> \u1edf m\u00f4i tr\u01b0\u1eddng Production. Ch\u1ec9 c\u1ea7n thay \u0111\u1ed5i URL server trong file <code>.env</code> m\u00e0 kh\u00f4ng c\u1ea7n ph\u1ea3i s\u1eeda \u0111\u1ed5i source code.</li> <li>\u0110\u1eb7t <code>.env</code> v\u00e0o <code>.gitignore</code>, \u0111\u1ed3ng th\u1eddi t\u1ea1o m\u1ed9t file <code>.env.example</code> \u0111\u1ec3 cung c\u1ea5p m\u1eabu cho <code>.env</code> v\u00e0 \u0111\u01b0\u1ee3c ph\u00e9p push file n\u00e0y l\u00ean repository chung. \u0110i\u1ec1u n\u00e0y gi\u00fap c\u00e1c th\u00e0nh vi\u00ean trong team nh\u1eadn ra c\u00e1c bi\u1ebfn m\u00f4i tr\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong d\u1ef1 \u00e1n v\u00e0 c\u00f3 c\u00e1ch c\u00e0i \u0111\u1eb7t ph\u00f9 h\u1ee3p.</li> </ul> <p>Th\u00f4ng th\u01b0\u1eddng, c\u00e1c \u0111o\u1ea1n m\u00e3 g\u1ecdi API s\u1ebd c\u00f3 logic gi\u1ed1ng nhau. Do \u0111\u00f3, ta vi\u1ebft m\u1ed9t h\u00e0m <code>invoke</code> \u0111\u1ec3 x\u1eed l\u00fd logic n\u00e0y v\u00e0 \u0111\u1eb7t n\u00f3 v\u00e0o file <code>services/common.ts</code>. M\u00ecnh s\u1ebd mock m\u1ed9t back-end \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 fake Authentication v\u00e0 CRUD APIs, logic \u0111\u01b0\u1ee3c \u0111\u1eb7t trong th\u01b0 m\u1ee5c <code>__mock_server__</code>(lo\u1ea1i b\u1ecf th\u01b0 m\u1ee5c n\u00e0y khi b\u1ea1n l\u00e0m vi\u1ec7c v\u1edbi c\u00e1c API th\u1eadt). C\u00e1c b\u1ea1n xem implementation c\u1ee7a h\u00e0m <code>invoke</code> v\u00e0 c\u00e1ch n\u00f3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong <code>authService</code> v\u00e0 <code>userService</code> t\u1ea1i commit n\u00e0y c\u1ee7a m\u00ecnh tr\u00ean github nh\u00e9.</p> <p>Ta c\u00f3 th\u1ec3 k\u1ebft th\u00fac vi\u1ec7c setup folder \u1edf b\u01b0\u1edbc n\u00e0y ngay t\u1ea1i \u0111\u00e2y \u0111\u1ed1i v\u1edbi c\u00e1c project c\u00f3 kh\u1ed1i l\u01b0\u1ee3ng c\u00f4ng vi\u1ec7c nh\u1ecf, ch\u1ec9 gi\u1ea3i quy\u1ebft c\u00e1c nghi\u1ec7p v\u1ee5 \u0111\u01a1n gi\u1ea3n nh\u01b0 \u0111\u0103ng k\u00ed, \u0111\u0103ng nh\u1eadp, CRUD,\u2026S\u1ebd c\u00f3 m\u1ed9t v\u1ea5n \u0111\u1ec1 ph\u00e1t sinh khi kh\u1ed1i l\u01b0\u1ee3ng c\u00f4ng vi\u1ec7c t\u0103ng l\u00ean v\u00e0 \u0111\u1eb7c bi\u1ec7t l\u00e0 c\u00f3 th\u00eam nh\u00e2n s\u1ef1 tham gia v\u00e0o d\u1ef1 \u00e1n c\u1ee7a b\u1ea1n, \u0111\u00f3 ch\u00ednh l\u00e0 vi\u1ec7c qu\u1ea3n l\u00fd c\u00e1c global state. N\u1ebfu ch\u1ec9 qu\u1ea3n l\u00fd c\u00e1c state b\u1eb1ng vi\u1ec7c s\u1eed d\u1ee5ng hook <code>useState</code> trong React v\u00e0 ph\u01b0\u01a1ng ph\u00e1p Passing Props (truy\u1ec1n state t\u1eeb component cha xu\u1ed1ng component con) th\u00ec c\u00f3 th\u1ec3 ngo\u00e0i b\u1ea1n ra, kh\u00f4ng m\u1ed9t ai c\u00f3 th\u1ec3 tham gia v\u00e0o project \u0111\u00f3 \u0111\u01b0\u1ee3c, ho\u1eb7c s\u1ebd m\u1ea5t r\u1ea5t nhi\u1ec1u th\u1eddi gian \u0111\u1ec3 c\u00e1c th\u00e0nh vi\u00ean kh\u00e1c trong team \u0111\u1ecdc hi\u1ec3u source code. \u0110i\u1ec1u n\u00e0y c\u00f2n \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn t\u00ednh scalability c\u1ee7a project khi ph\u00e1t tri\u1ec3n c\u00e1c t\u00ednh n\u0103ng m\u1edbi v\u00e0 h\u1eadu qu\u1ea3 t\u1ec7 nh\u1ea5t c\u00f3 th\u1ec3 x\u1ea3y ra l\u00e0 ph\u1ea3i \u201c\u0111\u1eadp \u0111i x\u00e2y l\u1ea1i\u201d, l\u00e0m ch\u1eadm th\u1eddi gian release s\u1ea3n ph\u1ea9m cho kh\u00e1ch h\u00e0ng. Ch\u00ednh v\u00ec v\u1eady, v\u1ea5n \u0111\u1ec1 global state ph\u1ea3i \u0111\u01b0\u1ee3c gi\u1ea3i quy\u1ebft \u0111\u1ea7u ti\u00ean ngay t\u1ea1i b\u01b0\u1edbc setup m\u00f4i tr\u01b0\u1eddng development. Solution t\u01b0\u01a1ng \u0111\u1ed1i ph\u1ed5 bi\u1ebfn trong h\u1ea7u h\u1ebft c\u00e1c project l\u00e0 s\u1eed d\u1ee5ng m\u1ed9t State Management Library, c\u00e1c tool ph\u1ed5 bi\u1ebfn g\u1ed3m c\u00f3 Redux, Zustand. \u1ede \u0111\u00e2y, m\u00ecnh s\u1eed d\u1ee5ng Zustand v\u00ec ki\u1ebfn tr\u00fac th\u01b0 vi\u1ec7n n\u00e0y \u0111\u01a1n gi\u1ea3n h\u01a1n Redux v\u1edbi vi\u1ec7c s\u1eed d\u1ee5ng c\u00e1c React Hook \u0111\u1ec3 qu\u1ea3n l\u00fd c\u00e1c store \u0111\u1ed9c l\u1eadp, gi\u00fap truy c\u1eadp v\u00e0o state v\u00e0 c\u00e1c action d\u1ec5 d\u00e0ng h\u01a1n, tham kh\u1ea3o t\u1ea1i \u0111\u00e2y. C\u00e0i \u0111\u1eb7t <code>zustand</code> \u1edf dependencies:</p> <pre><code>yarn add zustand\n</code></pre> <p>T\u1ea1o m\u1ed9t th\u01b0 m\u1ee5c <code>src/states</code> \u0111\u1ec3 l\u01b0u tr\u1eef c\u00e1c store. \u0110\u1ed1i v\u1edbi d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1 t\u1eeb API, ta x\u00e2y d\u1ef1ng m\u1ed9t flow call API nh\u01b0 sau:</p> <p></p> <p>Quy t\u1eafc tr\u00ean \u0111\u1ea3m b\u1ea3o c\u00e1c th\u00e0nh vi\u00ean trong team hi\u1ec3u r\u00f5 lu\u1ed3ng th\u1ef1c thi c\u1ee7a m\u00e3 ngu\u1ed3n, t\u0103ng kh\u1ea3 n\u0103ng \u0111\u1ecdc hi\u1ec3u v\u00e0 gi\u1ea3m th\u1eddi gian fix bug. Xem back-end nh\u01b0 b\u00ean cung c\u1ea5p interface \u0111\u1ec3 t\u1ea1o type cho d\u1eef li\u1ec7u tr\u1ea3 v\u1ec1 t\u1eeb API. G\u1ecdi API trong services, s\u1eed d\u1ee5ng c\u00e1c types \u0111\u00e3 t\u1ea1o. T\u1ea1o m\u1ed9t store l\u01b0u tr\u1eef c\u00e1c state v\u00e0 action cho b\u1ed9 APIs li\u00ean quan \u0111\u1ebfn m\u1ed9t feature c\u1ee7a s\u1ea3n ph\u1ea9m (Authentication, CRUD, Automatic Notification,\u2026). D\u00f9ng c\u00e1c store n\u00e0y trong components v\u00e0 pages. M\u00ecnh s\u1ebd implement 2 stores: <code>useUserStore</code> v\u00e0 <code>useProjectGeneralStore</code> bao g\u1ed3m state v\u00e0 action c\u1ee7a b\u1ed9 CRUD API. C\u00e1c b\u1ea1n xem c\u1ee5 th\u1ec3 t\u1ea1i commit n\u00e0y c\u1ee7a m\u00ecnh tr\u00ean github nh\u00e9.</p> </li> <li> <p>Y\u00eau c\u1ea7u cu\u1ed1i c\u00f9ng: \u1ee8ng d\u1ee5ng ph\u1ea3i t\u01b0\u01a1ng th\u00edch tr\u00ean c\u00e1c m\u00e0n h\u00ecnh desktop, mobile. \u0110\u00e2y ch\u00ednh l\u00e0 t\u00ednh n\u0103ng responsive c\u1ee7a s\u1ea3n ph\u1ea9m. Nguy\u00ean l\u00fd thi\u1ebft k\u1ebf UI/UX trong vi\u1ec7c design c\u00e1c \u1ee9ng d\u1ee5ng web l\u00e0 mobile first, t\u1ee9c l\u00e0 t\u1eebng module trong b\u1ea3n thi\u1ebft k\u1ebf ph\u1ea3i t\u01b0\u01a1ng th\u00edch tr\u00ean \u0111i\u1ec7n tho\u1ea1i tr\u01b0\u1edbc, sau \u0111\u00f3 m\u1edbi scale d\u1ea7n l\u00ean c\u00e1c m\u00e0n h\u00ecnh c\u00f3 k\u00edch th\u01b0\u1edbc l\u1edbn h\u01a1n nh\u01b0 tablet, desktop,\u2026TailwindCSS c\u0169ng tu\u00e2n theo design principle n\u00e0y v\u1edbi h\u1ec7 th\u1ed1ng breakpoint bao g\u1ed3m 5 breakpoints c\u01a1 b\u1ea3n: <code>xs</code>, <code>sm</code>, <code>md</code>, <code>lg</code>, <code>xl</code>. Tuy nhi\u00ean, \u0111\u1ed1i v\u1edbi c\u00e1c components c\u00f3 s\u1ef1 thay \u0111\u1ed5i qu\u00e1 l\u1edbn khi chuy\u1ec3n \u0111\u1ed5i gi\u1eefa c\u00e1c k\u00edch th\u01b0\u1edbc m\u00e0n h\u00ecnh, vi\u1ec7c s\u1eed d\u1ee5ng breakpoint c\u00f3 th\u1ec3 kh\u00f3 kh\u0103n h\u01a1n so v\u1edbi vi\u1ec7c t\u1ea1o ra c\u00e1c \u201cversion\u201d m\u1edbi. Do \u0111\u00f3, ta s\u1ebd t\u1ea1o m\u1ed9t hook t\u00ean l\u00e0 <code>useScreenSize</code> ch\u1ecbu tr\u00e1ch nhi\u1ec7m theo d\u00f5i s\u1ef1 thay \u0111\u1ed5i k\u00edch th\u01b0\u1edbc c\u1ee7a m\u00e0n h\u00ecnh (implement hook n\u00e0y trong th\u01b0 m\u1ee5c <code>src/hooks</code> - \u0111\u00e2y l\u00e0 th\u01b0 m\u1ee5c ch\u1ee9a t\u1ea5t c\u1ea3 c\u00e1c custom hooks m\u00e0 ch\u00fang ta \u0111\u1ecbnh ngh\u0129a) v\u00e0 m\u1ed9t enum <code>ScreenSize</code> ch\u1ee9a c\u00e1c lo\u1ea1i m\u00e0n h\u00ecnh trong th\u01b0 m\u1ee5c <code>src/constants</code> (t\u1ea5t c\u1ea3 c\u00e1c constants trong project \u0111\u01b0\u1ee3c \u0111\u1eb7t t\u1ea1i th\u01b0 m\u1ee5c n\u00e0y) \u0111\u1ec3 h\u1ed7 tr\u1ee3 vi\u1ec7c chuy\u1ec3n \u0111\u1ed5i gi\u1eefa c\u00e1c version c\u1ee7a component khi resize. C\u00e1c b\u1ea1n xem chi ti\u1ebft implementation c\u1ee7a <code>useScreenSize</code> v\u00e0 c\u00e1ch hook n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u1edf <code>ProjectGeneralPage.tsx</code> t\u1ea1i commit n\u00e0y.</p> <p>B\u1edfi v\u00ec c\u00e1c hooks ch\u1ec9 \u0111\u01b0\u1ee3c ph\u00e9p g\u1ecdi b\u00ean trong c\u00e1c function components v\u00e0 c\u00e1c custom hooks kh\u00e1c n\u00ean ta s\u1ebd t\u1ea1o th\u00eam m\u1ed9t th\u01b0 m\u1ee5c <code>src/utils</code> \u0111\u1ec3 ch\u1ee9a c\u00e1c h\u00e0m x\u1eed l\u00fd c\u00e1c t\u00e1c v\u1ee5 b\u00ecnh th\u01b0\u1eddng.</p> </li> </ul> <p>Ta \u0111\u00e3 \u0111i qua 3 requirements ph\u1ed5 bi\u1ebfn nh\u1ea5t c\u1ee7a h\u1ea7u h\u1ebft c\u00e1c \u1ee9ng d\u1ee5ng thu\u1ed9c lo\u1ea1i Management Application \u0111\u1ec3 t\u1eeb \u0111\u00f3 x\u00e2y d\u1ef1ng n\u00ean m\u1ed9t c\u1ea5u tr\u00fac th\u01b0 m\u1ee5c cho team Front-end developers. B\u00e2y gi\u1edd, h\u00e3y c\u00f9ng nh\u00ecn l\u1ea1i to\u00e0n b\u1ed9 source code v\u00e0 th\u1ea5y r\u1eb1ng: Thay v\u00ec s\u1eed d\u1ee5ng c\u00e1c \u0111\u01b0\u1eddng d\u1eabn ph\u1ee9c t\u1ea1p, ph\u1ea3i import ri\u00eang l\u1ebb t\u1eebng module m\u1eb7c d\u00f9 ch\u00fang c\u00f9ng thu\u1ed9c m\u1ed9t th\u01b0 m\u1ee5c, ta s\u1ebd s\u1eed d\u1ee5ng path alias trong Typescript v\u00e0 ph\u1ed1i h\u1ee3p v\u1edbi m\u1ed9t tool t\u1ef1 \u0111\u1ed9ng \u201c\u0111\u00f3ng g\u00f3i\u201d c\u00e1c \u0111\u01b0\u1eddng d\u1eabn v\u00e0o 1 file <code>index.ts</code> trong t\u1eebng th\u01b0 m\u1ee5c l\u00e0 <code>barrelsby</code> (tham kh\u1ea3o t\u1ea1i \u0111\u00e2y). C\u00e0i \u0111\u1eb7t <code>barrelsby</code> \u1edf devDependencies:</p> <pre><code>yarn add -D barrelsby\n</code></pre> <p>T\u1ea1o file <code>.barrelsby.json</code> \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh c\u00e1c th\u01b0 m\u1ee5c c\u1ea7n \u0111\u00f3ng g\u00f3i:</p> <pre><code>{\n    \"directory\": [\n      \"./src/components\",\n      \"./src/constants\",\n      \"./src/hooks\",\n      \"./src/interfaces\",\n      \"./src/layouts\",\n      \"./src/pages\",\n      \"./src/services\",\n      \"./src/states\",\n      \"./src/utils\"\n    ],\n    \"delete\": true\n  }\n</code></pre> <p>T\u1ea1o m\u1ed9t script command <code>\"barrels\": \"barrelsby --config .barrelsby.json -q\"</code> v\u00e0 th\u1ef1c thi b\u1eb1ng command line <code>yarn barrels</code>, file <code>index.ts</code> s\u1ebd t\u1ef1 \u0111\u1ed9ng xu\u1ea5t hi\u1ec7n trong c\u00e1c th\u01b0 m\u1ee5c \u0111\u01b0\u1ee3c ch\u1ec9 \u0111\u1ecbnh b\u1edfi <code>.barelsby.json</code>. C\u1ea5u h\u00ecnh th\u00eam paths v\u00e0 resolve alias t\u01b0\u01a1ng \u1ee9ng trong <code>tsconfig.json</code> v\u00e0 <code>vite.config.ts</code> (c\u1ee5 th\u1ec3 t\u1ea1i commit), Sau \u0111\u00f3, ta c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c path alias \u0111\u1ec3 \u0111\u01a1n gi\u1ea3n h\u00f3a c\u00e1c thao t\u00e1c import.</p>"},{"location":"blog/set-up-front-end-development-environment-part-1/#5-ket-luan","title":"5. K\u1ebft lu\u1eadn","text":"<p>M\u00ecnh s\u1ebd k\u1ebft th\u00fac ph\u1ea7n 1 c\u1ee7a b\u00e0i vi\u1ebft n\u00e0y t\u1ea1i \u0111\u00e2y, t\u1ea5t c\u1ea3 c\u00e1c b\u01b0\u1edbc setup \u0111\u1ec1u \u0111\u01b0\u1ee3c l\u01b0u l\u1ea1i theo t\u1eebng commit t\u01b0\u01a1ng \u1ee9ng tr\u00ean github repository. \u1ede ph\u1ea7n 2, m\u00ecnh s\u1ebd chia s\u1ebb v\u1edbi c\u00e1c b\u1ea1n c\u00e1ch setup m\u00f4i tr\u01b0\u1eddng container \u0111\u1ec3 deploy \u1ee9ng d\u1ee5ng web l\u00ean m\u1ed9t m\u00e1y \u1ea3o (Virtual Private Server) s\u1eed d\u1ee5ng Docker Container v\u00e0 Nginx Web Server d\u1ef1a tr\u00ean template n\u00e0y. \u0110\u00e2y l\u00e0 b\u01b0\u1edbc ti\u1ec1n \u0111\u1ec1 \u0111\u1ec3 t\u1ea1o n\u00ean workflows CI/CD c\u1ee7a project.</p>"},{"location":"blog/set-up-front-end-development-environment-part-1/#6-tham-khao","title":"6. Tham kh\u1ea3o","text":"<ol> <li>Responsve Design: https://tailwindcss.com/docs/responsive-design</li> <li>Sensor management web application: https://github.com/HPCMonitoring/sensor-manager</li> </ol>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/","title":"Accelerating Monocular Depth Estimation using TensorRT","text":"<p>Depth estimation l\u00e0 b\u00e0i to\u00e1n kinh \u0111i\u1ec3n trong l\u0129nh v\u1ef1c x\u1eed l\u00fd \u1ea3nh v\u00e0 \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng trong nhi\u1ec1u l\u0129nh v\u1ef1c nh\u01b0 xe t\u1ef1 h\u00e0nh, x\u00e2y d\u1ef1ng c\u1ea5u tr\u00fac 3D, \u0111i\u1ec1u khi\u1ec3n robot. Th\u00f4ng tin n\u00e0y th\u01b0\u1eddng \u0111\u01b0\u1ee3c \u01b0\u1edbc l\u01b0\u1ee3ng b\u1eb1ng thi\u1ebft b\u1ecb c\u1ea3m bi\u1ebfn nh\u01b0 LiDAR, ho\u1eb7c camera \u0111\u1eb7c bi\u1ec7t nh\u01b0 Stereo camera. V\u1edbi s\u1ef1 ph\u00e1t tri\u1ec3n c\u1ee7a Deep Learning, nhi\u1ec1u nh\u00e0 nghi\u00ean c\u1ee9u b\u1eaft \u0111\u1ea7u quan t\u00e2m \u0111\u1ebfn vi\u1ec7c \u1ee9ng d\u1ee5ng c\u00e1c m\u1ea1ng h\u1ecdc s\u00e2u \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n n\u00e0y. Monocular Depth Estimation (MDE) l\u00e0 b\u00e0i to\u00e1n s\u1eed d\u1ee5ng m\u1ea1ng h\u1ecdc s\u00e2u \u0111\u1ec3 \u01b0\u1edbc l\u01b0\u1ee3ng depth d\u1ef1a tr\u00ean 1 \u1ea3nh. </p> <p>Trong nghi\u00ean c\u1ee9u khoa h\u1ecdc, c\u00e1c ti\u00eau ch\u00ed v\u1ec1 \u1ee9ng d\u1ee5ng th\u1ef1c t\u1ebf bao g\u1ed3m nh\u01b0 kh\u1ea3 n\u0103ng tri\u1ec3n khai m\u00f4 h\u00ecnh, t\u1ed1c \u0111\u1ed9 th\u1ef1c thi, \u2026 th\u01b0\u1eddng \u0111\u01b0\u1ee3c b\u1ecf qua. Hi\u1ec7n t\u1ea1i, nh\u00f3m nghi\u00ean c\u1ee9u c\u1ee7a ch\u00fang t\u00f4i \u0111ang gi\u1ea3i quy\u1ebft MDE cho l\u0129nh v\u1ef1c Autonomous Driving. Ch\u00fang t\u00f4i ch\u1ec9 t\u1eadp trung nghi\u00ean c\u1ee9u c\u00e1c gi\u1ea3i ph\u00e1p sao cho \u0111\u1ed9 s\u00e2u \u01b0\u1edbc l\u01b0\u1ee3ng kh\u1edbp v\u1edbi \u0111\u00f4 s\u00e2u t\u1eeb LiDAR nhi\u1ec1u nh\u1ea5t c\u00f3 th\u1ec3. Tuy nhi\u00ean, Autonomous driving l\u00e0 nh\u1eefng b\u00e0i to\u00e1n c\u00f3 y\u00eau c\u1ea7u nghi\u00eam ng\u1eb7t v\u1ec1 th\u00f4ng l\u01b0\u1ee3ng v\u00e0 \u0111\u1ed9 tr\u1ec5 mong \u0111\u1ee3i t\u1eeb c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc s\u00e2u. H\u00f4m nay, ch\u00fang t\u00f4i mu\u1ed1n tr\u1ea3 l\u1eddi c\u00e2u h\u1ecfi: \u201cKh\u1ea3 n\u0103ng tri\u1ec3n khai th\u1ef1c t\u1ebf c\u1ee7a m\u00f4 h\u00ecnh MDE n\u00e0y nh\u01b0 th\u1ebf n\u00e0o?\u201d </p> <p>TensorRT l\u00e0 m\u1ed9t m\u1ed9t th\u01b0 vi\u1ec7n \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n b\u1edfi NVIDIA nh\u1eb1m c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 inference, gi\u1ea3m \u0111\u1ed9 tr\u00ec tr\u1ec7 tr\u00ean c\u00e1c thi\u1ebft b\u1ecb s\u1eed d\u1ee5ng card \u0111\u1ed3 h\u1ecda NVIDIA. V\u1edbi s\u1ef1 h\u1ed7 tr\u1ee3 cho m\u1ecdi framework nh\u01b0 Pytorch, Tensorflow, TensorRT gi\u00fap x\u1eed l\u00fd l\u01b0\u1ee3ng l\u1edbn d\u1eef li\u1ec7u v\u1edbi \u0111\u1ed9 tr\u1ec5 th\u1ea5p th\u00f4ng qua c\u00e1c t\u00ednh n\u0103ng t\u1ed1i \u01b0u h\u00f3a m\u1ea1nh m\u1ebd, gi\u1ea3m \u0111\u1ed9 ch\u00ednh x\u00e1c v\u00e0 s\u1eed d\u1ee5ng b\u1ed9 nh\u1edb hi\u1ec7u qu\u1ea3.</p> <p>Trong b\u00e0i vi\u1ebft n\u00e0y, ch\u00fang t\u00f4i s\u1ebd t\u0103ng t\u1ed1c \u0111\u1ed9 th\u1ef1c thi m\u00f4 h\u00ecnh MDE tr\u00ean TensorRT. C\u1ee5 th\u1ec3, ch\u00fang t\u00f4i so s\u00e1nh nhi\u1ec1u ti\u00eau ch\u00ed kh\u00e1c nhau gi\u1eefa Pytorch v\u00e0 TensorRT qua c\u00e1c ti\u00eau ch\u00ed nh\u01b0: Sai s\u1ed1 so v\u1edbi LiDAR, t\u1ed1c \u0111\u1ed9 th\u1ef1c thi, v\u00e0 k\u1ebft qu\u1ea3 tr\u1ef1c quan. K\u1ebft qu\u1ea3 th\u00ed nghi\u1ec7m cho th\u1ea5y TensorRT t\u0103ng t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd 2-2.5 l\u1ea7n so v\u1edbi PyTorch trong khi v\u1eabn c\u00f3 \u0111\u1ed9 l\u1ed7i t\u01b0\u01a1ng \u0111\u01b0\u01a1ng. Tuy nhi\u00ean, k\u1ebft qu\u1ea3 tr\u1ef1c quan cho th\u1ea5y \u0111\u1ed9 s\u00e2u c\u1ee7a TensorRT l\u1ea1i xu\u1ea5t hi\u1ec7n nh\u1eefng v\u00f9ng kh\u00e1c th\u01b0\u1eddng v\u00e0 c\u1ea7n \u0111\u01b0\u1ee3c ph\u00e2n t\u00edch k\u1ef9 h\u01a1n. </p> <p>T\u00f3m l\u1ea1i, trong b\u00e0i vi\u1ebft n\u00e0y, ch\u00fang t\u00f4i s\u1ebd: </p> <ul> <li>Gi\u1edbi thi\u1ec7u b\u00e0i to\u00e1n Monocular Depth Estimation.</li> <li>C\u00e1c b\u01b0\u1edbc chuy\u1ec3n \u0111\u1ed5i m\u00f4 h\u00ecnh Pytorch v\u1ec1 TensorRT.</li> <li>Th\u00ed nghi\u1ec7m chi ti\u1ebft \u0111\u1ed9 hi\u1ec7u qu\u00e1 Pytorch v\u00e0 TensorRT tr\u00ean m\u00e1y ch\u1ee7 RTX2080TI.</li> </ul>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#1-monocular-depth-estimation","title":"1. Monocular Depth Estimation","text":""},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#11-kien-truc-mo-hinh","title":"1.1. Ki\u1ebfn tr\u00fac m\u00f4 h\u00ecnh","text":"<p>Ch\u00fang t\u00f4i s\u1eed d\u1ee5ng ki\u1ebfn tr\u00fac m\u00f4 h\u00ecnh Encoder - Decoder. Gi\u1ed1ng nh\u01b0 Monodepth2, Encoder \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng b\u00e0i n\u00e0y l\u00e0 m\u00f4 h\u00ecnh Resnet. C\u00e1c block c\u01a1 b\u1ea3n c\u1ee7a Resnet bao g\u1ed3m c\u00e1c ph\u00e9p t\u00edch ch\u1eadp 3x3, BatchNorm, h\u00e0m k\u00edch ho\u1ea1t ReLU. T\u01b0\u01a1ng t\u1ef1, Decoder c\u0169ng bao g\u1ed3m c\u00e1c ph\u00e9p t\u00edch ch\u1eadp, h\u00e0m k\u00edch ho\u1ea1t ELU, v\u00e0 ph\u00e9p Upsampling. V\u1edbi th\u00f4ng tin to\u00e0n c\u1ee5c \u0111\u01b0\u1ee3c tr\u00edch xu\u1ea5t t\u1eeb Encoder, Decoder m\u00e3 h\u00f3a v\u00e0 suy lu\u1eadn th\u00f4ng ra th\u00f4ng tin \u0111\u1ed9 s\u00e2u. \u0110\u1ea7u ra c\u1ee7a DepthNet c\u00f3 gi\u00e1 tr\u1ecb trong kho\u1ea3ng [0, 1] (h\u00e0m Sigmoid) bi\u1ebfn \u0111\u1ed5i v\u00e0 scale v\u1edbi h\u1eb1ng s\u1ed1 \u0111\u1ec3 c\u00f3 gi\u00e1 tr\u1ecb depth. Ki\u1ebfn tr\u00fac chi ti\u1ebft m\u00f4 h\u00ecnh DepthNet \u0111\u01b0\u1ee3c m\u00f4 t\u1ea3 nh\u01b0 h\u00ecnh d\u01b0\u1edbi. B\u1ed9 tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng kh\u00f4ng ch\u1ec9 gi\u1edbi h\u1ea1n \u1edf ResNet m\u00e0 c\u00f2n c\u00f3 th\u1ec3 l\u00e0 c\u00e1c m\u00f4 h\u00ecnh m\u1ea1nh m\u1ebd h\u01a1n v\u00ed d\u1ee5 nh\u01b0 Transformer.</p> <p></p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#12-tap-du-lieu","title":"1.2. T\u1eadp d\u1eef li\u1ec7u","text":"<p>Ch\u00fang t\u00f4i s\u1eed d\u1ee5ng t\u1eadp d\u1eef li\u1ec7u Oxford Robotcar. \u0110\u00e2y l\u00e0 t\u1eadp d\u1eef li\u1ec7u kinh \u0111i\u1ec3n cho b\u00e0i to\u00e1n Autonomous driving, \u0111\u01b0\u1ee3c thu th\u1eadp \u1edf nhi\u1ec1u \u0111i\u1ec1u ki\u1ec7n th\u1eddi ti\u1ebft kh\u00e1c nhau, v\u1edbi th\u1eddi gian thu th\u1eadp h\u01a1n m\u1ed9t n\u0103m t\u1ea1i th\u00e0nh ph\u1ed1 Oxford. D\u1eef li\u1ec7u c\u00f3 th\u00f4ng tin c\u1ee7a nhi\u1ec1u sensors nh\u01b0 LiDAR, RADAR, GPS \u0111\u1ec3 h\u1ed7 tr\u1ee3 nhi\u1ec1u b\u00e0i to\u00e1n nghi\u00ean c\u1ee9u. Ch\u00fang t\u00f4i s\u1eed d\u1ee5ng \u1ea3nh RGB v\u00e0 LiDAR l\u1ea7n l\u01b0\u1ee3t l\u00e0m \u0111\u1ea7u v\u00e0o v\u00e0 ground truth \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh. T\u1eadp \u0111\u00e1nh gi\u00e1 \u0111\u01b0\u1ee3c ch\u00fang t\u00f4i tr\u00ecnh b\u00e0y trong \u1ea3nh d\u01b0\u1edbi</p> <p></p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#13-tieu-chi-anh-gia","title":"1.3. Ti\u00eau ch\u00ed \u0111\u00e1nh gi\u00e1","text":"<p>V\u1edbi \u0111\u1ed9 s\u00e2u \u01b0\u1edbc l\u01b0\u1ee3ng t\u1eeb m\u00f4 h\u00ecnh \\(\\bold{d}\\) v\u00e0 ground truth t\u1eeb LiDAR \\(\\bold{d}^*\\). Ch\u00fang t\u00f4i s\u1eed d\u1ee5ng ti\u00eau ch\u00ed Absolute error (Abs rel) v\u00e0 Root mean square error (RMSE) \u0111\u1ec3 \u0111\u00e1nh g\u00eda ch\u1ea5t l\u01b0\u1ee3ng \u0111\u1ed9 s\u00e2u:</p> \\[ abs\\_rel = \\frac{1}{|\\bold{d}|}\\sum |d^*-d|/d^* \\\\ rmse = \\sqrt{\\frac{1}{|\\bold{d}} \\sum||d^* - d||^2/d^*} \\]"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#2-chuyen-oi-pytorch-sang-tensorrt","title":"2. Chuy\u1ec3n \u0111\u1ed5i PyTorch sang TensorRT","text":"<p>C\u00f3 nhi\u1ec1u c\u00e1ch \u0111\u1ec3 chuy\u1ec3n \u0111\u1ed5i m\u00f4 h\u00ecnh PyTorch sang TensorRT. Trong b\u00e0i n\u00e0y, ch\u00fang t\u00f4i s\u1ebd s\u1eed d\u1ee5ng th\u01b0 vi\u00ean torch_tensorrt.  Torch-TensorRT l\u00e0 tr\u00ecnh bi\u00ean d\u1ecbch Ahead-of-Time (AOT), ngh\u0129a l\u00e0 tr\u01b0\u1edbc khi tri\u1ec3n khai m\u00e3 TorchScript, m\u00f4 h\u00ecnh s\u1ebd tr\u1ea3i qua m\u1ed9t b\u01b0\u1edbc bi\u00ean d\u1ecbch chuy\u1ec3n \u0111\u1ed5i ch\u01b0\u01a1ng tr\u00ecnh TorchScript ti\u00eau chu\u1ea9n th\u00e0nh m\u1ed9t m\u00f4-\u0111un \u0111\u01b0\u1ee3c h\u1ed7 tr\u1ee3 b\u1edfi TensorRT engine. Sau qu\u00e1 tr\u00ecnh bi\u00ean d\u1ecbch, m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng kh\u00f4ng kh\u00e1c g\u00ec m\u1ed9t TorchScript module.</p> <ol> <li> <p>Kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh v\u00e0 load pretrained weights cho m\u00f4 h\u00ecnh:</p> <pre><code>import networks\n\nclass DepthNet(nn.Module):\n    def __init__(self, encoder, decoder, min_depth = 0.1, max_depth=100, med_scale = 17.769):\n        super(DepthNet, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.min_depth = min_depth\n        self.max_depth = max_depth\n        self.med_scale = med_scale\n\n    def disp_to_depth(self, disp):\n        min_disp = 1 / self.max_depth\n        max_disp = 1 / self.min_depth\n        scaled_disp = min_disp + (max_disp - min_disp) * disp\n        depth = 1 / scaled_disp\n        return depth\n\n    def forward(self, input_image):\n        features = self.encoder(input_image)\n        disp = self.decoder(features)\n        depth = self.disp_to_depth(disp) * self.med_scale\n        return depth\n\n# Create Encoder, Decoder\nencoder = networks.ResnetEncoder(opt.num_layers, False)\ndecoder = networks.DepthDecoder(encoder.num_ch_enc)\n\n# Load weights\nencoder_path = os.path.join(opt.load_weights_folder, \"encoder.pth\")\ndecoder_path = os.path.join(opt.load_weights_folder, \"depth.pth\")\nencoder.load_state_dict(torch.load(encoder_path))\ndecoder.load_state_dict(torch.load(decoder_path))\n\n# DepthNet\nmodel = DepthNet(encoder, decoder)\nmodel.eval()\nmodel.cuda()\n</code></pre> </li> <li> <p>Chuy\u1ec3n m\u00f4 h\u00ecnh nn.Module qua TensorRT</p> <ul> <li>inputs: Trong \u0111\u00f3, ch\u00fang t\u00f4i thi\u1ebft l\u1eadp TensorRT c\u00f3 th\u1ec3 nh\u1eadn \u0111\u1ea7u v\u00e0o c\u00f3 nhi\u1ec1u shape kh\u00e1c nhau b\u1eb1ng c\u00e1ch th\u00eam 3 tham s\u1ed1 sau cho torch_tensorrt.Input:<ul> <li>min_shape: K\u00edch th\u01b0\u1edbc nh\u1ecf nh\u1ea5t m\u00e0 TensorRT s\u1ebd t\u1ed1i \u01b0u.</li> <li>opt_shape: TensorRT s\u1ebd t\u1eadp trung t\u1ed1i \u01b0u d\u1ef1a tr\u00ean k\u00edch th\u01b0\u1edbc n\u00e0y.</li> <li>max_shape: K\u00edch th\u01b0\u1edbc l\u1edbn nh\u1ea5t m\u00e0 TensorRT s\u1ebd t\u1ed1i \u01b0u.</li> </ul> </li> </ul> <p>Theo document, ng\u01b0\u1eddi d\u00f9ng s\u1ebd c\u00f3 ch\u1ea5t l\u01b0\u1ee3ng \u0111\u1ea7u ra t\u1ed1t nh\u1ea5t trong kho\u1ea3ng shape n\u00e0y. </p> <p>B\u00ean c\u1ea1nh \u0111\u00f3, torch_tensorrt.Input c\u00f3 tham s\u1ed1 dtype \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o.</p> <ul> <li>enabled_precisions: Tham s\u1ed1 ch\u1ecdn \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh m\u00e0 TensoRT s\u1ebd t\u1ed1i \u01b0u. TensorRT h\u1ed7 tr\u1ee3 3 lo\u1ea1i \u0111\u1ed9 ch\u00ednh x\u00e1c l\u00e0 float-32, float-16, v\u00e0 int-8. B\u00ean c\u1ea1nh \u0111\u00f3, ng\u01b0\u1eddi d\u00f9ng c\u00f3 th\u1ec3 ch\u1ecdn option mixed-precision: TensorRT s\u1ebd t\u1ef1 \u0111\u1ed9ng ch\u1ecdn ki\u1ec3u d\u1eef li\u1ec7u \u0111\u1ec3 t\u1ed1i \u01b0u nh\u1ea5t. Trong b\u00e0i vi\u1ebft n\u00e0y, ch\u00fang t\u00f4i s\u1ebd th\u00ed nghi\u1ec7m 3 lo\u1ea1i \u0111\u1ed9 ch\u00ednh x\u00e1c sau:<ul> <li>Float-32: enabled_precisions = { torch.float }</li> <li>Float-16: enabled_precisions = { torch.half }</li> <li>Mixed-precision: enabled_precisions = { torch.float, torch.half }</li> </ul> </li> </ul> <pre><code>inputs = [\ntorch_tensorrt.Input(\n        min_shape=[1, 3, 128, 256],\n        opt_shape=[1, 3, 256, 512],\n        max_shape=[8, 3, 384, 768],\n        dtype=torch.float  # or torch.half\n    )]\nenabled_precisions = {torch.float, torch.half}   \ntrt_ts_module = torch_tensorrt.compile(\n        model, inputs=inputs, enabled_precisions=enabled_precisions\n    )\n# torch.jit.save(trt_ts_module, \"path/to/trt_ts.ts\")\n# trt_ts_module = torch.jit.load(\"path/to/trt_ts.ts\")\n</code></pre> </li> </ol> <p>B\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 xem source t\u1ea1i \u0111\u00e2y: TickLabVN/TensorRT-MDE</p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#3-thiet-lap","title":"3. Thi\u1ebft l\u1eadp","text":"<p>To\u00e0n b\u1ed9 qu\u00e1 tr\u00ecnh bi\u1ebft \u0111\u1ed5i \u1edf tr\u00ean v\u00e0 qu\u00e1 tr\u00ecnh benchmark \u0111\u01b0\u1ee3c ch\u00fang t\u00f4i th\u1ef1c hi\u1ec7n tr\u00ean server c\u00f3 c\u1ea5u h\u00ecnh b\u00ean d\u01b0\u1edbi. Ch\u00fang t\u00f4i s\u1eed d\u1ee5ng docker image nvcr.io/nvidia/pytorch:23.09-py3 cung c\u1ea5p b\u1edfi Nivida:</p> torch 2.1.0 TensorRT 8.6.1 CUDA version 12.2 GPU RTX 2080TI CPU Intel(R) Core(TM) i9-10900F CPU @ 2.80GHz <p>\u0110o\u1ea1n code ch\u00fang t\u00f4i th\u1ef1c hi\u1ec7n benchmark nh\u01b0 sau:</p> <pre><code># Reference:https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/\nimport torch\nimport torch_tensorrt\nimport time\nimport numpy as np\nimport torch.backends.cudnn as cudnn\n\ncudnn.benchmark = True\n\ndef benchmark(model, use_cuda = True, input_shape=(1, 3, 256, 512), dtype='fp32', nwarmup=50, nruns=1000):\n    input_data = torch.randn(input_shape)\n    if use_cuda:\n        input_data = input_data.to(\"cuda\")\n    if dtype=='fp16':\n        input_data = input_data.half()\n\n    print(\"Warm up ...\")\n    with torch.no_grad():\n        for _ in range(nwarmup):\n            features = model(input_data)\n    torch.cuda.synchronize()\n    print(\"Start timing ...\")\n    timings = []\n    with torch.no_grad():\n        for i in range(1, nruns+1):\n            start_time = time.time()\n            pred_loc  = model(input_data)\n            torch.cuda.synchronize()\n            end_time = time.time()\n            timings.append(end_time - start_time)\n            if i%10==0:\n                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n\n    print(\"Input shape:\", input_data.size())\n    print('Average throughput: %.2f images/second'%(input_shape[0]/np.mean(timings)))\n\n# Load model\ntrt_model = torch.jit.load(\"path/to/trt_ts.ts\")\nbenchmark(trt_model, \n                    use_cuda = True, \n                    input_shape=(1, 3, 256, 512), \n                    nruns=100, \n                    dtype=\"fp16\") # fp32\n</code></pre>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#4-ket-qua-thi-nghiem","title":"4. K\u1ebft qu\u1ea3 th\u00ed nghi\u1ec7m","text":""},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#41-ket-qua-inh-luong","title":"4.1. K\u1ebft qu\u1ea3 \u0111\u1ecbnh l\u01b0\u1ee3ng","text":""},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#411-anh-gia-chat-luong-o-sau-m","title":"4.1.1. \u0110\u00e1nh gi\u00e1 ch\u1ea5t l\u01b0\u1ee3ng \u0111\u1ed9 s\u00e2u (m)","text":"<p>batch-size = 1, resnet18</p> <p>S\u1ed1 li\u1ec7u cho th\u1ea5y vi\u1ec7c chuy\u1ec3n m\u00f4 h\u00ecnh t\u1eeb Pytorch sang TensorRT c\u00f3 thay \u0111\u1ed5i ch\u1ea5t l\u01b0\u1ee3ng \u0111\u1ed9 s\u00e2u nh\u01b0ng kh\u00f4ng \u0111\u00e1ng k\u1ec3.</p> Method abs_rel rmse Pytorch 0.132 5.345 TensorRT Float-16 0.132 5.317 TensorRT Float-32 0.132 5.321 TensorRT Mixed Precision 0.132 5.317"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#412-anh-gia-toc-o-thuc-thi-voi-kich-thuoc-au-vao-khac-nhau","title":"4.1.2. \u0110\u00e1nh gi\u00e1 t\u1ed1c \u0111\u1ed9 th\u1ef1c thi v\u1edbi k\u00edch th\u01b0\u1edbc \u0111\u1ea7u v\u00e0o kh\u00e1c nhau","text":"<p>batch-size = 1, resnet18 (images / seconds)</p> <p>\u0110\u1ed3 th\u1ecb cho th\u1ea5y TensorRT cho t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd \u1edf 3 thi\u1ebft l\u1eadp \u0111\u1ec1u nhanh h\u01a1n so v\u1edbi Pytorch, trong \u0111\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c Float-16 v\u00e0 mixed-precision t\u0103ng t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd t\u1eeb 2-2.5 l\u1ea7n. B\u00ean c\u1ea1nh \u0111\u00f3, thi\u1ebft l\u1eadp Float-16 c\u00f3 s\u1ed1 li\u1ec7u t\u1ed1t h\u01a1n Float-32 t\u1eeb 1.5 (256x512) t\u1edbi 2 l\u1ea7n (128x256, 384x768) ch\u1ee9ng t\u1ecf vi\u1ec7c t\u0103ng t\u1ed1c n\u1eb1m \u1edf vi\u1ec7c t\u1ed1i \u01b0u \u0111\u1ed9 ch\u00ednh x\u00e1c c\u00f2n float 16 bit.</p> <p></p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#413-anh-gia-toc-o-thuc-thi-khi-xu-ly-theo-batch","title":"4.1.3. \u0110\u00e1nh gi\u00e1 t\u1ed1c \u0111\u1ed9 th\u1ef1c thi khi x\u1eed l\u00fd theo batch","text":"<p>input-shape = 256x512, resnet18 (images / seconds)*</p> <p>\u0110\u1ed3 th\u1ecb cho th\u1ea5y t\u1ea5t c\u1ea3 thi\u1ebft l\u1eadp \u0111\u1ec1u cho th\u00f4ng l\u01b0\u1ee3ng t\u0103ng khi t\u0103ng batch size, trong \u0111\u00f3 s\u1ed1 li\u1ec7u c\u1ea3i thi\u1ec7n v\u01b0\u1ee3t tr\u1ed9i khi thay \u0111\u1ed5i batch size l\u00ean 16, v\u00e0 t\u0103ng nh\u1eb9 cho batchsize t\u1eeb 16 tr\u1edf \u0111i. TensorRT \u1edf \u0111\u1ed9 ch\u1ec9nh x\u00e1c float-16 v\u00e0 mixed precision \u0111\u1ec1u c\u00f3 b\u0103ng th\u00f4ng cao g\u1ea5p 2.5-3 l\u1ea7n so v\u1edbi Pytorch.</p> <p></p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#414-anh-gia-toc-o-thuc-thi-tren-nhieu-encoder-khac-nhau","title":"4.1.4. \u0110\u00e1nh gi\u00e1 t\u1ed1c \u0111\u1ed9 th\u1ef1c thi tr\u00ean nhi\u1ec1u Encoder kh\u00e1c nhau","text":"<p>batch-size=1, input-shape = 256x512 (images / seconds)</p> <p>T\u01b0\u01a1ng t\u1ef1 nh\u01b0 vi\u1ec7c thay \u0111\u1ed5i k\u00edch th\u01b0\u1edbc \u0111\u1ea7u v\u00e0o,  TensorRT cho t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd \u1edf 3 thi\u1ebft l\u1eadp \u0111\u1ec1u nhanh h\u01a1n so v\u1edbi Pytorch, trong \u0111\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c Float-16 v\u00e0 mixed-precision t\u0103ng t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd t\u1eeb 2-2.5 l\u1ea7n</p> <p></p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#42-ket-qua-inh-tinh","title":"4.2. K\u1ebft qu\u1ea3 \u0111\u1ecbnh t\u00ednh","text":"<p>K\u1ebft qu\u1ea3 tr\u1ef1c quan cho th\u1ea5y depth map d\u1ef1 \u0111o\u1ea1n b\u1edfi TensorRT xu\u1ea5t hi\u1ec7n nh\u1eefng v\u00f9ng b\u1ea5t th\u01b0\u1eddng. Nh\u1eefng v\u00f9ng n\u00e0y xu\u1ea5t hi\u1ec7n \u1edf c\u1ea3 \u0111\u1ed9 ch\u00ednh x\u00e1c Float-32 v\u00e0 Float-16 ch\u1ee9ng t\u1ecf l\u1ed7i do c\u00e1ch k\u1ebft h\u1ee3p c\u00e1c ph\u00e9p t\u00ednh to\u00e1n c\u1ee7a TensorRT.</p> <p></p> <p></p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#5-ket-luan","title":"5. K\u1ebft lu\u1eadn","text":"<p>Trong b\u00e0i vi\u1ebft n\u00e0y, ch\u00fang t\u00f4i \u0111\u00e3 gi\u1edbi thi\u1ec7u b\u00e0i to\u00e1n Monocular Depth Estimation (MDE) l\u00e0 b\u00e0i to\u00e1n s\u1eed d\u1ee5ng m\u1ea1ng h\u1ecdc s\u00e2u \u0111\u1ec3 \u01b0\u1edbc l\u01b0\u1ee3ng depth d\u1ef1a tr\u00ean 1 \u1ea3nh. M\u1ed9t m\u1ea1ng h\u1ecdc s\u00e2u ki\u1ebfn tr\u00fac Encoder-Decoder nh\u1eadn \u0111\u1ea7u l\u00e0 \u1ea3nh v\u00e0 \u0111\u1ea7u ra l\u00e0 Depth map ch\u01b0a \u0111\u01b0\u1ee3c scale.</p> <p>\u0110\u1ec3 t\u0103ng t\u1ed1c \u0111\u1ec3 x\u1eed l\u00fd cho m\u00f4 h\u00ecnh, ch\u00fang t\u00f4i chuy\u1ec3n \u0111\u1ed5i m\u00f4 h\u00ecnh t\u1eeb d\u1ea1ng Pytorch sang TensorRT. Ch\u00fang t\u00f4i \u0111\u00e3 tr\u00ecnh b\u00e0y qu\u00e1 tr\u00ecnh chuy\u1ec3n \u0111\u1ed5i n\u00e0y s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n torch_tensorrt, c\u0169ng nh\u01b0 c\u00e1c tham s\u1ed1 t\u1ed1i \u01b0u. </p> <p>Ch\u00fang t\u00f4i \u0111\u00e3 tr\u00ecnh b\u00e0y k\u1ebft qu\u1ea3 so s\u00e1nh s\u00e2u v\u1ec1 b\u0103ng th\u00f4ng x\u1eed l\u00fd gi\u1eefa Pytorch v\u00e0 TensorRT. K\u1ebft qu\u1ea3 cho th\u1ea5y TensorRT c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 t\u1eeb 2-2.5 l\u1ea7n so v\u1edbi Pytorch \u1edf thi\u1ebft l\u1eadp Float-16 v\u00e0 Mixed precision.</p> <p>Trong ph\u1ea7n t\u1edbi, ch\u00fang t\u00f4i s\u1ebd tr\u00ecnh b\u00e0y c\u00e1ch tri\u1ec3n khai m\u00f4 h\u00ecnh MDE l\u00ean Jetson Nano v\u00e0 t\u0103ng t\u1ed1c \u0111\u1ed9 x\u1eed l\u00fd b\u1eb1ng TensorRT.</p>"},{"location":"blog/accelerating-monocular-depth-estimation-using-tensorrt/#6-tham-khao","title":"6. Tham kh\u1ea3o","text":"<p>[1] https://medium.com/@zergtant/accelerating-model-inference-with-tensorrt-tips-and-best-practices-for-pytorch-users-7cd4c30c97bc</p> <p>[2] https://developer.nvidia.com/blog/accelerating-inference-up-to-6x-faster-in-pytorch-with-torch-tensorrt/</p> <p>[3] https://pytorch.org/TensorRT/</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/","title":"COLMAP: A software for 3D Spare &amp; Dense Reconstruction","text":"<p>X\u00e2y d\u1ef1ng l\u1ea1i c\u1ea5u tr\u00fac 3D l\u00e0 m\u1ed9t trong c\u00e1c \u1ee9ng d\u1ee5ng quan tr\u1ecdng trong Th\u1ecb gi\u00e1c m\u00e1y t\u00ednh. V\u1edbi \u0111\u1ea7u v\u00e0o l\u00e0 m\u1ed9t lo\u1ea1i c\u00e1c b\u1ee9c \u1ea3nh ch\u1ee5p xung quanh m\u1ed9t \u0111\u1ed1i t\u01b0\u1ee3ng, y\u00eau c\u1ea7u \u0111\u1ea7u ra l\u00e0 h\u00ecnh \u1ea3nh kh\u00f4ng gian 3D c\u1ee7a v\u1eadt th\u1ec3. Qu\u00e1 tr\u00ecnh x\u00e2y d\u01b0ng n\u00e0y y\u00eau c\u1ea7u th\u1ef1c hi\u1ec7n nhi\u1ec1u b\u01b0\u1edbc v\u00e0 ph\u1ee9c t\u1ea1p. Trong ph\u1ea7n n\u00e0y, ch\u00fang t\u00f4i s\u1ebd gi\u1edbi thi\u1ec7u c\u00e1c b\u1ea1n ph\u1ea7n m\u1ec1m COLMAP - ph\u1ea7n m\u1ec1m \u0111a d\u1ee5ng gi\u00fap b\u1ea1n d\u1ec5 d\u00e0ng x\u00e2y d\u1ef1ng kh\u00f4ng gian 3D c\u1ee7a \u0111\u1ed5i t\u01b0\u1ee3ng. Qua b\u00e0i vi\u1ebft, ch\u00fang t\u00f4i hi v\u1ecdng b\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 t\u00ecm th\u1ea5y \u0111\u01b0\u1ee3c ngu\u1ed3n c\u1ea3m h\u1ee9ng m\u1edbi t\u1eeb c\u00e1c b\u00e0i to\u00e1n th\u1ecb gi\u00e1c m\u00e1y t\u00ednh. </p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#1-tap-du-lieu-va-cac-buoc-thuc-hien","title":"1. T\u1eadp d\u1eef li\u1ec7u v\u00e0 c\u00e1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n","text":"<p>COLMAP x\u00e2y d\u1ef1ng kh\u00f4ng gian v\u1eadt th\u1ec3 qua c\u00e1c b\u01b0\u1edbc sau: 1) Feature Detection, COLMAP s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n SIFT \u0111\u1ec3 tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng c\u1ee7a \u1ea3nh. 2) Feature Matching, x\u00e9t l\u1ea7n l\u01b0\u1ee3t t\u1eebng \u1ea3nh, COLMAP s\u1ebd t\u00ecm t\u1eadp c\u00e1c \u1ea3nh c\u00f3 ph\u1ea7n \u1ea3nh tr\u00f9ng kh\u1edbp v\u1edbi \u1ea3nh \u0111ang x\u00e9t. 3) Spare Reconstruction, s\u1ef1 d\u1ee5ng c\u00e1c \u1ea3nh \u0111\u00e3 bi\u1ebft c\u00f3 v\u00f9ng tr\u00f9ng kh\u1edbp nhau \u1edf b\u01b0\u1edbc tr\u01b0\u1edbc, COLMAP gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n Triangulation \u0111\u1ec3 t\u00ecm ra \u0111\u1ed9 s\u00e2u \u1ea3nh, t\u1eeb \u0111\u00f3 x\u00e2y d\u1ef1ng \u0111\u01b0\u1ee3c kh\u00f4ng gian th\u01b0a. 4) Dense Reconstruction: X\u00e2y d\u1ef1ng kh\u00f4ng gian 3D d\u00e0y \u0111\u1eb7c t\u1eeb kh\u00f4ng gian th\u01b0a.</p> <p>Trong ph\u1ea7n tr\u00ecnh b\u00e0y n\u00e0y, ch\u00fang t\u00f4i s\u1eed d\u1ee5ng d\u1eef li\u1ec7u South Building, g\u1ed3m 128 b\u1ee9c \u1ea3nh k\u00edch th\u01b0\u1edbc l\u1edbn chung xung quanh t\u00f2a nh\u00e0 nh\u01b0 h\u00ecnh d\u01b0\u1edbi. Ch\u00fang t\u00f4i s\u1ebd s\u1eed d\u1ee5ng ph\u1ea7n m\u1ec1m Colmap \u0111\u1ec3 x\u00e2y d\u1ef1ng l\u1ea1i kh\u00f4ng gian 3D d\u00e0y \u0111\u1eb7c nh\u01b0 h\u00ecnh b\u00ean ph\u1ea3i:</p> <p> </p> <p>H\u00ecnh 1. D\u1eef li\u1ec7u South Building \u0111\u1ec3 x\u00e2y d\u1ef1ng kh\u00f4ng 3D.   </p> <p> </p> <p>H\u00ecnh 2: K\u1ebft qu\u1ea3 kh\u00f4ng gian d\u00e0y \u0111\u1eb7c x\u00e2y d\u1ef1ng t\u1eeb Colmap   </p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#2-feature-detection","title":"2. Feature Detection","text":"<p>Thu\u1eadt to\u00e1n tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng m\u1eb7c \u0111\u1ecbnh c\u1ee7a COLMAP l\u00e0 SIFT. Do \u0111\u00f3, ch\u00fang t\u00f4i s\u1ebd gi\u1edbi thi\u1ec7u thu\u1eadt to\u00e1n n\u00e0y tr\u01b0\u1edbc khi tr\u00ecnh b\u00e0y k\u1ebft qu\u1ea3 t\u1eeb COLMAP.</p> <p>Scale-Invariant Feature Transform (SIFT) l\u00e0 thu\u1eadt to\u00e1n tr\u00edch xu\u1ea5t \u1ea3nh c\u1ee5c b\u1ed9 \u0111\u01b0\u1ee3c \u0111\u1ec1 xu\u1ea5t b\u1edfi nh\u00e0 nghi\u00ean c\u1ee9u David Lowe trong th\u1ecb gi\u00e1c m\u00e1y t\u00ednh. Thu\u1eadt to\u00e1n ho\u1ea1t \u0111\u1ed9ng v\u1edbi k\u1ebft qu\u1ea3 \u1ed5n \u0111\u1ecbnh v\u1edbi nh\u1eefng scale c\u1ee7a \u1ea3nh kh\u00e1c nhau, b\u00ean c\u1ea1nh \u0111\u00f3 c\u0169ng c\u00f3 th\u1ec3 n\u00f3i gi\u1ea3i thu\u1eadt n\u00e0y c\u00f3 t\u00ednh rotation-invariant. Gi\u1ea3i thu\u1eadt \u0111\u01b0\u1ee3c \u1ee9ng d\u1ee5ng r\u1ed9ng r\u00e3i trong c\u00e1c b\u00e0i to\u00e1n Object recognition, Image stitching,.... Thu\u1eadt to\u00e1n n\u00e0y bao g\u1ed3m c\u00e1c b\u01b0\u1edbc sau:</p> <ol> <li> <p>Scale-space Extrema Detection: \u1ede b\u01b0\u1edbc n\u00e0y, thu\u1eadt to\u00e1n t\u00ecm ra v\u1ecb tr\u00ed nh\u1eefng \u0111i\u1ec3m mang th\u00f4ng tin quan tr\u1ecdng trong b\u1ee9c \u1ea3nh b\u1eb1ng b\u1ed9 l\u1ecdc Laplacian of Gaussian (LoG). Tuy nhi\u00ean nh\u1eefng \u0111\u1eb7c tr\u01b0ng n\u00e0y c\u00f3 th\u1ec3 b\u1ecb thay \u0111\u1ed5i cho nh\u1eefng \u1ea3nh c\u00f3 \u0111\u1ed9 co d\u00e3n kh\u00e1c nhau . Do \u0111\u00f3 t\u00e1c gi\u1ea3 \u0111\u1ec1 xu\u1ea5t s\u1eed d\u1ee5ng c\u00f9ng l\u00fac nhi\u1ec1u b\u1ed9 l\u1ecdc c\u00f3 k\u00edch th\u01b0\u1edbc kh\u00e1c nhau th\u1ef1c hi\u1ec7n tr\u00ean \u1ea3nh \u0111\u01b0\u1ee3c thay \u0111\u1ed5i k\u00edch th\u01b0\u1edbc kh\u00e1c nhau. B\u00ean c\u1ea1nh \u0111\u00f3, v\u00ec b\u1ed9 l\u1ecdc LoC t\u00ednh to\u00e1n ph\u1ee9c t\u1ea1p, t\u00e1c gi\u1ea3 \u0111\u1ec1 xu\u1ea5t ph\u00e9p x\u1ea5p x\u1ec9 b\u1ed9 l\u1ecdc n\u00e0y l\u00e0 ph\u00e9p Difference of Gaussian: L\u1ea5y hi\u1ec7u c\u1ee7a hai ph\u00e9p Gaussian \u0111\u1ed9 l\u1edbn li\u00ean ti\u1ebfp. Thu\u1eadt to\u00e1n ti\u1ebfp t\u1ee5c th\u1ef1c hi\u1ec7n m\u1ed9t ph\u00e9p ph\u00e9p l\u1ecdc d\u1ef1 li\u1ec7u \u0111\u1ec3 c\u00f3 k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng.      <p>  H\u00ecnh 3: H\u00ecnh mi\u00eau t\u1ea3 b\u01b0\u1edbc Scale-space Extrema Detection \u0111\u1ec3 t\u00ecm v\u1ecb tr\u00ed c\u00f3 th\u00f4ng tin quan tr\u1ecdng. \u1ede t\u1eebng scale, SIFT s\u1ebd x\u1ea5p x\u1ec9 LoG b\u1eb1ng c\u00e1ch t\u00ecnh hi\u1ec7u gi\u1eefa k\u1ebft qu\u1ea3 c\u00e1c Gaussian Kernel kh\u00e1c nhau.  </p> <li> <p>Keypoint Localization: \u1ede b\u01b0\u1edbc n\u00e0y, t\u00e1c gi\u1ea3 \u0111\u1ec1 xu\u1ea5t c\u00e1ch tinh ch\u1ec9nh v\u1ecb tr\u00ed c\u00e1c \u0111i\u1ec3m quan tr\u1ecdng \u1edf b\u01b0\u1edbc tr\u00ean t\u1ed1t h\u01a1n. H\u01a1n n\u1eefa, t\u00e1c gi\u1ea3 c\u0169ng lo\u1ea1i b\u1ecf nh\u01b0ng \u0111i\u1ec3m quan tr\u1ecdng n\u00e0o \u0111\u01b0\u1ee3c nghi ng\u1edd l\u00e0 c\u1ea1nh \u1edf b\u01b0\u1edbc n\u00e0y.</p> </li> <li> <p>Orientation assignment: B\u00ean c\u1ea1nh v\u1ea5n \u0111\u1ec1 co d\u00e3n, \u0111\u1eb7c tr\u01b0ng c\u1ee7a m\u1ed9t \u0111i\u1ec3m c\u00f3 th\u1ec3 kh\u00e1c nhau khi th\u1ef1c hi\u1ec7n ph\u00e9p xoay. \u0110\u1ec3 tr\u00e1nh t\u00ecnh tr\u1ea1ng n\u00e0y, t\u00e1c gi\u1ea3 \u0111\u1ec1 xu\u1ea5t t\u00ednh h\u01b0\u1edbng xoay c\u1ee7a \u0111\u1eb7c tr\u01b0ng. Nh\u1edd \u0111\u00f3, hai \u0111\u1eb7c tr\u01b0ng s\u1ebd \u0111\u01b0\u1ee3c xoay c\u00f9ng v\u1ec1 1 g\u00f3c tr\u01b0\u1edbc khi \u0111\u01b0\u1ee3c so s\u00e1nh. T\u00e1c gi\u1ea3 l\u1ea5y h\u01b0\u1edbng chi\u1ebfm \u0111a s\u1ed1 c\u1ee7a t\u1ea5t c\u1ea3 pixel trong \u00f4 vu\u00f4ng 16x16 xung quanh \u0111i\u1ec3m \u0111\u1eb7c tr\u01b0ng.</p> <p>  H\u00ecnh 4: H\u00ecnh mi\u00eau t\u1ea3 b\u01b0\u1edbc Orientation assignment. V\u1edbi m\u1ed7i \u0111i\u1ec3m keypoint, SIFT s\u1ebd t\u00ednh gradient c\u1ee7a v\u00f9ng 16x16 xung quanh (Trong h\u00ecnh ch\u1ec9 minh ho\u1ea1t 8 \u00f4) \u0111\u1ec3 x\u00e2y d\u1ef1ng bi\u1ec3u \u0111\u1ed3 histogram bi\u1ec3u di\u1ec5n h\u01b0\u1edbng \u1edf b\u00ean ph\u1ea3i. H\u01b0\u1edbng chi\u1ebfm \u0111a s\u1ed1 (c\u1ed9t cao nh\u1ea5t) ch\u00ednh l\u00e0 h\u01b0\u1edbng \u0111\u1ea1i di\u1ec7n cho keypoint.  </p> <li> <p>Keypoint Descriptor: SIFT m\u00f4 t\u1ea3 \u0111\u1eb7c tr\u01b0ng c\u1ee7a \u0111i\u1ec3m quan tr\u1ecdng b\u1eb1ng vector 128 chi\u1ec1u. X\u00e9t v\u00f9ng 16x16 xung quanh \u0111i\u1ec3m n\u00e0y, SIFT chia v\u00f9ng n\u00e0y th\u00e0nh 4 \u00f4 vu\u00f4ng 4x4 li\u1ec1n k\u1ec1 kh\u00f4ng trung nhau. M\u1ed7i \u00f4 vu\u00f4ng, SIFT t\u00ecnh \u0111\u1ed3 th\u00ec histogram theo h\u01b0\u1edbng chia l\u00e0m 8 chi\u1ec1u, m\u1ed7i chi\u1ec1u bi\u1ec3u di\u1ec5n g\u00f3c 45 \u0111\u1ed9. Histogram c\u1ee7a 4 v\u00f9ng n\u00e0y h\u1ee3p l\u1ea1i th\u00e0nh vector \\(4 \\times 4 \\times 8  = 128\\) chi\u1ec1u.</p> <p>  H\u00ecnh 5: H\u00ecnh minh ho\u1ea1t c\u00e1ch tr\u00edch xu\u1ea5t Keypoint Descriptor c\u1ee7a SIFT.  </p> <p>  H\u00ecnh 6: K\u1ebft qu\u1ea3 Feature Detection s\u1eed d\u1ee5ng SIFT t\u1eeb COLMAP. Nh\u1eefng \u0111i\u1ec3m m\u00e0u \u0111\u1ecf l\u00e0 nh\u1eefng \u0111i\u1ec3m keypoints.  </p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#3-feature-matching","title":"3. Feature Matching","text":"<p>\u1ede b\u01b0\u1edbc n\u00e0y, COLMAP s\u1ebd s\u1eed d\u1ee5ng \u0111\u1eb7c tr\u01b0ng tr\u00edch xu\u1ea5t \u0111\u01b0\u1ee3c t\u1eeb b\u01b0\u1edbc Feature Detection \u0111\u1ec3 t\u00ecm ra c\u00e1c t\u1ea5m \u1ea3nh c\u00f3 v\u00f9ng tr\u00f9ng nhau. Colmap cung c\u1ea5p nhi\u1ec1u thu\u1eadt to\u00e1n matching kh\u00e1c nhau bao g\u1ed3m:</p> <ul> <li>Exhaustive matching: Thu\u1eadt to\u00e1n ph\u00f9 h\u1ee3p v\u1edbi d\u1eef li\u1ec7u \u00edt. M\u1ed7i t\u1ea5m \u1ea3nh s\u1ebd \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n v\u1edbi T\u1ea4T C\u1ea2 t\u1ea5m \u1ea3nh trong kho d\u1eef li\u1ec7u. Thu\u1eadt to\u00e1n n\u00e0y cho \u0111\u1ed9 ch\u00ednh x\u00e1c cao, nh\u01b0ng l\u1ea1i c\u00f3 chi ph\u00ed t\u00ednh to\u00e1n l\u1edbn.</li> <li>Sequential Matching: Ph\u00f9 h\u1ee3p khi d\u1eef li\u1ec7u bi\u1ebft tr\u01b0\u1edbc l\u00e0 c\u00f3 th\u1ee9 t\u1ef1.</li> <li>Vocabulary Tree Matching: Thu\u1eadt to\u00e1n ph\u1ed5 bi\u1ebfn nh\u1ea5t, c\u00f3 t\u1ed1c \u0111\u1ed9 th\u1ef1c thi nhanh v\u1edbi t\u1eadp d\u1eef li\u1ec7u l\u1edbn.</li> <li>Spatial Matching: Ph\u00f9 h\u1ee3p khi m\u1ed7i sample c\u00f3 th\u00f4ng tin t\u1ecda \u0111\u1ed9. Colmap s\u1ebd l\u1ea5y nh\u1eefng \u1ea3nh c\u00f3 th\u00f4ng tin n\u00e0y g\u1ea7n nhau \u0111\u1ec3 matching.</li> <li>Transitive Matching: Thu\u1eadt to\u00e1n n\u00e0y ph\u00f9 h\u1ee3p khi d\u1eef li\u1ec7u \u0111\u00e3 c\u00f3 th\u00f4ng tin m\u1ed9t v\u00e0i \u1ea3nh \u0111\u00e3 \u0111\u01b0\u1ee3c match v\u1edbi nhau.</li> <li>Custom Matching</li> </ul> <p>Trong ph\u1ea7n n\u00e0y, ch\u00fang t\u00f4i s\u1ebd s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n Vocabulary Tree Matching. Ch\u00fang t\u00f4i \u0111\u1ea7u ti\u00ean s\u1ebd tr\u00ecnh b\u00e0y nhanh c\u00e1ch t\u1ea1o Vocabulary Tree, v\u00e0 d\u00f9ng c\u00e2y n\u00e0y \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n Image Retrieval. Sau \u0111\u00f3, ch\u00fang t\u00f4i s\u1ebd tr\u00ecnh b\u00e0y k\u1ebft qu\u1ea3 t\u1eeb Colmap.</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#vocabulary-tree","title":"Vocabulary Tree","text":"<p>Vocabulary Tree \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng b\u1eb1ng c\u00e1ch ph\u00e2n c\u1ef1m t\u1eebng t\u1ea7ng kh\u00f4ng gian \u0111\u1eb7c tr\u01b0ng t\u1eeb t\u1eadp d\u1eef li\u1ec7u \u1ea3nh hu\u1ea5n luy\u1ec7n. C\u1ee5 th\u1ec3, Vocabulary Tree \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng t\u1eebng b\u01b0\u1edbc nh\u01b0 sau: </p> <ol> <li>Cho t\u1eadp d\u1eef li\u1ec7u \u1ea3nh hu\u1ea5n luy\u1ec7n ,t\u1eadp d\u1eef li\u1ec7u c\u00f3 th\u1ec3 l\u00e0 t\u1eadp \u1ea3nh d\u00f9ng \u0111\u1ec3 x\u00e2y d\u1ef1ng l\u1ea1i c\u1ea3nh hi\u1ec7n t\u1ea1i, ho\u1eb7c t\u1eadp d\u1eef li\u1ec7u kh\u00e1c (vd ImageNet). To\u00e0n b\u1ed9 b\u1ee9c \u1ea3nh n\u00e0y s\u1ebd bi\u1ebfn \u0111\u1ed5i v\u00e0o kh\u00f4ng gian \u0111\u1eb7c tr\u01b0ng b\u1eb1ng thu\u1eadt to\u00e1n, vd SIFT detection.</li> <li>V\u1edbi kh\u00f4ng gian \u0111\u1eb7c tr\u01b0ng \u0111\u01b0\u1ee3c tr\u00edch xu\u1ea5t, thu\u1eadt to\u00e1n gom c\u1ee5m ph\u00e2n t\u1ea7ng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng v\u00e0 nh\u00f3m c\u00e1c \u0111\u1eb7c tr\u01b0ng gi\u1ed1ng nhau l\u1ea1i th\u00e0nh c\u1ee5m. \u0110\u00e2y ch\u00ednh l\u00e0 Vocabulary Tree</li> </ol> <p></p> <p>H\u00ecnh 7: H\u00ecnh mi\u00eau t\u1ea3 c\u00e1ch x\u00e2y d\u1ef1ng Vocabulary Tree</p> <p>Sau khi \u0111\u00e3 x\u00e2y d\u1ef1ng Vocabulary Tree, Colmap th\u1ef1c hi\u1ec7n t\u00ecm c\u1eb7p \u1ea3nh t\u01b0\u01a1ng \u0111\u1ed3ng nh\u01b0 sau:</p> <ol> <li> <p>Tr\u00edch xu\u1ea5t \u0111\u1eb7c tr\u01b0ng c\u00e1c \u1ea3nh d\u00f9ng \u0111\u1ec3 x\u00e2y d\u1ef1ng, s\u1eed d\u1ee5ng vocabulary tree \u0111\u1ec3 t\u00ecm node l\u00e1 t\u01b0\u01a1ng \u1ee9ng cho \u0111\u1eb7c tr\u01b0ng. M\u1ed7i node l\u00e1 s\u1ebd l\u01b0u l\u1ea1i c\u00e1c th\u00f4ng tin: C\u00f3 bao nhi\u00eau \u1ea3nh c\u00f3 \u0111\u1eb7c tr\u01b0ng thu\u1ed9c l\u00e1 n\u00e0y? \u1ea2nh n\u00e0y c\u00f3 bao nhi\u00eau \u0111\u1eb7c tr\u01b0ng thu\u1ed9c l\u00e0 n\u00e0y? C\u00e1c th\u00f4ng tin n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 t\u00ednh kho\u1ea3ng c\u00e1ch v\u1edbi \u1ea3nh truy v\u1ea5n.</p> <p>  H\u00ecnh 8: H\u00ecnh mi\u00eau t\u1ea3 c\u00e1c ph\u00e2n b\u1ed1 \u0111\u1eb7c tr\u01b0ng c\u1ee7a d\u1eef li\u1ec7u v\u00e0o c\u00e1c node l\u00e1 trong Vocabulary Tree.   </p> <li> <p>X\u00e9t 1 \u1ea3nh b\u1ea5t k\u1ef3 (\u1ea3nh truy v\u1ea5n), Colmap tr\u00edch xu\u1ea5t v\u00e0 t\u00ecm node l\u00e1 t\u01b0\u01a1ng \u1ee9ng cho t\u1eebng \u0111\u1eb7c tr\u01b0ng. C\u00e1c \u1ea3nh c\u00f3 s\u1ed1 l\u01b0\u1ee3ng \u0111\u1eb7c tr\u01b0ng c\u00f9ng c\u1ee5m v\u1edbi \u0111\u1eb7c tr\u01b0ng c\u1ee7a \u1ea3nh truy v\u1ea5n c\u00e0ng nhi\u1ec1u th\u00ec c\u00e0ng gi\u1ed1ng \u1ea3nh truy v\u1ea5n (Th\u1ef1c t\u1ebf, Colmap s\u1eed d\u1ee5ng ph\u00e9p t\u00ednh \u0111i\u1ec3m t\u1eeb thu\u1eadt to\u00e1n \u0111\u1ec1 xu\u1ea5t). Thu\u1eadt to\u00e1n s\u1ebd ch\u1ecdn ra t\u1ecdp K \u1ea3nh c\u00f3 \u0111i\u1ec3m cao nh\u1ea5t v\u1edbi \u1ea3nh truy v\u1ea5n nh\u1ea5t trong kho d\u1eef li\u1ec7u.</p> <p>  H\u00ecnh 9: H\u00ecnh mi\u00eau t\u1ea3 c\u00e1ch t\u00ecm c\u00e1c h\u00ecnh trung kh\u1edbp v\u1edbi \u1ea3nh truy v\u1ea5n s\u1eed d\u1ee5ng Vocabulary Tree.  </p> <p>Sau khi \u0111\u00e3 t\u00ecm \u1ea3nh c\u1eb7p, Colmap s\u1ebd th\u1ef1c hi\u1ec7n ph\u00e9p Feature Matching: T\u00ecm c\u00e1c \u0111i\u1ec3m keypoint t\u01b0\u01a1ng \u0111\u1ed3ng gi\u1eefa hai \u1ea3nh. \u0110\u1ec3 h\u1ea1n ch\u1ebf nhi\u1ec5u, thu\u1eadt to\u00e1n th\u1ef1c hi\u1ec7n th\u00eam b\u01b0\u1edbc Image Verification: Lo\u1ea1i b\u1ecf outlier b\u1eb1ng thu\u1eadt to\u00e1n RANSAC.</p> <p></p> <p>H\u00ecnh 10: K\u1ebft qu\u1ea3 t\u00ecm ki\u1ebfm h\u00ecnh \u1ea3nh t\u01b0\u01a1ng \u1ee9ng v\u1edbi \u1ea3nh truy v\u1ea5n v\u00e0 ph\u00e9p g\u00e1n c\u00e1c keypoint t\u01b0\u01a1ng \u0111\u1ed3ng v\u1edbi nhau.</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#4-spare-reconstruction","title":"4. Spare reconstruction","text":"<p>Sau khi \u0111\u00e3 t\u00ecm \u0111\u01b0\u1ee3c c\u00e1c c\u1eb7p \u1ea3nh c\u0169ng nh\u01b0 c\u00e1c keypoint t\u01b0\u01a1ng \u1ee9ng c\u1ee7a hai \u1ea3nh kh\u1edbp nhau, COLMAP s\u1ebd x\u00e2y d\u1ef1ng l\u1ea1i kh\u00f4ng gian 3D th\u01b0a b\u1eb1ng c\u00e1ch chi\u1ebfu c\u00e1c \u0111i\u1ec3m keypoint n\u00e0y l\u00ean kh\u00f4ng gian. X\u00e2y d\u1ef1ng kh\u00f4ng gian th\u01b0a thu\u1ed9c v\u1ec1 b\u00e0i to\u00e1n Structure From Motion (SFM): Colmap th\u1ef1c hi\u1ec7n t\u1eebng b\u01b0\u1edbc \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n n\u00e0y nh\u01b0 sau:</p> <p>Step 1 - Initialization</p> <p>Colmap ch\u1ecdn ra hai \u1ea3nh ng\u1eabu nhi\u00ean (ch\u00fang t\u00f4i ngh\u0129 l\u00e0 c\u1eb7p \u1ea3nh c\u00f3 s\u1ed1 l\u01b0\u1ee3ng keypoint matching l\u1edbn nh\u1ea5t). V\u1edbi keypoint matching c\u1ee7a hai \u1ea3nh \u0111\u00e3 \u0111\u01b0\u1ee3c t\u00ednh, Colmap s\u1ebd gi\u1ea3i quy\u1ebft c\u00e1c b\u00e0i to\u00e1n:</p> <ol> <li>T\u00ecm v\u1ecb tr\u00ed t\u01b0\u01a1ng \u0111\u1ed1i gi\u1eefa hai camera: b\u1eb1ng c\u00e1ch gi\u1ea3i b\u00e0i to\u00e1n 1) T\u00ecm Fundamental matrix, 2) t\u00ecm Essential matrix t\u1eeb Fundamental matrix, 3) T\u00ecm camera pose t\u1eeb Essential matrix, b\u01b0\u1edbc n\u00e0y ra 4 k\u1ebft qu\u1ea3, v\u00e0 4) ch\u1ecdn ra k\u1ebft qu\u1ea3 th\u00f5a \u0111i\u1ec1u ki\u1ec7n Cheirality. C\u00e1c ph\u01b0\u01a1ng ph\u00e1p l\u1ecdc nhi\u1ec5u nh\u01b0 RANSAC \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng \u1edf b\u01b0\u1edbc 1, 2, 3.</li> <li>Chi\u1ebfu c\u00e1c \u0111i\u1ec3m n\u00e0y l\u00ean kh\u00f4ng gian 3D, b\u01b0\u1edbc n\u00e0y c\u00f3 th\u1ec3 \u0111\u00e3 \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n \u1edf b\u01b0\u1edbc 1.4</li> </ol> <p>Step 2: Image Registration:</p> <p>Sau khi \u0111\u00e3 x\u00e2y d\u1ef1ng kh\u00f4ng gian th\u01b0a t\u1eeb hai \u1ea3nh, Colmap s\u1ebd th\u00eam \u1ea3nh \u0111\u1ec3 ti\u1ebfp t\u1ee5c x\u00e2y d\u1ef1ng kh\u00f4ng gian th\u01b0a. \u1ea2nh \u0111\u01b0\u1ee3c ch\u1ecdn ph\u1ea3i c\u00f3 c\u00e1c \u0111i\u1ec3m keypoint matching v\u1edbi c\u00e1c \u0111i\u1ec3m keypoint c\u00f3 s\u1eb5n t\u1eeb c\u00e1c h\u00ecnh \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng. \u1ede b\u01b0\u1edbc n\u00e0y, Colmap gi\u1ea3i quy\u1ebft c\u00e1c b\u00e0i to\u00e1n:</p> <ul> <li>X\u00e1c \u0111\u1ecbnh \u0111i\u1ec3m keypoint c\u1ee7a \u1ea3nh \u0111\u01b0\u1ee3c th\u00eam v\u00e0o matching v\u1edbi c\u00e1c \u0111i\u1ec3m keypoint v\u00e0 \u0111i\u1ec3m \u0111\u00e3 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng l\u00ean kh\u00f4ng gian 3D n\u00e0o trong c\u00e1c \u1ea3nh \u0111\u00e3 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng.</li> <li>Perspective-n-Point problem: T\u00ecm v\u1ecb tr\u00ed t\u01b0\u01a1ng t\u01b0\u01a1ng \u0111\u1ed1i gi\u1eefa camera \u0111\u01b0\u1ee3c th\u00eam v\u00e0o so v\u1edbi c\u00e1c camera.</li> </ul> <p>Step 3: Triangulation</p> <p>Colmap s\u1ebd chi\u1ebfu c\u00e1c \u0111i\u1ec3m keypoint c\u1ee7a \u1ea3nh \u0111\u01b0\u1ee3c th\u00eam ch\u01b0a \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng v\u00e0o kh\u00f4ng gian 3D.  </p> <p>Step 4: Bundle Adjustment</p> <p>\u1ede b\u01b0\u1edbc 2, v\u1ecb tr\u00ed camera \u0111\u01b0\u1ee3c t\u1ed1i \u01b0u t\u1eeb c\u00e1c \u0111i\u1ec3m point cloud \u0111\u00e3 c\u00f3 s\u1eb5n. \u1ede b\u01b0\u1edbc n\u00e0y, Colmap s\u1ebd thay \u0111\u1ed5i \u0111\u1ed3ng th\u1eddi v\u1ecb tr\u00ed camera v\u00e0 point cloud \u0111\u1ec3 c\u00f3 \u0111\u01b0\u01a1c k\u1ebft qu\u1ea3 t\u1ed1i \u01b0u nh\u1ea5t.</p> <p>Step 5: Outlier filtering</p> <p>Sau khi \u0111\u00e3 t\u1ed1i \u01b0u c\u00e1c \u0111i\u1ec3m point cloud v\u00e0 v\u1ecb tr\u00ed camera, Colmap s\u1ebd lo\u1ea1i b\u1ecf nh\u1eefng \u0111i\u1ec3m point cloud n\u00e0o cho error l\u1edbn khi chi\u1ebfu tr\u00ean m\u1eb7t ph\u1eb3ng \u1ea3nh 2D.</p> <p>Colmap ti\u1ebfp t\u1ee5c tr\u1edf l\u1ea1i b\u01b0\u1edbc hai cho \u0111\u1ebfn khi t\u1ea5t c\u1ea3 c\u00e1c \u1ea3nh \u0111\u00e3 \u0111\u01b0\u1ee3c th\u00eam v\u00e0o.</p> <p>Colmap th\u1ef1c hi\u1ec7n thu\u1eadt to\u00e1n Incremental Structure from Motion. Ngo\u00e0i ra c\u00f2n m\u1ed9t v\u00e0i thu\u1eadt to\u00e1n kh\u00e1c nh\u01b0 Global, Hierachical SFM.</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#ket-qua-tu-colmap","title":"K\u1ebft qu\u1ea3 t\u1eeb Colmap:","text":"<p>H\u00ecnh 11: K\u1ebft qu\u1ea3 Spare reconstruction x\u00e2y d\u1ef1ng t\u1eeb 19 g\u00f3c nh\u00ecn.</p> <p></p> <p>H\u00ecnh 12: K\u1ebft qu\u1ea3 Spare reconstruction x\u00e2y d\u1ef1ng t\u1eeb 57 g\u00f3c nh\u00ecn.</p> <p></p> <p>H\u00ecnh 13: K\u1ebft qu\u1ea3 Spare reconstruction x\u00e2y d\u1ef1ng t\u1eeb 128 g\u00f3c nh\u00ecn.</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#5-dense-reconstruction","title":"5. Dense Reconstruction","text":"<p>\u1ede b\u01b0\u1edbc tr\u01b0\u1edbc, Colmap \u0111\u00e3 t\u00ednh \u0111\u01b0\u1ee3c v\u1ecb tr\u00ed t\u01b0\u01a1ng \u0111\u1ed1i gi\u1eefa c\u00e1c camera, v\u00e0 x\u00e2y d\u1ef1ng \u0111\u01b0\u1ee3c kh\u00f4ng gian th\u01b0a. \u1ede b\u01b0\u1edbc n\u00e0y, Colmap s\u1ebd x\u00e2y d\u1ef1ng kh\u00f4ng gian d\u00e0y \u0111\u1eb7c l\u1ea5p \u0111\u1ea7y nh\u1eefng kho\u1ea3ng kh\u00f4ng c\u1ee7a kh\u00f4ng gian spare. Giai \u0111o\u1ea1n n\u00e0y \u0111\u01b0\u1ee3c t\u1eaft trong b\u01b0\u1edbc sau:</p> <ol> <li>\u01af\u1edbc l\u01b0\u1ee3ng \u0111\u1ed9 s\u00e2u v\u00e0 normal map cho t\u1eebng \u1ea3nh. Gi\u00e1 tr\u1ecb n\u00e0y ph\u1ea3i nh\u01b0 nhau \u1edf c\u00e1c pixel t\u01b0\u01a1ng trong c\u00e1c view kh\u00e1c nhau. So v\u1edbi b\u01b0\u1edbc spare reconstruction, c\u00e1c \u0111i\u1ec3m point cloud ch\u1ec9 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng t\u1eeb to\u00e0n b\u1ed9 c\u00e1c pixel thay v\u00ec ch\u1ec9 c\u00e1c \u0111i\u1ec3m keypoint do \u0111\u00f3 Colmap c\u00f3 th\u1ec3 x\u00e2y d\u1ef1ng \u0111\u01b0\u1ee3c kh\u00f4ng gian th\u01b0a d\u00e0y \u0111\u1eb7c h\u01a1n.</li> <li>L\u1ea5y \u0111\u1eebng kho\u1ea3ng tr\u1ed1n gi\u1eefa c\u00e1c \u0111i\u1ec3m point cloud s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n Poisson Surface Reconstruction.</li> </ol> <p></p> <p>H\u00ecnh 14: K\u1ebft qu\u1ea3 \u01b0\u1edbc l\u01b0\u1ee3ng normal map (c\u1ed9t gi\u1eefa) v\u00e0 \u0111\u1ed9 s\u00e2u (c\u1ed9t b\u00ean ph\u1ea3i) t\u1eeb b\u01b0\u1edbc 1.</p> <p></p> <p>H\u00ecnh 15: K\u1ebft qu\u1ea3 x\u00e2y d\u1ef1ng Dense Reconstruction.</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#6-ket-luan","title":"6. K\u1ebft lu\u1eadn","text":"<p>Trong b\u00e0i vi\u1ebft n\u00e0y, ch\u00fang t\u00f4i \u0111\u00e3 tr\u00ecnh b\u00e0y c\u00e1c b\u01b0\u1edbc th\u1ef1c hi\u1ec7n ph\u1ea7n m\u1ec1m COLMAP t\u1eeb t\u1eadp c\u00e1c \u1ea3nh \u0111\u1ea7u v\u00e0o \u0111\u1ebfn kh\u00f4ng gian 3D d\u00e0y \u0111\u1eb7c. \u1ede m\u1ed7i b\u01b0\u1edbc, ch\u00fang t\u00f4i t\u00f3m t\u1eaft thu\u1eadt to\u00e1n COLMAP s\u1eed d\u1ee5ng bao g\u1ed3m SIFT (Feature Detection), Vocabulary Tree (Feature Matching), Structure From Motion. K\u1ebft qu\u1ea3 t\u1eeb kh\u00f4ng gian d\u00e0y \u0111\u1eb7c cho th\u1ea5y COLMAP x\u00e2y d\u1ef1ng l\u1ea1i kh\u00f4ng gian 3D r\u1ea5t t\u1ed1t ch\u1ec9 v\u1edbi 128 \u1ea3nh d\u1eef li\u1ec7u. B\u1ea1n \u0111\u1ecdc quan t\u00e2m t\u1edbi COLMAP c\u00f3 th\u1ec3 t\u00ecm th\u1ea5y nh\u1eefng k\u1ebft qu\u1ea3 kh\u00e1c t\u1ea1i trang ch\u1ee7 ph\u1ea7n m\u1ec1m.</p>"},{"location":"blog/colmap-a-software-for-3d-spare--dense-reconstruction/#7-tham-khao","title":"7. Tham kh\u1ea3o","text":"<ul> <li>Large-scale 3D modeling. CVPR 2017 Tutorial - Large-\u00adscale 3D Modeling from Crowdsourced Data. (n.d.) - https://demuc.de/tutorials/cvpr2017/.</li> <li>Vocabulary Tree - https://www.micc.unifi.it/delbimbo/wp-content/uploads/2013/12/A62_Vocabulary_tree.pdf.</li> <li>Chahat Deep Singh. \u201cStructure from Motion\u201d, CMSC 426 Computer Vision.</li> </ul>"},{"location":"blog/edge-detection/","title":"Edge detection","text":"<p>Trong giai \u0111o\u1ea1n \u0111\u1ea7u c\u1ee7a qu\u00e1 tr\u00ecnh x\u1eed l\u00fd \u1ea3nh, ch\u00fang ta mong mu\u1ed1n \u0111\u00fac k\u1ebft ra nh\u1eefng th\u00f4ng tin v\u1ec1 c\u1ea5u tr\u00fac c\u0169ng nh\u01b0 t\u00ednh ch\u1ea5t c\u1ee7a c\u00e1c v\u1eadt th\u1ec3 trong \u1ea3nh. \u0110\u1ec3 l\u00e0m \u0111\u01b0\u1ee3c \u0111i\u1ec1u n\u00e0y, vi\u1ec7c t\u00ecm ra nh\u1eefng \u0111\u1eb7c tr\u01b0ng c\u01a1 b\u1ea3n (feature) c\u1ee7a c\u00e1c v\u1eadt th\u1ec3 n\u00e0y l\u00e0 c\u1ea7n thi\u1ebft. C\u1ea1nh (edge) l\u00e0 m\u1ed9t trong nh\u1eefng \u0111\u1eb7c tr\u01b0ng n\u00e0y.  Ch\u00fang ta c\u00f9ng quan s\u00e1t hai b\u1ee9c \u1ea3nh sau:</p> <p> </p> <p>H\u00ecnh 1: B\u00ean tr\u00e1i: \u1ea2nh ch\u1ee5p b\u1ee9c t\u01b0\u1ee3ng The Archer c\u1ee7a Henry Moore (1964). B\u00ean ph\u1ea3i: B\u1ea3n ph\u00e1c th\u1ea3o c\u1ee7a b\u1ee9c t\u01b0\u1ee3ng n\u00e0y. \u1ea2nh tham kh\u1ea3o t\u1eeb [4].   </p> <p>Tuy \u0111\u00e3 gi\u1ea3n l\u01b0\u1ee3c r\u1ea5t nhi\u1ec1u chi ti\u1ebft v\u00e0 ch\u1ec9 l\u01b0u l\u1ea1i nh\u1eefng \u0111\u01b0\u1eddng n\u00e9t n\u1ed5i b\u1eadt nh\u1ea5t, b\u1ee9c \u1ea3nh b\u00ean ph\u1ea3i v\u1eabn b\u1ea3o to\u00e0n \u0111\u01b0\u1ee3c m\u1ed9t l\u01b0\u1ee3ng th\u00f4ng tin \u0111\u1ee7 \u0111\u1ec3 gi\u00fap ta h\u00ecnh dung \u0111\u01b0\u1ee3c m\u1ed9t c\u00e1ch d\u1ec5 d\u00e0ng k\u1ebft c\u1ea5u v\u00e0 m\u1ed9t v\u00e0i \u0111\u1eb7c \u0111i\u1ec3m nh\u1ea5t \u0111\u1ecbnh c\u1ee7a v\u1eadt th\u1ec3 trong b\u1ee9c \u1ea3nh g\u1ed1c. Do v\u1eady, nh\u1eadn di\u1ec7n c\u1ea1nh th\u01b0\u1eddng l\u00e0 m\u1ed9t trong nh\u1eefng b\u01b0\u1edbc \u0111\u1ea7u ti\u00ean c\u1ee7a qu\u00e1 tr\u00ecnh kh\u00f4i ph\u1ee5c th\u00f4ng tin t\u1eeb m\u1ed9t b\u1ee9c \u1ea3nh. C\u0169ng b\u1edfi vai tr\u00f2 quan tr\u1ecdng n\u00e0y n\u00ean b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh v\u1eabn ti\u1ebfp t\u1ee5c l\u00e0 m\u1ed9t l\u0129nh v\u1ef1c \u0111\u01b0\u1ee3c nghi\u00ean c\u1ee9u \u0111\u01b0\u1ee3c quan t\u00e2m r\u1ed9ng r\u00e3i. Trong b\u00e0i vi\u1ebft n\u00e0y, ch\u00fang ta s\u1ebd c\u00f9ng nhau t\u00ecm hi\u1ec3u nh\u1eefng kh\u00e1i ni\u1ec7m c\u01a1 b\u1ea3n li\u00ean quan \u0111\u1ebfn b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh, c\u00e1c v\u1ea5n \u0111\u1ec1 th\u01b0\u1eddng g\u1eb7p trong b\u00e0i to\u00e1n n\u00e0y c\u0169ng nh\u01b0 nh\u1eefng thu\u1eadt to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh th\u00f4ng d\u1ee5ng.</p>"},{"location":"blog/edge-detection/#1-mot-vai-khai-niem","title":"1. M\u1ed9t v\u00e0i kh\u00e1i ni\u1ec7m","text":""},{"location":"blog/edge-detection/#11-anh-trang-en-grayscale-image","title":"1.1. \u1ea2nh tr\u1eafng \u0111en (grayscale image)","text":"<p>B\u00e0i vi\u1ebft n\u00e0y ch\u1ec9 s\u1eed d\u1ee5ng \u1ea3nh grayscale l\u00e0m minh h\u1ecda cho b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh. \u1ea2nh grayscale \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng c\u00f3 gi\u00e1 tr\u1ecb pixel dao \u0111\u1ed9ng trong kho\u1ea3ng t\u1eeb 0 \u0111\u1ebfn 255 v\u1edbi 0 l\u00e0 m\u00e0u \u0111en v\u00e0 255 l\u00e0 m\u00e0u tr\u1eafng, c\u00e1c gi\u00e1 tr\u1ecb \u1edf gi\u1eefa l\u00e0 nh\u1eefng s\u1eafc th\u00e1i x\u00e1m kh\u00e1c nhau \u0111\u01b0\u1ee3c pha tr\u1ed9n theo m\u1ed9t t\u1ec9 l\u1ec7 nh\u1ea5t \u0111\u1ecbnh hai m\u00e0u \u0111en v\u00e0 tr\u1eafng n\u00e0y. C\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a pixel \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 m\u1ee9c \u0111\u1ed9 x\u00e1m (gray level), gi\u00e1 tr\u1ecb n\u00e0y trong m\u1ed9t s\u1ed1 t\u00e0i li\u1ec7u c\u0169ng \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 c\u01b0\u1eddng \u0111\u1ed9 \u1ea3nh (image intensity) hay c\u01b0\u1eddng \u0111\u1ed9 (intensity).</p> <p> </p> <p>H\u00ecnh 2: D\u00e3y m\u00e0u cho \u1ea3nh grayscale (\u0111\u1ec3 \u00fd r\u1eb1ng gi\u00e1 tr\u1ecb c\u1ee7a pixel c\u00e0ng th\u1ea5p c\u00f3 m\u00e0u ng\u1ea3 sang \u0111en nhi\u1ec1u h\u01a1n v\u00e0 ng\u01b0\u1ee3c l\u1ea1i). \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.   </p>"},{"location":"blog/edge-detection/#12-cach-anh-chi-so-cac-pixel","title":"1.2. C\u00e1ch \u0111\u00e1nh ch\u1ec9 s\u1ed1 c\u00e1c pixel","text":"<p>Trong m\u1eb7t ph\u1eb3ng \u1ea3nh, ta th\u01b0\u1eddng \u0111\u1eb7t g\u1ed1c t\u1ecda \u0111\u1ed9 \u1edf trung t\u00e2m, tia Ox h\u01b0\u1edbng t\u1eeb tr\u00e1i sang ph\u1ea3i v\u00e0 tia Oy h\u01b0\u1edbng t\u1eeb d\u01b0\u1edbi l\u00ean tr\u00ean. Tuy nhi\u00ean khi s\u1ed1 h\u00f3a th\u00ec b\u1ee9c \u1ea3nh \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng m\u1ed9t m\u1ea3ng 2 chi\u1ec1u, trong \u0111\u00f3 m\u1ed7i ph\u1ea7n t\u1eed l\u01b0u gi\u00e1 tr\u1ecb c\u1ee7a pixel \u1edf v\u1ecb tr\u00ed t\u01b0\u01a1ng \u1ee9ng. L\u01b0u \u00fd r\u1eb1ng \u0111\u1ec3 l\u00e0m vi\u1ec7c v\u1edbi m\u1ed9t b\u1ee9c \u1ea3nh, ta c\u1ea7n chuy\u1ec3n b\u1ee9c \u1ea3nh n\u00e0y th\u00e0nh d\u1ea1ng s\u1ed1, l\u00fac n\u00e0y b\u1ee9c \u1ea3nh kh\u00f4ng c\u00f2n gi\u1eef \u0111\u01b0\u1ee3c t\u00ednh \"li\u00ean t\u1ee5c\" c\u1ee7a n\u00f3 n\u1eefa m\u00e0 b\u1ecb \"l\u01b0\u1ee3ng h\u00f3a\" (quantized) th\u00e0nh c\u00e1c \u0111\u01a1n v\u1ecb nh\u1ecf g\u1ecdi l\u00e0 c\u00e1c pixel. Th\u00f4ng th\u01b0\u1eddng ng\u01b0\u1eddi ta \u0111\u00e1nh ch\u1ec9 s\u1ed1 c\u00e1c pixel nh\u01b0 sau: m\u1ed7i pixel c\u00f3 ch\u1ec9 s\u1ed1 \\([i,j]\\), trong \u0111\u00f3 \\(i\\) t\u0103ng d\u1ea7n t\u1eeb tr\u00e1i sang ph\u1ea3i, \\(j\\) t\u0103ng d\u1ea7n t\u1eeb tr\u00ean xu\u1ed1ng d\u01b0\u1edbi, pixel tr\u00ean c\u00f9ng b\u00ean tr\u00e1i c\u00f3 ch\u1ec9 s\u1ed1 \\([0,0]\\).</p> <p> </p> <p>H\u00ecnh 3: M\u1ed9t \u0111i\u1ec3m tr\u00ean m\u1eb7t ph\u1eb3ng \u1ea3nh v\u00e0 pixel t\u01b0\u01a1ng \u1ee9ng bi\u1ec3u di\u1ec5n \u0111i\u1ec3m \u1ea3nh \u1ea5y (l\u01b0u \u00fd r\u1eb1ng nhi\u1ec1u \u0111i\u1ec3m \u1ea3nh c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1edfi m\u1ed9t pixel duy nh\u1ea5t). \u1ea2nh tham kh\u1ea3o t\u1eeb [1].   </p>"},{"location":"blog/edge-detection/#13-phep-tich-chap-convolution","title":"1.3. Ph\u00e9p t\u00edch ch\u1eadp (convolution)","text":"<p>Ph\u00e9p t\u00edch ch\u1eadp c\u00f3 quan h\u1ec7 m\u1eadt thi\u1ebft \u0111\u1ebfn ph\u00e9p t\u01b0\u01a1ng quan ch\u00e9o (cross correlation). Trong ph\u00e9p t\u01b0\u01a1ng quan ch\u00e9o, ta c\u00f3 m\u1ed9t h\u1ea1t nh\u00e2n (kernel) v\u1edbi m\u1ed9t ph\u1ea7n t\u1eed trung t\u00e2m (center) v\u00e0 m\u1ed9t m\u1ea3ng \u0111\u1ea7u v\u00e0o, l\u1ea7n l\u01b0\u1ee3t tr\u01b0\u1ee3t kernel t\u1eeb g\u00f3c tr\u00ean c\u00f9ng b\u00ean tr\u00e1i c\u1ee7a m\u1ea3ng \u0111\u1ea7u v\u00e0o theo chi\u1ec1u t\u1eeb tr\u00e1i sang ph\u1ea3i, t\u1eeb tr\u00ean xu\u1ed1ng d\u01b0\u1edbi. T\u1ea1i m\u1ed7i ph\u1ea7n t\u1eed c\u1ee7a m\u1ea3ng \u0111\u1ea7u v\u00e0o m\u00e0 trung t\u00e2m kernel n\u00e0y tr\u01b0\u1ee3t \u0111\u1ebfn, gi\u00e1 tr\u1ecb c\u1ee7a ph\u1ea7n t\u1eed n\u00e0y \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng c\u00e1ch nh\u00e2n gi\u00e1 tr\u1ecb m\u1ed7i ph\u1ea7n t\u1eed trong kernel v\u1edbi gi\u00e1 tr\u1ecb ph\u1ea7n t\u1eed t\u1ea1i v\u1ecb tr\u00ed t\u01b0\u01a1ng \u1ee9ng c\u1ee7a m\u1ea3ng \u0111\u1ea7u v\u00e0o, r\u1ed3i l\u1ea5y t\u1ed5ng c\u00e1c gi\u00e1 tr\u1ecb n\u00e0y [3]. Trong ph\u00e9p t\u00edch ch\u1eadp, ta c\u1ea7n xoay kernel 180 \u0111\u1ed9 tr\u01b0\u1edbc khi th\u1ef1c hi\u1ec7n qu\u00e1 tr\u00ecnh tr\u00ean. B\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 t\u00ecm hi\u1ec3u k\u0129 h\u01a1n v\u1ec1 kh\u00e1i ni\u1ec7m n\u00e0y qua b\u00e0i vi\u1ebft M\u1ea1ng n\u01a1-ron t\u00edch ch\u1eadp - Convolutional Neural Network (CNN/ConvNet), ph\u1ea7n 2.1 vi\u1ebft v\u1ec1 L\u1edbp t\u00edch ch\u1eadp (Convolutional layer).</p> <p>\u1ede nh\u1eefng ph\u1ea7n ti\u1ebfp theo c\u00f3 s\u1eed d\u1ee5ng ph\u00e9p t\u00edch ch\u1eadp, ta m\u1eb7c \u0111\u1ecbnh r\u1eb1ng c\u00e1c kernel s\u1eed d\u1ee5ng \u0111\u00e3 \u0111\u01b0\u1ee3c xoay 180 \u0111\u1ed9.</p>"},{"location":"blog/edge-detection/#14-canh-va-cac-nguyen-nhan-tao-thanh-canh","title":"1.4. C\u1ea1nh v\u00e0 c\u00e1c nguy\u00ean nh\u00e2n t\u1ea1o th\u00e0nh c\u1ea1nh","text":"<p>C\u1ea1nh \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a l\u00e0 s\u1ef1 thay \u0111\u1ed5i c\u01b0\u1eddng \u0111\u1ed9 \u1ea3nh m\u1ed9t c\u00e1ch \u0111\u1ed9t ng\u1ed9t v\u00e0 \u0111\u00e1ng k\u1ec3.</p> <p> </p> <p>H\u00ecnh 4: H\u00ecnh a: \u1ea2nh c\u1ee7a \u0111\u1ea7u m\u1ed9t thanh truy\u1ec1n. H\u00ecnh b: \u0110\u1ed3 th\u1ecb bi\u1ec3u di\u1ec5n m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a m\u1ed7i pixel trong d\u00e3y pixel n\u1eb1m tr\u00ean \u0111\u01b0\u1eddng m\u00e0u \u0111\u1ecf. \u1ea2nh tham kh\u1ea3o t\u1eeb [1].   </p> <p>D\u1ef1a v\u00e0o \u0111\u1ed3 th\u1ecb, ta th\u1ea5y r\u1eb1ng m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a pixel trong nh\u1eefng \u00f4 m\u00e0u xanh lam c\u00f3 s\u1ef1 thay \u0111\u1ed5i \u0111\u1ed9t ng\u1ed9t nh\u01b0ng kh\u00f4ng \u0111\u00e1ng k\u1ec3. Ch\u1ec9 c\u00f3 m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a c\u00e1c pixel trong c\u00e1c \u00f4 m\u00e0u xanh l\u1ee5c l\u00e0 thay \u0111\u1ed5i v\u1eeba \u0111\u00e1ng k\u1ec3, v\u1eeba \u0111\u1ed9t ng\u1ed9t. Do v\u1eady, ch\u1ec9 c\u00f3 nh\u1eefng s\u1ef1 thay \u0111\u1ed5i trong c\u00e1c \u00f4 m\u00e0u xanh l\u1ee5c m\u1edbi \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh (l\u01b0u \u00fd r\u1eb1ng trong \u0111\u1ed3 th\u1ecb c\u00f3 nhi\u1ec1u ch\u1ed7 bi\u1ec5u di\u1ec5n c\u1ea1nh, m\u00ecnh ch\u1ec9 l\u1ea5y v\u00ed d\u1ee5 \u1edf hai ch\u1ed7), c\u00f2n s\u1ef1 thay \u0111\u1ed5i \u1edf nh\u1eefng \u00f4 m\u00e0u xanh lam ch\u1ec9 l\u00e0 nhi\u1ec5u (noise).</p> <p>V\u1edbi \u0111\u1ecbnh ngh\u0129a n\u00e0y, ta th\u1ea5y r\u1eb1ng c\u1ea1nh t\u1ea1o ra d\u1ef1a tr\u00ean s\u1ef1 thay \u0111\u1ed5i c\u1ee7a c\u01b0\u1eddng \u0111\u1ed9 \u1ea3nh kh\u00f4ng nh\u1ea5t thi\u1ebft ph\u1ea3i t\u01b0\u01a1ng \u1ee9ng v\u1edbi c\u1ea1nh th\u1ef1c t\u1ebf c\u1ee7a v\u1eadt th\u1ec3. V\u1eady th\u00ec c\u00e1c c\u1ea1nh n\u00e0y th\u01b0\u1eddng \u0111\u01b0\u1ee3c t\u1ea1o th\u00e0nh do \u0111\u00e2u?</p> <p>B\u1ed1n nguy\u00ean nh\u00e2n ph\u1ed5 bi\u1ebfn t\u1ea1o n\u00ean c\u1ea1nh l\u00e0:</p> <ul> <li>S\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u0111\u1ed9 s\u00e2u (Depth discontinuity): Gi\u1eefa hai v\u1eadt th\u1ec3 (m\u1ed9t \u1edf tr\u01b0\u1edbc v\u00e0 m\u1ed9t \u1edf sau) ho\u1eb7c gi\u1eefa v\u1eadt th\u1ec3 v\u00e0 n\u1ec1n kh\u1ea3 n\u0103ng cao s\u1ebd c\u00f3 m\u1ed9t s\u1ef1 \u0111\u1ee9t \u0111o\u1ea1n, s\u1ef1 \u0111\u1ee9t \u0111o\u1ea1n \u0111\u00f3 h\u00ecnh th\u00e0nh m\u1ed9t c\u1ea1nh.</li> <li>S\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u0111\u1ecbnh h\u01b0\u1edbng b\u1ec1 m\u1eb7t (Surface normal discontinuity): Tuy n\u1eb1m tr\u00ean c\u00f9ng m\u1ed9t v\u1eadt th\u1ec3 v\u00e0 c\u00f3 c\u00f9ng m\u1ed9t ch\u1ea5t li\u1ec7u nh\u01b0ng s\u1ef1 \u0111\u1ecbnh h\u01b0\u1edbng kh\u00e1c nhau c\u1ee7a c\u00e1c b\u1ec1 m\u1eb7t d\u1eabn \u0111\u1ebfn \u00e1nh s\u00e1ng m\u1ed7i b\u1ec1 m\u1eb7t nh\u1eadn \u0111\u01b0\u1ee3c t\u1eeb ngu\u1ed3n s\u00e1ng kh\u00e1c nhau n\u00ean gi\u1eefa c\u00e1c b\u1ec1 m\u1eb7t \u1ea5y h\u00ecnh th\u00e0nh c\u1ea1nh.</li> <li>S\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u0111\u1ed9 ph\u1ea3n x\u1ea1 c\u1ee7a b\u1ec1 m\u1eb7t (Surface reflectance discontinuity): N\u1ebfu m\u1ed9t v\u1eadt th\u1ec3 \u0111\u01b0\u1ee3c l\u00e0m t\u1eeb nhi\u1ec1u ch\u1ea5t li\u1ec7u kh\u00e1c nhau th\u00ec kh\u1ea3 n\u0103ng cao l\u00e0 \u1edf nh\u1eefng ch\u1ed7 giao nhau gi\u1eefa c\u00e1c ch\u1ea5t li\u1ec7u \u1ea5y s\u1ebd h\u00ecnh th\u00e0nh c\u1ea1nh do s\u1ef1 ph\u1ea3n x\u1ea1 \u00e1nh s\u00e1ng c\u1ee7a m\u1ed7i ch\u1ea5t li\u1ec7u l\u00e0 kh\u00e1c nhau.</li> <li>S\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u00e1nh s\u00e1ng (Illumination discontinuity): Trong nhi\u1ec1u tr\u01b0\u1eddng h\u1ee3p, v\u1eadt th\u1ec3 s\u1ebd ng\u0103n m\u1ed9t l\u01b0\u1ee3ng \u00e1nh s\u00e1ng nh\u1ea5t \u0111\u1ecbnh, t\u1ea1o th\u00e0nh b\u00f3ng. S\u1ef1 thay \u0111\u1ed5i \u0111\u1ed9t ng\u1ed9t v\u1ec1 m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a pixel gi\u1eefa v\u00f9ng b\u1ecb h\u1eaft b\u00f3ng v\u00e0 v\u00f9ng kh\u00f4ng b\u1ecb h\u1eaft b\u00f3ng d\u1eabn \u0111\u1ebfn vi\u1ec7c t\u1ea1o th\u00e0nh c\u1ea1nh gi\u1eefa hai v\u00f9ng n\u00e0y.</li> </ul> <p>H\u00ecnh d\u01b0\u1edbi \u0111\u00e2y gi\u00fap b\u1ea1n \u0111\u1ecdc h\u00ecnh dung r\u00f5 h\u01a1n v\u1ec1 c\u00e1c nguy\u00ean nh\u00e2n n\u00e0y:</p> <p> </p> <p>H\u00ecnh 5: C\u00e1c nguy\u00ean nh\u00e2n t\u1ea1o th\u00e0nh c\u1ea1nh (c\u00e1c \u0111\u01b0\u1eddng m\u00e0u xanh lam l\u00e0 nh\u1eefng c\u1ea1nh t\u1ea1o th\u00e0nh do s\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u0111\u1ecbnh h\u01b0\u1edbng b\u1ec1 m\u1eb7t, c\u00e1c \u0111\u01b0\u1eddng m\u00e0u v\u00e0ng l\u00e0 nh\u1eefng c\u1ea1nh t\u1ea1o th\u00e0nh do s\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u0111\u1ed9 ph\u1ea3n x\u1ea1 c\u1ee7a b\u1ec1 m\u1eb7t, c\u00e1c \u0111\u01b0\u1eddng m\u00e0u xanh l\u1ee5c l\u00e0 nh\u1eefng c\u1ea1nh t\u1ea1o th\u00e0nh do s\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u00e1nh s\u00e1ng, c\u00e1c \u0111\u01b0\u1eddng m\u00e0u \u0111\u1ecf l\u00e0 nh\u1eefng c\u1ea1nh t\u1ea1o th\u00e0nh do s\u1ef1 kh\u00f4ng li\u00ean t\u1ee5c v\u1ec1 \u0111\u1ed9 s\u00e2u). \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.   </p>"},{"location":"blog/edge-detection/#15-cac-khai-niem-khac","title":"1.5. C\u00e1c kh\u00e1i ni\u1ec7m kh\u00e1c:","text":"<p>Ngo\u00e0i nh\u1eefng kh\u00e1i ni\u1ec7m tr\u00ean th\u00ec c\u00f3 m\u1ed9t v\u00e0i kh\u00e1i ni\u1ec7m m\u00e0 m\u00ecnh cho r\u1eb1ng s\u1ebd c\u1ea7n thi\u1ebft cho b\u1ea1n \u0111\u1ecdc \u1edf nh\u1eefng ph\u1ea7n sau:</p> <ul> <li>\u0110i\u1ec3m c\u1ea1nh (edge point): l\u00e0 m\u1ed9t \u0111i\u1ec3m trong \u1ea3nh c\u00f3 t\u1ecda \u0111\u1ed9 \\([i,j]\\) m\u00e0 \u1edf \u0111\u00f3 x\u1ea3y ra s\u1ef1 thay \u0111\u1ed5i \u0111\u1ed9t ng\u1ed9t v\u00e0 \u0111\u00e1ng k\u1ec3 v\u1ec1 c\u01b0\u1eddng \u0111\u1ed9 \u1ea3nh.</li> <li>\u0110\u01b0\u1eddng bi\u00ean (contour): M\u1ed9t t\u1eadp c\u00e1c \u0111i\u1ec3m c\u1ea1nh \u0111\u00e3 \u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp th\u1ee9 t\u1ef1 ho\u1eb7c m\u1ed9t \u0111\u01b0\u1eddng cong bi\u1ec3u di\u1ec5n t\u1eadp c\u00e1c \u0111i\u1ec3m c\u1ea1nh \u1ea5y.</li> <li>T\u1eadp c\u00e1c \u0111i\u1ec3m c\u1ea1nh \u0111\u00e3 \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n c\u00f3 th\u1ec3 chia l\u00e0m hai: t\u1eadp c\u00e1c c\u1ea1nh \u0111\u00fang (correct edges) v\u00e0 t\u1eadp c\u00e1c c\u1ea1nh sai (false edges), t\u1eadp c\u00e1c c\u1ea1nh sai n\u00e0y c\u00f2n \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 false positives, n\u00f3 t\u01b0\u01a1ng \u1ee9ng v\u1edbi nh\u1eefng \u0111i\u1ec3m kh\u00f4ng ph\u1ea3i l\u00e0 c\u1ea1nh trong \u1ea3nh g\u1ed1c. Ngo\u00e0i ra c\u00f2n m\u1ed9t t\u1eadp c\u1ea1nh th\u1ee9 ba, l\u00e0 nh\u1eefng c\u1ea1nh l\u1ebd ra ph\u1ea3i \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh, nh\u01b0ng b\u1ecb b\u1ecf qua b\u1edfi thu\u1eadt to\u00e1n, t\u1eadp n\u00e0y \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 false negatives.</li> </ul>"},{"location":"blog/edge-detection/#2-phuong-phap-ao-ham-cap-mot-cho-bai-toan-nhan-dien-canh","title":"2. Ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t cho b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh","text":"<p>\u1ede tr\u00ean ch\u00fang ta \u0111\u00e3 c\u00f3 \u0111\u1ecbnh ngh\u0129a v\u1ec1 c\u1ea1nh, l\u00e0 s\u1ef1 thay \u0111\u1ed5i \u0111\u1ed9t ng\u1ed9t v\u00e0 \u0111\u00e1ng k\u1ec3 v\u1ec1 m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a c\u00e1c pixel trong \u1ea3nh. S\u1ef1 thay \u0111\u1ed5i \u1ea5y nh\u1eafc ta nh\u1edb \u0111\u1ebfn kh\u00e1i ni\u1ec7m g\u00ec trong to\u00e1n h\u1ecdc nh\u1ec9? \u0110\u00fang r\u1ed3i, l\u00e0 \u0111\u1ea1o h\u00e0m! Ch\u00fang ta c\u00f9ng nhau quan s\u00e1t b\u1ee9c h\u00ecnh d\u01b0\u1edbi \u0111\u00e2y:</p> <p> </p> <p>H\u00ecnh 6: H\u00ecnh 1: D\u00e3y c\u00e1c pixel \u0111\u01b0\u1ee3c x\u00e9t. H\u00ecnh 2: \u0110\u1ed3 th\u1ecb bi\u1ec3u di\u1ec5n m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a m\u1ed7i pixel trong d\u00e3y. H\u00ecnh 3: \u0110\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a h\u00e0m s\u1ed1 bi\u1ec3u di\u1ec5n b\u1edfi \u0111\u1ed3 th\u1ecb th\u1ee9 nh\u1ea5t. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.   </p> <p>Trong b\u1ee9c \u1ea3nh tr\u00ean, ta kh\u00f4ng x\u00e9t to\u00e0n b\u1ed9 \u1ea3nh m\u00e0 ch\u1ec9 x\u00e9t d\u00e3y c\u00e1c pixel n\u1eb1m tr\u00ean \u0111\u01b0\u1eddng k\u1ebb m\u00e0u \u0111\u1ecf (xem b\u1ee9c \u1ea3nh c\u00f3 1 chi\u1ec1u). \u1ede \u0111\u1ed3 th\u1ecb th\u1ee9 nh\u1ea5t, ta nh\u1eadn th\u1ea5y c\u00f3 hai ch\u1ed7 m\u1ee9c \u0111\u1ed9 x\u00e1m thay \u0111\u1ed5i \u0111\u1ed9t ng\u1ed9t, t\u01b0\u01a1ng \u1ee9ng v\u1edbi hai l\u1ea7n c\u00e1c pixel tr\u00ean \u0111\u01b0\u1eddng k\u1ebb m\u00e0u \u0111\u1ecf chuy\u1ec3n m\u00e0u t\u1eeb tr\u1eafng sang \u0111en v\u00e0 t\u1eeb \u0111en sang tr\u1eafng, \u0111\u00f3 c\u0169ng ch\u00ednh l\u00e0 v\u1ecb tr\u00ed c\u1ee7a hai c\u1ea1nh. \u0110\u1ed3 th\u1ecb th\u1ee9 hai bi\u1ec3u di\u1ec5n \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a h\u00e0m s\u1ed1 bi\u1ec3u di\u1ec5n b\u1edfi \u0111\u1ed3 th\u1ecb th\u1ee9 nh\u1ea5t. \u0110\u1ebfn \u0111\u00e2y n\u1ebfu nh\u01b0 b\u1ea1n th\u1eafc m\u1eafc r\u1eb1ng: nh\u1eefng gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c pixel l\u00e0 r\u1eddi r\u1ea1c, th\u1ebf th\u00ec v\u00ec sao l\u1ea1i c\u00f3 \u0111\u1ea1o h\u00e0m? \u1ede \u0111\u00e2y, ch\u00fang ta xem nh\u01b0 gi\u00e1 tr\u1ecb c\u1ee7a d\u00e3y c\u00e1c pixel n\u1eb1m tr\u00ean \u0111\u01b0\u1eddng m\u00e0u \u0111\u1ecf \u0111\u01b0\u1ee3c m\u00f4 t\u1ea3 b\u1edfi m\u1ed9t h\u00e0m s\u1ed1, v\u00e0 b\u1edfi v\u00ec h\u00e0m s\u1ed1 n\u00e0y li\u00ean t\u1ee5c n\u00ean n\u00f3 c\u00f3 \u0111\u1ea1o h\u00e0m. Tr\u00ean th\u1ef1c t\u1ebf, ta kh\u00f4ng c\u1ea7n ph\u1ea3i m\u00f4 h\u00ecnh h\u00f3a gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c pixel b\u1edfi m\u1ed9t h\u00e0m s\u1ed1 m\u00e0 c\u00f3 th\u1ec3 x\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m c\u1ee7a ch\u00fang m\u1ed9t c\u00e1ch r\u1eddi r\u1ea1c. B\u1ea1n \u0111\u1ecdc s\u1ebd \u0111\u01b0\u1ee3c t\u00ecm hi\u1ec3u k\u0129 h\u01a1n v\u1ec1 c\u00e1ch x\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m \u1edf ph\u1ea7n d\u01b0\u1edbi.</p> <p>Trong kh\u00f4ng gian 2 chi\u1ec1u th\u00ec gradient t\u1ea1i m\u1ed9t \u0111i\u1ec3m l\u00e0 m\u1ed9t vector</p> \\[ \\boldsymbol{G}[f(x,y)] = \\begin{bmatrix} G_x \\\\ G_y \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} \\] <p>Trong \u0111\u00f3 \\(f(x,y)\\) l\u00e0 h\u00e0m s\u1ed1 bi\u1ec3u di\u1ec5n gi\u00e1 tr\u1ecb c\u1ee7a pixel t\u1ea1i v\u1ecb tr\u00ed \\((x,y)\\)</p> <ol> <li> <p>Hai t\u00ednh ch\u1ea5t quan tr\u1ecdng li\u00ean quan \u0111\u1ebfn gradient t\u1ea1i m\u1ed9t \u0111i\u1ec3m: H\u01b0\u1edbng c\u1ee7a gradient (gradient direction) l\u00e0 h\u01b0\u1edbng m\u00e0 m\u1ee9c \u0111\u1ed9 x\u00e1m pixel t\u0103ng nhanh nh\u1ea5t, \u0111\u01b0\u1ee3c cho b\u1edfi c\u00f4ng th\u1ee9c</p> \\[ \\alpha(x,y) = \\tan^{-1}\\left( \\frac{G_y}{G_x}\\right)  \\] </li> <li> <p>\u0110\u1ed9 l\u1edbn c\u1ee7a gradient (gradient magnitude), \u0111\u01b0\u1ee3c cho b\u1edfi c\u00f4ng th\u1ee9c</p> \\[ G[f(x,y)] = \\sqrt{G_x^2 + G_y^2}  \\] <p>\u0110\u1ed9 l\u1edbn n\u00e0y \u0111\u01b0\u1ee3c t\u00ednh theo chu\u1ea9n 2 (norm 2). Tr\u00ean th\u1ef1c t\u1ebf, \u0111\u1ec3 d\u1ec5 t\u00ednh to\u00e1n th\u00ec ng\u01b0\u1eddi ta th\u01b0\u1eddng \u01b0\u1edbc l\u01b0\u1ee3ng \u0111\u1ed9 l\u1edbn d\u1ef1a theo chu\u1ea9n 1</p> \\[ G[f(x,y)] = |G_x| + |G_y| \\] <p>ho\u1eb7c chu\u1ea9n v\u00f4 c\u00f9ng</p> \\[ G[f(x,y)] = \\max\\{|G_x|, |G_y|\\} \\] </li> </ol>"},{"location":"blog/edge-detection/#21-xap-xi-ao-ham","title":"2.1. X\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m","text":"<p>Trong \u1ea3nh s\u1ed1, do gi\u00e1 tr\u1ecb pixel l\u00e0 r\u1eddi r\u1ea1c n\u00ean \u0111\u1ea1o h\u00e0m th\u01b0\u1eddng \u0111\u01b0\u1ee3c t\u00ednh b\u1edfi s\u1ef1 sai kh\u00e1c gi\u1eefa gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c pixel g\u1ea7n nhau. M\u1ed9t c\u00e1ch t\u1ef1 nhi\u00ean ta ngh\u0129 \u0111\u1ebfn vi\u1ec7c x\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m nh\u01b0 sau (nh\u1edb r\u1eb1ng h\u01b0\u1edbng t\u0103ng c\u1ee7a \\(j\\) t\u01b0\u01a1ng \u1ee9ng v\u1edbi h\u01b0\u1edbng \\(Ox\\) v\u00e0 h\u01b0\u1edbng t\u0103ng c\u1ee7a \\(i\\) t\u01b0\u01a1ng \u1ee9ng v\u1edbi h\u01b0\u1edbng \\(-Oy\\)):</p> \\[ G_x \\approx f[i,j+1] - f[i,j] \\] \\[ G_y \\approx f[i,j] - f[i+1, j] \\] <p>C\u00e1ch x\u1ea5p x\u1ec9 n\u00e0y \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1edfi 2 kernel t\u00edch ch\u1eadp sau</p> \\[ s_x = \\begin{bmatrix} -1 &amp; 1\\end{bmatrix} \\qquad \\qquad s_y = \\begin{bmatrix} 1 \\\\ -1\\end{bmatrix} \\] <p>Tuy nhi\u00ean m\u1ed9t h\u1ea1n ch\u1ebf c\u1ee7a c\u00e1ch l\u00e0m n\u00e0y \u0111\u00f3 l\u00e0 vi\u1ec7c \u00e1p d\u1ee5ng c\u00f4ng th\u1ee9c tr\u00ean cho ta \u0111\u1ea1o h\u00e0m theo ph\u01b0\u01a1ng \\(Ox\\) t\u1ea1i \u0111i\u1ec3m \\([i, j+\\frac{1}{2}]\\) v\u00e0 \u0111\u1ea1o h\u00e0m theo ph\u01b0\u01a1ng \\(Oy\\) t\u1ea1i \u0111i\u1ec3m \\([i+\\frac{1}{2}, j]\\). \u0110\u1ec3 kh\u1eafc ph\u1ee5c t\u00ecnh tr\u1ea1ng n\u00e0y, ng\u01b0\u1eddi ta th\u01b0\u1eddng s\u1eed d\u1ee5ng kernel \\(2 \\times 2\\) thay v\u00ec kernel \\(2 \\times 1\\) hay \\(1 \\times 2\\) nh\u01b0 tr\u00ean:</p> \\[ s_x = \\begin{bmatrix} -1 &amp; 1 \\\\ -1 &amp; 1\\end{bmatrix} \\qquad \\qquad s_y = \\begin{bmatrix} 1 &amp; 1 \\\\ -1 &amp; -1\\end{bmatrix} \\] <p>Vi\u1ec7c \u00e1p d\u1ee5ng 2 kernel tr\u00ean s\u1ebd gi\u00fap ta x\u1ea5p x\u1ec9 \u0111\u01b0\u1ee3c \u0111\u1ea1o h\u00e0m t\u1ea1i \u0111i\u1ec3m \\([i+\\frac{1}{2}, j + \\frac{1}{2}]\\), nh\u01b0ng c\u00e1ch l\u00e0m n\u00e0y v\u1eabn c\u00f3 h\u1ea1n ch\u1ebf: v\u00ec \u0111i\u1ec3m \u0111\u01b0\u1ee3c t\u00ednh \u0111\u1ea1o h\u00e0m n\u1eb1m \u1edf gi\u1eefa c\u00e1c pixel ch\u1ee9 kh\u00f4ng thu\u1ed9c m\u1ed9t pixel n\u00e0o c\u1ea3 (\u0111i\u1ec3m n\u00e0y c\u00f2n \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 interpolation point, hay \u0111i\u1ec3m n\u1ed9i suy). M\u1ed9t h\u01b0\u1edbng ti\u1ebfp c\u1eadn m\u1edbi \u0111\u01b0\u1ee3c \u0111\u01b0a ra: d\u00f9ng kernel \\(3 \\times 3\\) (hay c\u00e1c kernel c\u00f3 s\u1ed1 pixel \u1edf m\u1ed7i chi\u1ec1u l\u00e0 m\u1ed9t s\u1ed1 l\u1ebb) \u0111\u1ec3 x\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m t\u1ea1i pixel trung t\u00e2m, \u0111a ph\u1ea7n c\u00e1c kernel (hay to\u00e1n t\u1eed) ph\u1ed5 bi\u1ebfn ng\u00e0y nay \u0111\u1ec1u s\u1eed d\u1ee5ng h\u01b0\u1edbng ti\u1ebfp c\u1eadn n\u00e0y.</p>"},{"location":"blog/edge-detection/#22-mot-so-toan-tu-pho-bien-su-dung-ao-ham-cap-mot","title":"2.2. M\u1ed9t s\u1ed1 to\u00e1n t\u1eed ph\u1ed5 bi\u1ebfn s\u1eed d\u1ee5ng \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t","text":"<ul> <li> <p>To\u00e1n t\u1eed Robert</p> <p>To\u00e1n t\u1eed Robert cho ta m\u1ed9t c\u00f4ng th\u1ee9c kh\u00e1 \u0111\u01a1n gi\u1ea3n \u0111\u1ec3 x\u1ea5p x\u1ec9 gradient t\u1ea1i m\u1ed9t \u0111i\u1ec3m theo ph\u01b0\u01a1ng \\(Ox\\) v\u00e0 \\(Oy\\):</p> \\[ G_x = f[i,j] - f[i+1,j+1] \\] \\[ G_y = f[i+1,j] - f[i,j+1] \\] <p>Hai gi\u00e1 tr\u1ecb \\(G_x\\), \\(G_y\\) n\u00e0y \u0111\u01b0\u1ee3c t\u00ednh b\u1edfi 2 kernel \\(2 \\times 2\\) sau</p> \\[ s_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1\\end{bmatrix} \\qquad \\qquad s_y = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0\\end{bmatrix} \\] <p>C\u00f4ng th\u1ee9c t\u00ednh \u0111\u1ea1o h\u00e0m c\u1ee7a to\u00e1n t\u1eed Robert kh\u00e1 \u0111\u01a1n gi\u1ea3n, \u0111\u1ed3ng th\u1eddi gi\u00e1 tr\u1ecb t\u00ednh \u0111\u01b0\u1ee3c thu\u1ed9c v\u1ec1 m\u1ed9t \u0111i\u1ec3m n\u1ed9i suy n\u1eb1m gi\u1eefa c\u00e1c pixel n\u00ean to\u00e1n t\u1eed Robert hi\u1ebfm khi \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong th\u1ef1c ti\u1ec5n.</p> </li> <li> <p>To\u00e1n t\u1eed Sobel</p> <p>\u0110\u1ed1i v\u1edbi to\u00e1n t\u1eed Sobel, \\(G_x\\), \\(G_y\\) \u0111\u01b0\u1ee3c t\u00ednh b\u1edfi 2 kernel \\(3 \\times 3\\) sau</p> \\[ s_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1\\\\ \\end{bmatrix} \\qquad \\qquad s_y = \\begin{bmatrix} 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; -2 &amp; -1\\\\ \\end{bmatrix} \\] <p>Ta c\u00f3 nh\u1eadn x\u00e9t r\u1eb1ng to\u00e1n t\u1eed Sobel x\u1ea5p x\u1ec9 \u0111\u01b0\u1ee3c \u0111\u1ea1o h\u00e0m t\u1ea1i \u0111i\u1ec3m n\u1eb1m trong pixel trung t\u00e2m nh\u01b0 \u0111\u00e3 n\u00f3i \u1edf tr\u00ean, \u0111\u1ed3ng th\u1eddi to\u00e1n t\u1eed n\u00e0y \u0111\u1eb7t tr\u1ecdng s\u1ed1 cao cho nh\u1eefng pixel n\u1eb1m g\u1ea7n trung t\u00e2m h\u01a1n n\u00ean n\u00f3 c\u00f2n c\u00f3 t\u00e1c d\u1ee5ng kh\u1eed nhi\u1ec5u. V\u00ec v\u1eady to\u00e1n t\u1eed Sobel l\u00e0 m\u1ed9t trong nh\u1eefng to\u00e1n t\u1eed d\u00f9ng \u0111\u1ec3 t\u00ednh gradient \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ph\u1ed5 bi\u1ebfn nh\u1ea5t.</p> <p>K\u1ebft qu\u1ea3 \u00e1p d\u1ee5ng to\u00e1n t\u1eed Sobel:</p> <p>      H\u00ecnh 7: \u1ea2nh g\u1ed1c \u0111\u00e3 \u0111\u01b0\u1ee3c chuy\u1ec3n sang d\u1ea1ng grayscale.      </p> <p>      H\u00ecnh 8: H\u00ecnh 1, 2: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c sau khi \u00e1p d\u1ee5ng 2 kernel \\(s_x\\), \\(s_y\\) l\u00ean \u1ea3nh g\u1ed1c. H\u00ecnh 3: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c sau khi t\u00ednh \u0111\u1ed9 l\u1edbn gradient t\u1ea1i m\u1ed7i pixel trong \u1ea3nh. H\u00ecnh 4: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c sau khi ph\u00e2n ng\u01b0\u1ee1ng h\u00ecnh 3 v\u1edbi ng\u01b0\u1ee1ng T = 100.      </p> <p>L\u01b0u \u00fd r\u1eb1ng khi d\u00f9ng kernel \u0111\u1ec3 t\u00ednh \u0111\u1ea1o h\u00e0m t\u1ea1i m\u1ed7i pixel, gi\u00e1 tr\u1ecb thu \u0111\u01b0\u1ee3c c\u00f3 th\u1ec3 l\u00e0 \u00e2m ho\u1eb7c d\u01b0\u01a1ng, \u0111\u1ec3 c\u00f3 th\u1ec3 bi\u1ec3u di\u1ec5n k\u1ebft qu\u1ea3 n\u00e0y d\u01b0\u1edbi d\u1ea1ng \u1ea3nh grayscale (\u1ea3nh m\u00e0 gi\u00e1 tr\u1ecb c\u00e1c pixel n\u1eb1m trong kho\u1ea3ng t\u1eeb 0 \u0111\u1ebfn 255), ta d\u00f9ng m\u1ed9t ph\u00e9p \u00e1nh x\u1ea1 chuy\u1ec3n gi\u00e1 tr\u1ecb nh\u1ecf nh\u1ea5t (l\u00fac n\u00e0y l\u00e0 m\u1ed9t s\u1ed1 \u00e2m) th\u00e0nh 0 v\u00e0 gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t th\u00e0nh 255. V\u00ec v\u1eady ta quan s\u00e1t th\u1ea5y b\u1ee9c \u1ea3nh thu \u0111\u01b0\u1ee3c \u0111a ph\u1ea7n c\u00f3 m\u00e0u x\u00e1m v\u00ec \u0111a s\u1ed1 gi\u00e1 tr\u1ecb \u0111\u1ea1o h\u00e0m n\u1eb1m \u1edf kho\u1ea3ng gi\u1eefa hai gi\u00e1 tr\u1ecb n\u00e0y, nh\u1eefng \u0111\u01b0\u1eddng m\u00e0u \u0111en l\u00e0 nh\u1eefng v\u1ecb tr\u00ed m\u00e0 t\u1ea1i \u0111\u00f3 gi\u00e1 tr\u1ecb \u0111\u1ea1o h\u00e0m \u0111\u1ea1t c\u1ef1c ti\u1ec3u, nh\u1eefng \u0111\u01b0\u1eddng m\u00e0u tr\u1eafng l\u00e0 nh\u1eefng v\u1ecb tr\u00ed m\u00e0 t\u1ea1i \u0111\u00f3 gi\u00e1 tr\u1ecb \u0111\u1ea1o h\u00e0m \u0111\u1ea1t c\u1ef1c \u0111\u1ea1i.</p> <p>B\u1ee9c \u1ea3nh th\u1ee9 ba bi\u1ec3u di\u1ec5n \u0111\u1ed9 l\u1edbn gradient t\u1ea1i m\u1ed7i pixel, thu \u0111\u01b0\u1ee3c b\u1eb1ng c\u00e1ch l\u1ea5y c\u0103n c\u1ee7a t\u1ed5ng b\u00ecnh ph\u01b0\u01a1ng gi\u00e1 tr\u1ecb pixel t\u01b0\u01a1ng \u1ee9ng \u1edf hai b\u1ee9c h\u00ecnh tr\u00ean (l\u00fac ch\u01b0a scale). B\u1ee9c \u1ea3nh cu\u1ed1i c\u00f9ng l\u00e0 k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c sau khi ph\u00e2n ng\u01b0\u1ee1ng b\u1ee9c \u1ea3nh th\u1ee9 ba, c\u00e1c pixel \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh c\u00f3 m\u00e0u tr\u1eafng, ng\u01b0\u1ee3c l\u1ea1i c\u00f3 m\u00e0u \u0111en (vi\u1ec7c ph\u00e2n ng\u01b0\u1ee1ng n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c n\u00f3i r\u00f5 h\u01a1n \u1edf m\u1ee5c 3).</p> <li> <p>To\u00e1n t\u1eed Prewitt</p> <p>To\u00e1n t\u1eed Prewitt gi\u1ed1ng v\u1edbi to\u00e1n t\u1eed Sobel, ngo\u1ea1i tr\u1eeb vi\u1ec7c to\u00e1n t\u1eed n\u00e0y kh\u00f4ng \u0111\u1eb7t tr\u1ecdng s\u1ed1 cao cho nh\u1eefng pixel n\u1eb1m g\u1ea7n trung t\u00e2m, t\u1ee9c xem nh\u1eefng pixel xung quanh c\u00f3 vai tr\u00f2 ngang b\u1eb1ng nhau trong vi\u1ec7c quy\u1ebft \u0111\u1ecbnh \u0111\u1ea1o h\u00e0m t\u1ea1i \u0111i\u1ec3m n\u1eb1m trong pixel trung t\u00e2m. 2 kernel \\(s_x\\) v\u00e0 \\(s_y\\) c\u1ee7a to\u00e1n t\u1eed n\u00e0y c\u00f3 d\u1ea1ng nh\u01b0 sau</p> \\[ s_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1\\\\ \\end{bmatrix} \\qquad \\qquad s_y = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; -1 &amp; -1\\\\ \\end{bmatrix} \\] </li>"},{"location":"blog/edge-detection/#23-anh-huong-cua-o-lon-kernel-en-kha-nang-nhan-dien-canh","title":"2.3. \u1ea2nh h\u01b0\u1edfng c\u1ee7a \u0111\u1ed9 l\u1edbn kernel \u0111\u1ebfn kh\u1ea3 n\u0103ng nh\u1eadn di\u1ec7n c\u1ea1nh","text":"<p>M\u1ed9t kernel c\u00f3 k\u00edch th\u01b0\u1edbc nh\u1ecf s\u1ebd x\u00e1c \u0111\u1ecbnh \u0111\u01b0\u1ee3c v\u1ecb tr\u00ed c\u1ee7a c\u1ea1nh v\u1edbi \u0111\u1ed9 ch\u00ednh x\u00e1c cao (good localization) nh\u01b0ng k\u1ebft qu\u1ea3 t\u00ednh to\u00e1n \u0111\u01b0\u1ee3c l\u1ea1i r\u1ea5t d\u1ec5 b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nhi\u1ec5u (noise sensitive): ta ch\u1ec9 s\u1eed d\u1ee5ng 4 hay 9 gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c pixel l\u00e2n c\u1eadn \u0111\u1ec3 x\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m, n\u1ebfu nh\u01b0 m\u1ed9t trong nh\u1eefng pixel n\u00e0y ch\u1ee9a nhi\u1ec5u th\u00ec k\u1ebft qu\u1ea3 b\u1ecb \u1ea3nh h\u01b0\u1edfng r\u1ea5t nhi\u1ec1u. M\u1ed9t kernel l\u1edbn s\u1ebd gi\u1ea3m \u0111\u01b0\u1ee3c \u1ea3nh h\u01b0\u1edfng c\u1ee7a nhi\u1ec5u, nh\u01b0ng \u0111\u1ed9 ch\u00ednh x\u00e1c khi x\u00e1c \u0111\u1ecbnh v\u1ecb tr\u00ed c\u1ee7a c\u1ea1nh l\u1ea1i kh\u00f4ng cao (poor localization): gi\u00e1 tr\u1ecb \u0111\u1ea1o h\u00e0m x\u1ea5p x\u1ec9 cho m\u1ed9t pixel b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nh\u1eefng th\u00f4ng tin \u1edf xa.</p>"},{"location":"blog/edge-detection/#3-cac-buoc-cua-mot-thuat-toan-nhan-dien-canh","title":"3. C\u00e1c b\u01b0\u1edbc c\u1ee7a m\u1ed9t thu\u1eadt to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh","text":"<p>B\u01b0\u1edbc 1: Filtering (l\u1ecdc nhi\u1ec5u)</p> <p>Vi\u1ec7c t\u00ednh to\u00e1n \u0111\u1ea1o h\u00e0m \u0111\u1ed1i v\u1edbi c\u00e1c kernel nh\u1ecf (\\(2 \\times 2\\), \\(3 \\times 3\\)) r\u1ea5t d\u1ec5 b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nhi\u1ec5u b\u1edfi n\u00f3 ch\u1ec9 d\u1ef1a tr\u00ean m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a 2 hay 3 pixel n\u1eb1m k\u1ec1 nhau (m\u00ecnh s\u1ebd n\u00f3i r\u00f5 h\u01a1n \u1edf ph\u1ea7n d\u01b0\u1edbi), ch\u00ednh v\u00ec v\u1eady tr\u01b0\u1edbc khi \u0111i v\u00e0o thu\u1eadt to\u00e1n ch\u00ednh th\u00ec l\u1ecdc nhi\u1ec5u l\u00e0 m\u1ed9t b\u01b0\u1edbc kh\u00f4ng th\u1ec3 thi\u1ebfu. Tuy nhi\u00ean c\u1ea7n l\u01b0u \u00fd v\u1ec1 m\u1ed1i t\u01b0\u01a1ng quan gi\u1eefa m\u1ee9c \u0111\u1ed9 l\u1ecdc nhi\u1ec5u v\u00e0 \u0111\u1ed9 s\u1eafc c\u1ee7a c\u1ea1nh: c\u00e0ng l\u1ecdc \u0111\u01b0\u1ee3c nhi\u1ec1u nhi\u1ec5u th\u00ec c\u1ea1nh s\u1ebd c\u00e0ng m\u1edd.</p> <p>B\u01b0\u1edbc 2: Enhancement</p> <p>Vi\u1ec7c nh\u1eadn di\u1ec7n c\u1ea1nh kh\u00f4ng d\u1ef1a tr\u00ean gi\u00e1 tr\u1ecb m\u1ed7i pixel m\u1ed9t c\u00e1ch ri\u00eang l\u1ebb m\u00e0 d\u1ef1a tr\u00ean m\u1ed1i t\u01b0\u01a1ng quan c\u1ee7a pixel \u0111ang x\u00e9t v\u1edbi c\u00e1c pixel l\u00e2n c\u1eadn. \u1ede b\u01b0\u1edbc n\u00e0y, ta nh\u1ea5n m\u1ea1nh s\u1ef1 thay \u0111\u1ed5i m\u1ee9c \u0111\u1ed9 x\u00e1m gi\u1eefa c\u00e1c pixel b\u1eb1ng c\u00e1ch bi\u1ec3u di\u1ec5n m\u1ed7i pixel b\u1eb1ng \u0111\u1ed9 l\u1edbn gradient t\u1ea1i \u0111i\u1ec3m \u0111\u00f3.</p> <p>B\u01b0\u1edbc 3: Detection (nh\u1eadn di\u1ec7n c\u1ea1nh)</p> <p>Sau b\u01b0\u1edbc 2, ta thu \u0111\u01b0\u1ee3c m\u1ed9t b\u1ee9c \u1ea3nh v\u1edbi gi\u00e1 tr\u1ecb m\u1ed7i pixel l\u00e0 \u0111\u1ed9 l\u1edbn \u0111\u1ea1o c\u1ee7a gradient t\u1ea1i pixel \u0111\u00f3. \u0110\u00e2y ch\u01b0a ph\u1ea3i l\u00e0 \u0111\u1ea7u ra m\u00e0 ch\u00fang ta mong mu\u1ed1n. Nh\u01b0 \u0111\u00e3 n\u00f3i \u1edf tr\u00ean, nh\u1eefng pixel c\u00f3 \u0111\u1ed9 l\u1edbn gradient l\u1edbn th\u00ec m\u1edbi l\u00e0 c\u1ea1nh, c\u00f2n nh\u1eefng pixel m\u00e0 t\u1ea1i \u0111\u00f3 c\u00f3 s\u1ef1 thay \u0111\u1ed5i \u0111\u1ea1o h\u00e0m nh\u01b0ng kh\u00f4ng \u0111\u00e1ng k\u1ec3 th\u00ec ch\u1ec9 l\u00e0 nhi\u1ec5u. V\u00ec v\u1eady ta c\u00f2n ph\u1ea3i th\u1ef1c hi\u1ec7n th\u00eam m\u1ed9t b\u01b0\u1edbc n\u1eefa, \u0111\u00f3 l\u00e0 ph\u00e2n ng\u01b0\u1ee1ng (thresholding). Sau b\u01b0\u1edbc n\u00e0y, ta s\u1ebd thu \u0111\u01b0\u1ee3c m\u1ed9t b\u1ee9c \u1ea3nh v\u1edbi gi\u00e1 tr\u1ecb m\u1ed7i pixel l\u00e0 m\u1ed9t gi\u00e1 tr\u1ecb nh\u1ecb ph\u00e2n (0 n\u1ebfu pixel kh\u00f4ng ph\u1ea3i l\u00e0 c\u1ea1nh v\u00e0 1 n\u1ebfu pixel l\u00e0 c\u1ea1nh).</p>"},{"location":"blog/edge-detection/#4-phuong-phap-ao-ham-cap-hai-cho-bai-toan-nhan-dien-canh","title":"4. Ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ea1o h\u00e0m c\u1ea5p hai cho b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh","text":"<p>\u1ede ph\u01b0\u01a1ng ph\u00e1p tr\u00ean, ta t\u00ednh \u0111\u1ed9 l\u1edbn gradient t\u1ea1i m\u1ed7i pixel r\u1ed3i d\u00f9ng m\u1ed9t ng\u01b0\u1ee1ng \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh m\u1ed9t pixel c\u00f3 ph\u1ea3i l\u00e0 m\u1ed9t c\u1ea1nh hay kh\u00f4ng, d\u1eabn \u0111\u1ebfn vi\u1ec7c c\u00f3 qu\u00e1 nhi\u1ec1u pixel \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh. \u0110\u1ec3 kh\u1eafc ph\u1ee5c h\u1ea1n ch\u1ebf n\u00e0y, m\u1ed9t c\u00e1ch ti\u1ebfp c\u1eadn kh\u00e1c l\u00e0 \u0111\u1ed1i v\u1edbi nhi\u1ec1u \u0111i\u1ec3m c\u1ea1nh n\u1eb1m k\u1ec1 nhau, ta ch\u1ec9 nh\u1eadn nh\u1eefng pixel c\u00f3 \u0111\u1ed9 l\u1edbn gradient \u0111\u1ea1t c\u1ef1c \u0111\u1ea1i c\u1ee5c b\u1ed9 (local maxima). Ta c\u0169ng bi\u1ebft r\u1eb1ng, gi\u00e1 tr\u1ecb c\u1ef1c \u0111\u1ea1i c\u1ee7a \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi giao \u0111i\u1ec3m c\u1ee7a \u0111\u1ea1o h\u00e0m c\u1ea5p hai v\u00e0 tr\u1ee5c ho\u00e0nh, \u0111i\u1ec3m n\u00e0y c\u00f2n \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 \u0111i\u1ec3m v\u1ec1 kh\u00f4ng (zero crossing).</p> <p>      H\u00ecnh 9: \u0110\u1ed3 th\u1ecb \u0111\u1ea7u ti\u00ean bi\u1ec3u di\u1ec5n gi\u00e1 tr\u1ecb m\u1ee9c \u0111\u1ed9 x\u00e1m c\u1ee7a m\u1ed7i pixel trong kh\u00f4ng gian 1 chi\u1ec1u (xem gi\u00e1 tr\u1ecb n\u00e0y l\u00e0 m\u1ed9t h\u00e0m li\u00ean t\u1ee5c), \u0111\u1ed3 th\u1ecb th\u1ee9 hai l\u00e0 \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a gi\u00e1 tr\u1ecb n\u00e0y, \u0111\u1ed3 th\u1ecb cu\u1ed1i c\u00f9ng l\u00e0 \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u1ee7a gi\u00e1 tr\u1ecb n\u00e0y. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>C\u00f3 hai to\u00e1n t\u1eed g\u1eafn v\u1edbi \u0111\u1ea1o h\u00e0m c\u1ea5p hai trong kh\u00f4ng gian hai chi\u1ec1u: to\u00e1n t\u1eed Laplacian (Laplacian operator) v\u00e0 to\u00e1n t\u1eed \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng (second directional derivative)</p>"},{"location":"blog/edge-detection/#41-toan-tu-laplacian","title":"4.1. To\u00e1n t\u1eed Laplacian","text":"<p>To\u00e1n t\u1eed Laplacian \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a b\u1eb1ng t\u1ed5ng \u0111\u1ea1o h\u00e0m ri\u00eang c\u1ea5p hai theo ph\u01b0\u01a1ng \\(Ox\\) v\u00e0 \\(Oy\\)</p> \\[ \\nabla^2f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} \\] <p>Trong \u0111\u00f3 \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u1ee7a \\(f\\) theo ph\u01b0\u01a1ng \\(Ox\\) \u0111\u01b0\u1ee3c x\u1ea5p x\u1ec9 nh\u01b0 sau:</p> \\[ \\begin{aligned} \\frac{\\partial^2 f}{\\partial x^2} &amp; = \\frac{\\partial G_x}{\\partial x} \\\\ &amp; = \\frac{\\partial f[i,j+1] - \\partial f[i,j]}{\\partial x} \\\\ &amp; = \\frac{\\partial f[i,j+1]}{\\partial x} - \\frac{\\partial f[i,j]}{\\partial x} \\\\ &amp; = (f[i,j+2] - f[i,j+1]) - (f[i,j+1] - f[i,j]) \\\\ &amp; = f[i,j+2] - 2f[i,j+1] + f[i,j] \\\\ \\end{aligned} \\] <p>Tuy nhi\u00ean c\u00f4ng th\u1ee9c tr\u00ean cho ta x\u1ea5p x\u1ec9 \u0111\u1ea1o h\u00e0m ri\u00eang c\u1ea5p hai theo ph\u01b0\u01a1ng \\(Ox\\) t\u1ea1i pixel c\u00f3 v\u1ecb tr\u00ed \\([i,j+1]\\), \u0111\u1ec3 t\u00ednh \u0111\u1ea1o h\u00e0m n\u00e0y t\u1ea1i v\u1ecb tr\u00ed \\([i,j]\\) ta thay \\(j\\) b\u1edfi \\(j-1\\):</p> \\[ \\begin{aligned} \\frac{\\partial^2 f}{\\partial x^2} &amp; = f[i,j+1] - 2f[i,j] + f[i,j-1] \\end{aligned} \\] <p>T\u01b0\u01a1ng t\u1ef1, ta c\u00f3 \u0111\u1ea1o h\u00e0m ri\u00eang c\u1ea5p 2 theo ph\u01b0\u01a1ng \\(Oy\\) t\u1ea1i pixel c\u00f3 v\u1ecb tr\u00ed \\([i,j]\\):</p> \\[ \\frac{\\partial^2 f}{\\partial y^2} = f[i+1,j] - 2f[i,j] + f[i-1,j] \\] <p>T\u1ed5ng hai gi\u00e1 tr\u1ecb tr\u00ean cho ta to\u00e1n t\u1eed Laplacian. Ta c\u00f3 kernel t\u00edch ch\u1eadp c\u1ee7a to\u00e1n t\u1eed n\u00e0y nh\u01b0 sau:</p> \\[ \\nabla^2 \\approx \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; -4 &amp; 1 \\\\ 0 &amp; 1 &amp; 0\\end{bmatrix} \\] <p>Sau khi d\u00f9ng kernel \u0111\u1ec3 t\u00ednh to\u00e1n gi\u00e1 tr\u1ecb Laplacian c\u1ee7a m\u1ed7i pixel trong \u1ea3nh, to\u00e1n t\u1eed Laplacian s\u1ebd nh\u1eadn di\u1ec7n m\u1ed9t pixel l\u00e0 c\u1ea1nh n\u1ebfu qua n\u00f3 c\u00f3 s\u1ef1 chuy\u1ec3n h\u00f3a qua \u0111i\u1ec3m kh\u00f4ng. Kh\u00f4ng ph\u1ea3i b\u1ea5t k\u00ec pixel n\u00e0o mang gi\u00e1 tr\u1ecb 0 c\u0169ng \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh, v\u00ed d\u1ee5 nh\u1eefng v\u00f9ng c\u00f3 c\u00e1c pixel \u0111\u1ec1u mang gi\u00e1 tr\u1ecb 0 (trivial zeros) th\u00ec kh\u00f4ng \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh.</p> <p>T\u1eeb c\u00e1c c\u00f4ng th\u1ee9c tr\u00ean v\u00e0 kernel t\u00edch ch\u1eadp c\u1ee7a to\u00e1n t\u1eed Laplacian, ta nh\u1eadn th\u1ea5y r\u1eb1ng tuy \u0111\u01b0\u1ee3c \u0111\u1ecbnh ngh\u0129a d\u1ef1a tr\u00ean \u0111\u1ea1o h\u00e0m ri\u00eang theo ph\u01b0\u01a1ng \\(Ox\\) v\u00e0 \\(Oy\\), to\u00e1n t\u1eed Laplacian l\u00e0 m\u1ed9t \u0111\u1ea1o h\u00e0m v\u00f4 h\u01b0\u1edbng. Ngh\u0129a l\u00e0 n\u1ebfu ta t\u00ednh gi\u00e1 tr\u1ecb Laplacian t\u1ea1i m\u1ed9t pixel nh\u1ea5t \u0111\u1ecbnh, n\u1ebfu ta xoay b\u1ee9c \u1ea3nh 2D xung quanh pixel n\u00e0y theo b\u1ea5t k\u00ec chi\u1ec1u n\u00e0o th\u00ec gi\u00e1 tr\u1ecb Laplacian n\u00e0y v\u1eabn kh\u00f4ng thay \u0111\u1ed5i (\u0111i\u1ec1u n\u00e0y l\u00e0 kh\u00f4ng \u0111\u00fang \u0111\u1ed1i v\u1edbi \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng m\u00e0 ta s\u1ebd \u0111\u1ec1 c\u1eadp \u0111\u1ebfn \u1edf ph\u1ea7n ti\u1ebfp theo).</p>"},{"location":"blog/edge-detection/#42-ao-ham-cap-hai-co-huong","title":"4.2. \u0110\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng","text":"<p>\u0110\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng l\u00e0 \u0111\u1ea1o h\u00e0m c\u1ea5p hai \u0111\u01b0\u1ee3c t\u00ednh theo h\u01b0\u1edbng c\u1ee7a gradient. S\u1eed d\u1ee5ng \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng kh\u00e1c v\u1edbi s\u1eed d\u1ee5ng Laplacian \u1edf ch\u1ed7, n\u1ebfu nh\u01b0 Laplacian nh\u1eadn m\u1ed9t \u0111i\u1ec3m l\u00e0 c\u1ea1nh n\u1ebfu nh\u01b0 \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a n\u00f3 theo ph\u01b0\u01a1ng \\(Ox\\) v\u00e0 \\(Oy\\) \u0111\u1ec1u \u0111\u1ea1t c\u1ef1c tr\u1ecb, th\u00ec \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng s\u1ebd nh\u1eadn m\u1ed9t \u0111i\u1ec3m l\u00e0 c\u1ea1nh n\u1ebfu nh\u01b0 n\u00f3 l\u00e0 \u0111i\u1ec3m v\u1ec1 kh\u00f4ng \u0111\u1ed1i v\u1edbi \u0111\u1ea1o h\u00e0m c\u1ea5p hai theo h\u01b0\u1edbng vu\u00f4ng g\u00f3c v\u1edbi h\u01b0\u1edbng c\u1ee7a c\u1ea1nh (h\u01b0\u1edbng gradient).</p> \\[ \\frac{\\partial^2 f}{\\partial n^2} = \\frac{f_x^2f_{xx} + 2f_xf_yf_{xy} + f_y^2f_{yy}}{f_x^2 + f_y^2} \\] <p>Ph\u1ea7n ch\u1ee9ng minh d\u01b0\u1edbi \u0111\u00e2y d\u00e0nh cho b\u1ea1n \u0111\u1ecdc mu\u1ed1n hi\u1ec3u th\u00eam v\u1ec1 c\u00f4ng th\u1ee9c tr\u00ean, kh\u00f4ng li\u00ean quan \u0111\u1ebfn nh\u1eefng ph\u1ea7n kh\u00e1c n\u00ean ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c b\u1ecf qua.</p> Ch\u1ee9ng minh <p>Gradient t\u1ea1i \u0111i\u1ec3m \\((x,y)\\): \\(\\nabla f = (f_x, f_y)\\)</p> <p>Vector \u0111\u01a1n v\u1ecb c\u1ee7a vector gradient t\u1ea1i \u0111i\u1ec3m \\((x,y)\\): \\(\\overrightarrow{n}_0 = \\frac{1}{\\sqrt{f_x^2 + f_y^2}}(f_x,f_y)\\)</p> <p>\u0110\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a h\u00e0m \\(f\\) t\u1ea1i \u0111i\u1ec3m \\((x,y)\\) theo h\u01b0\u1edbng c\u1ee7a vector \\(\\overrightarrow{n}\\):</p> \\[ \\frac{\\partial f}{\\partial n} = \\nabla f \\cdot \\overrightarrow{n}_0 = f_x \\cdot \\frac{f_x}{\\sqrt{f_x^2+f_y^2}} + f_y \\cdot \\frac{f_y}{\\sqrt{f_x^2 + f_y^2}} = \\sqrt{f_x^2 + f_y^2} \\] <p>\u0110\u1ea1o h\u00e0m c\u1ea5p hai c\u1ee7a h\u00e0m \\(f\\) t\u1ea1i \u0111i\u1ec3m \\((x,y)\\) theo h\u01b0\u1edbng c\u1ee7a vector \\(\\overrightarrow{n}\\):</p> \\[ \\begin{aligned} \\frac{\\partial^2 f}{\\partial n^2} &amp; = \\frac{\\partial \\left(\\sqrt{f_x^2+f_y^2}\\right)}{\\partial n} \\\\ &amp; = \\frac{f_xf_{xx}+f_yf_{xy}}{\\sqrt{f_x^2+f_y^2}} \\cdot \\frac{f_x}{\\sqrt{f_x^2+f_y^2}} + \\frac{f_xf_{xy}+f_yf_{yy}}{\\sqrt{f_x^2+f_y^2}} \\cdot \\frac{f_y}{\\sqrt{f_x^2+f_y^2}} \\\\ &amp; = \\frac{f_x^2f_{xx} + 2f_xf_yf_{xy} + f_y^2f_{yy}}{f_x^2+f_y^2} \\end{aligned} \\] <p>Tuy mang l\u1ea1i k\u1ebft qu\u1ea3 t\u1ed1t trong tr\u01b0\u1eddng h\u1ee3p l\u00fd t\u01b0\u1edfng, c\u00e1c to\u00e1n t\u1eed li\u00ean quan \u0111\u1ebfn \u0111\u1ea1o h\u00e0m c\u1ea5p hai l\u1ea1i hi\u1ebfm khi \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng b\u1edfi ch\u00fang r\u1ea5t d\u1ec5 b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nhi\u1ec5u: ch\u1ec9 m\u1ed9t dao \u0111\u1ed9ng nh\u1ecf c\u1ee7a \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t t\u1ea1o th\u00e0nh m\u1ed9t \u0111\u1ec9nh s\u1ebd d\u1eabn \u0111\u1ebfn s\u1ef1 t\u1ea1o th\u00e0nh m\u1ed9t \u0111i\u1ec3m v\u1ec1 kh\u00f4ng c\u1ee7a \u0111\u1ea1o h\u00e0m c\u1ea5p hai. \u0110\u1ec3 gi\u1ea3m s\u1ef1 \u1ea3nh h\u01b0\u1edfng c\u1ee7a nhi\u1ec5u, ng\u01b0\u1eddi ta d\u00f9ng c\u00e1c b\u1ed9 l\u1ecdc nhi\u1ec5u (filter) m\u00e0 n\u1ed5i b\u1eadt l\u00e0 Gaussian filter. Trong ph\u1ea7n ti\u1ebfp theo, ch\u00fang ta s\u1ebd c\u00f9ng nhau t\u00ecm hi\u1ec3u v\u1ec1 h\u01b0\u1edbng ti\u1ebfp c\u1eadn k\u1ebft h\u1ee3p Gaussian filter v\u00e0 to\u00e1n t\u1eed Laplacian, \u0111\u1ed3ng th\u1eddi s\u1eed d\u1ee5ng \u0111\u1ecbnh l\u00fd \u0111\u1ea1o h\u00e0m c\u1ee7a ph\u00e9p t\u00edch ch\u1eadp (Derivative Theorem of Convolution) \u0111\u1ec3 r\u00fat g\u1ecdn c\u00e1c b\u01b0\u1edbc t\u00ednh to\u00e1n c\u1ee7a thu\u1eadt to\u00e1n n\u00e0y.</p>"},{"location":"blog/edge-detection/#5-phuong-phap-tim-canh-tren-anh-nhieu","title":"5. Ph\u01b0\u01a1ng ph\u00e1p t\u00ecm c\u1ea1nh tr\u00ean \u1ea3nh nhi\u1ec5u","text":""},{"location":"blog/edge-detection/#51-anh-huong-cua-nhieu-en-ao-ham","title":"5.1. \u1ea2nh h\u01b0\u1edfng c\u1ee7a nhi\u1ec5u \u0111\u1ebfn \u0111\u1ea1o h\u00e0m","text":"<p>\u0110\u1ec3 c\u00f3 th\u1ec3 h\u00ecnh dung r\u00f5 r\u00e0ng h\u01a1n v\u1ec1 t\u1ea7m \u1ea3nh h\u01b0\u1edfng c\u1ee7a nhi\u1ec5u \u0111\u1ebfn b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh, ch\u00fang ta h\u00e3y c\u00f9ng nhau quan s\u00e1t b\u1ee9c h\u00ecnh d\u01b0\u1edbi \u0111\u00e2y. L\u01b0u \u00fd r\u1eb1ng ch\u00fang ta \u0111ang gi\u1ea3 s\u1eed b\u1ee9c \u1ea3nh ta \u0111ang x\u00e9t l\u00e0 1D.</p> <p>      H\u00ecnh 10: H\u00ecnh 1, 2: \u0110\u1ed3 th\u1ecb bi\u1ec3u di\u1ec5n m\u1ed9t c\u1ea1nh v\u00e0 \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a c\u1ee7a n\u00f3. H\u00ecnh 3, 4: \u0110\u1ed3 th\u1ecb bi\u1ec3u di\u1ec5n c\u1ea1nh t\u01b0\u01a1ng \u1ee9ng \u0111\u00e3 \u0111\u01b0\u1ee3c th\u00eam nhi\u1ec5u v\u00e0 \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a n\u00f3. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>Ta th\u1ea5y r\u1eb1ng v\u1edbi s\u1ef1 xu\u1ea5t hi\u1ec7n c\u1ee7a nhi\u1ec5u, ta kh\u00f4ng th\u1ec3 x\u00e1c \u0111\u1ecbnh \u0111\u00e2u l\u00e0 v\u1ecb tr\u00ed c\u1ee7a c\u1ea1nh d\u1ef1a tr\u00ean \u0111\u1ed3 th\u1ecb \u0111\u1ea1o h\u00e0m n\u1eefa.</p> <p>\u0110\u1ec3 kh\u1eafc ph\u1ee5c t\u00ecnh tr\u1ea1ng n\u00e0y, ta s\u1eed d\u1ee5ng Gaussian filter \u0111\u1ec3 lo\u1ea1i b\u1ecf nhi\u1ec5u tr\u01b0\u1edbc khi t\u00ednh \u0111\u1ea1o h\u00e0m. K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c kh\u1ea3 quan h\u01a1n r\u1ea5t nhi\u1ec1u:</p> <p>      H\u00ecnh 11: \u0110\u1ed3 th\u1ecb bi\u1ec3u di\u1ec5n c\u1ea1nh sau khi \u0111\u01b0\u1ee3c kh\u1eed nhi\u1ec5u b\u1edfi Gaussian filter v\u00e0 \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u1ee7a n\u00f3. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p>"},{"location":"blog/edge-detection/#52-ao-ham-cua-gaussian-derivative-of-gaussian","title":"5.2. \u0110\u1ea1o h\u00e0m c\u1ee7a Gaussian (Derivative of Gaussian)","text":"<p>M\u1ed9t c\u00e1ch t\u1ef1 nhi\u00ean, ta r\u00fat ra quy tr\u00ecnh t\u00ecm \u0111\u1ea1o h\u00e0m c\u1ee7a b\u1ee9c \u1ea3nh nh\u01b0 sau: d\u00f9ng Gaussian kernel \u0111\u1ec3 th\u1ef1c hi\u1ec7n ph\u00e9p t\u00edch ch\u1eadp l\u00ean \u1ea3nh \u0111ang b\u1ecb nhi\u1ec5u, sau \u0111\u00f3 t\u00ednh \u0111\u1ea1o h\u00e0m \u0111\u1ed1i v\u1edbi b\u1ee9c \u1ea3nh \u0111\u00e3 \u0111\u01b0\u1ee3c kh\u1eed nhi\u1ec5u.</p> <p>      H\u00ecnh 12: C\u00e1c b\u01b0\u1edbc c\u1ee7a m\u1ed9t b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh \u0111\u1ed1i v\u1edbi \u1ea3nh nhi\u1ec5u (C\u00e1ch 1: L\u1ea5y \u0111\u1ea1o h\u00e0m c\u1ee7a \u1ea3nh sau khi \u0111\u00e3 \u00e1p d\u1ee5ng Gaussian filter cho \u1ea3nh). \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>Tuy nhi\u00ean, ta c\u00f3 th\u1ec3 r\u00fat g\u1ecdn quy tr\u00ecnh n\u00e0y b\u1eb1ng c\u00e1ch l\u1ea5y \u0111\u1ea1o h\u00e0m c\u1ee7a Gaussian kernel \u0111\u1ec3 t\u1ea1o th\u00e0nh m\u1ed9t kernel m\u1edbi, sau \u0111\u00f3 d\u00f9ng ph\u00e9p t\u00edch ch\u1eadp \u0111\u1ed1i v\u1edbi kernel n\u00e0y v\u00e0 b\u1ee9c \u1ea3nh g\u1ed1c (c\u00f3 ch\u1ee9a nhi\u1ec5u). Quy tr\u00ecnh n\u00e0y gi\u00fap ta ti\u1ebft ki\u1ec7m m\u1ed9t l\u01b0\u1ee3ng l\u1edbn th\u1eddi gian v\u00ec th\u00f4ng th\u01b0\u1eddng k\u00edch th\u01b0\u1edbc c\u1ee7a b\u1ee9c \u1ea3nh l\u1edbn h\u01a1n k\u00edch th\u01b0\u1edbc c\u1ee7a kernel r\u1ea5t nhi\u1ec1u. M\u1ed9t tr\u01b0\u1eddng h\u1ee3p m\u00e0 vi\u1ec7c l\u1ea5y \u0111\u1ea1o h\u00e0m c\u1ee7a Gaussian kernel \u0111\u1eb7c bi\u1ec7t h\u1eefu d\u1ee5ng \u0111\u00f3 l\u00e0 khi ta mu\u1ed1n \u00e1p d\u1ee5ng kernel n\u00e0y cho nhi\u1ec1u b\u1ee9c \u1ea3nh kh\u00e1c nhau.</p> <p>      H\u00ecnh 13: C\u00e1c b\u01b0\u1edbc c\u1ee7a m\u1ed9t b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh \u0111\u1ed1i v\u1edbi \u1ea3nh nhi\u1ec5u (C\u00e1ch 2: L\u1ea5y \u0111\u1ea1o h\u00e0m c\u1ee7a Gaussian kernel, sau \u0111\u00f3 m\u1edbi \u00e1p d\u1ee5ng kernel n\u00e0y cho \u1ea3nh). \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>Vi\u1ec7c r\u00fat g\u1ecdn n\u00e0y d\u1ef1a tr\u00ean t\u00ednh tuy\u1ebfn t\u00ednh c\u1ee7a ph\u00e9p t\u00edch ch\u1eadp (ch\u00fang ta s\u1ebd kh\u00f4ng \u0111i s\u00e2u v\u00e0o ch\u1ee9ng minh c\u00f4ng th\u1ee9c n\u00e0y), c\u1ee5 th\u1ec3:</p> \\[ \\frac{d}{dx}(g*f) = \\left(\\frac{d}{dx}g\\right)*f \\]"},{"location":"blog/edge-detection/#53-laplacian-cua-gaussian-laplacian-of-gaussian-log","title":"5.3. Laplacian c\u1ee7a Gaussian (Laplacian of Gaussian - LoG)","text":"<p>T\u01b0\u01a1ng t\u1ef1 nh\u01b0 khi l\u1ea5y \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t, ta c\u0169ng c\u00f3 th\u1ec3 d\u00f9ng b\u1ed9 l\u1ecdc Gaussian \u0111\u1ec3 kh\u1eed nhi\u1ec5u tr\u01b0\u1edbc khi \u00e1p d\u1ee5ng to\u00e1n t\u1eed Laplacian l\u00ean \u1ea3nh, quy tr\u00ecnh n\u00e0y c\u0169ng c\u00f3 th\u1ec3 r\u00fat g\u1ecdn th\u00e0nh hai b\u01b0\u1edbc nh\u01b0 tr\u00ean:</p> <ol> <li>\u00c1p d\u1ee5ng to\u00e1n t\u1eed Laplacian l\u00ean Gaussian kernel \u0111\u1ec3 t\u1ea1o th\u00e0nh kernel m\u1edbi, kernel n\u00e0y c\u00f2n \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 Laplacian c\u1ee7a Gaussian (Laplacian of Gaussian hay LoG)</li> <li>Th\u1ef1c hi\u1ec7n ph\u00e9p t\u00edch ch\u1eadp \u0111\u1ed1i v\u1edbi kernel m\u1edbi t\u1ea1o th\u00e0nh v\u00e0 \u1ea3nh g\u1ed1c (ch\u01b0a kh\u1eed nhi\u1ec5u)</li> </ol> <p>Vi\u1ec7c r\u00fat g\u1ecdn n\u00e0y c\u0169ng d\u1ef1a tr\u00ean m\u1ed9t t\u00ednh ch\u1ea5t c\u1ee7a ph\u00e9p t\u00edch ch\u1eadp:</p> \\[ \\nabla^2(g*f) = \\nabla^2g*f \\] <p>Trong \u0111\u00f3 $ g $ l\u00e0 m\u1ed9t h\u00e0m Gaussian hai bi\u1ebfn s\u1ed1</p> \\[ g(x,y) = \\frac{1}{{2\\pi}\\sigma^2}e^{-\\frac{x^2+y^2}{2\\sigma^2}} \\] <p>Suy ra</p> \\[ \\begin{aligned} \\nabla^2g(x,y) &amp; = \\frac{1}{2 \\pi \\sigma^2}(\\frac{x^2}{\\sigma^4} - \\frac{1}{\\sigma^2})e^{-\\frac{x^2+y^2}{2 \\sigma^2}} + \\frac{1}{2 \\pi \\sigma^2}(\\frac{y^2}{\\sigma^4} - \\frac{1}{\\sigma^2})e^{-\\frac{x^2+y^2}{2 \\sigma^2}} \\\\ &amp; = -\\frac{1}{\\pi \\sigma^4}\\left( 1 -\\frac{x^2 + y^2}{2 \\sigma^2}\\right)e^{-\\frac{x^2+y^2}1{2\\sigma^2}}\\end{aligned} \\] <p>Thu\u1eadt to\u00e1n tr\u00ean \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 Laplacian of Gausian (LoG) hay Marr-Hildreth detector (thu\u1eadt to\u00e1n \u0111\u01b0\u1ee3c \u0111\u1ec1 xu\u1ea5t b\u1edfi D. Marr v\u00e0 E. Hildreth v\u00e0o n\u0103m 1980). To\u00e1n t\u1eed LoG c\u00f2n \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 to\u00e1n t\u1eed Mexican Hat v\u00ec h\u00ecnh d\u1ea1ng \u0111\u1eb7c bi\u1ec7t c\u1ee7a n\u00f3:</p> <p>      H\u00ecnh 14: H\u00ecnh d\u1ea1ng c\u1ee7a Gaussian kernel sau khi \u0111\u01b0\u1ee3c \u00e1p d\u1ee5ng to\u00e1n t\u1eed Laplacian gi\u1ed1ng nh\u01b0 m\u1ed9t chi\u1ebfc m\u0169. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p>"},{"location":"blog/edge-detection/#6-canny-edge-detector","title":"6. Canny Edge Detector","text":"<p>\u0110\u01b0\u1ee3c ph\u00e1t tri\u1ec3n b\u1edfi John F. Canny v\u00e0o n\u0103m 1986, thu\u1eadt to\u00e1n Canny Edge Detection cho \u0111\u1ebfn nay v\u1eabn l\u00e0 m\u1ed9t trong nh\u1eefng thu\u1eadt to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng ph\u1ed5 bi\u1ebfn nh\u1ea5t v\u00ec t\u00ednh hi\u1ec7u qu\u1ea3 c\u1ee7a n\u00f3.</p> <p>Tr\u01b0\u1edbc khi \u0111i v\u00e0o thu\u1eadt to\u00e1n c\u1ee5 th\u1ec3, ch\u00fang ta c\u00f9ng nhau t\u00ecm hi\u1ec3u m\u1ed9t v\u00e0i kh\u00e1i ni\u1ec7m nh\u00e9!</p>"},{"location":"blog/edge-detection/#61-non-maxima-suppression","title":"6.1. Non-maxima suppression","text":"<p>Nh\u01b0 ch\u00fang ta \u0111\u00e3 bi\u1ebft, vi\u1ec7c s\u1eed d\u1ee5ng \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t cho b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh \u0111\u00f2i h\u1ecfi ta ph\u1ea3i x\u00e1c \u0111\u1ecbnh m\u1ed9t ng\u01b0\u1ee1ng nh\u1ea5t \u0111\u1ecbnh \u0111\u1ec3 nh\u1eadn di\u1ec7n m\u1ed9t pixel v\u1edbi \u0111\u1ed9 l\u1edbn gradient b\u1ea5t k\u00ec c\u00f3 l\u00e0 c\u1ea1nh hay kh\u00f4ng. \u0110\u00e2y l\u00e0 c\u00f4ng vi\u1ec7c kh\u00e1 \u0111au \u0111\u1ea7u b\u1edfi n\u1ebfu ch\u1ecdn m\u1ed9t ng\u01b0\u1ee1ng th\u1ea5p, s\u1ebd c\u00f3 r\u1ea5t nhi\u1ec1u pixel \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh, \u0111\u1eb7c bi\u1ec7t \u1edf nh\u1eefng n\u01a1i m\u00e0 s\u1ef1 thay \u0111\u1ed5i \u0111\u1ea1o h\u00e0m di\u1ec5n ra m\u1ed9t c\u00e1ch t\u1eeb t\u1eeb do c\u1ea1nh \u0111\u00e3 b\u1ecb l\u00e0m m\u1edd khi l\u1ecdc nhi\u1ec5u ho\u1eb7c do h\u1ea1n ch\u1ebf nh\u1ea5t \u0111\u1ecbnh c\u1ee7a c\u00e1c lo\u1ea1i m\u00e1y \u1ea3nh. Nh\u01b0ng n\u1ebfu ch\u1ecdn m\u1ed9t ng\u01b0\u1ee1ng cao, ta s\u1ebd b\u1ecb m\u1ea5t r\u1ea5t nhi\u1ec1u pixel l\u1ebd ra ph\u1ea3i \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n l\u00e0 c\u1ea1nh. B\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 tham kh\u1ea3o b\u1ee9c h\u00ecnh b\u00ean d\u01b0\u1edbi.</p> <p>      H\u00ecnh 15: \u1ea2nh thu \u0111\u01b0\u1ee3c sau khi t\u00ednh to\u00e1n \u0111\u1ed9 l\u1edbn gradient c\u1ee7a t\u1eebng pixel trong \u1ea3nh g\u1ed1c. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>Trong b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh th\u00ec vi\u1ec7c x\u00e1c \u0111\u1ecbnh ch\u00ednh x\u00e1c v\u1ecb tr\u00ed c\u1ee7a c\u1ea1nh l\u00e0 c\u1ea7n thi\u1ebft. Trong b\u1ee9c \u1ea3nh tr\u00ean, ch\u00fang ta kh\u00f4ng bi\u1ebft ph\u1ea3i ch\u1ecdn ng\u01b0\u1ee1ng nh\u01b0 th\u1ebf n\u00e0o \u0111\u1ec3 v\u1eeba c\u00f3 th\u1ec3 thu h\u1eb9p \u0111\u01b0\u1ee3c \u0111\u1ed9 r\u1ed9ng c\u1ee7a c\u1ea1nh, v\u1eeba tr\u00e1nh kh\u00f4ng \u0111\u1ec3 b\u1ecb m\u1ea5t c\u1ea1nh \u1edf nh\u1eefng n\u01a1i kh\u00e1c.</p> <p>\u1ede tr\u00ean ch\u00fang ta \u0111\u00e3 t\u00ecm hi\u1ec3u m\u1ed9t to\u00e1n t\u1eed gi\u00fap nh\u1eadn di\u1ec7n c\u1ea1nh ch\u1ec9 t\u1ea1i nh\u1eefng v\u1ecb tr\u00ed m\u00e0 \u0111\u1ed9 l\u1edbn gradient \u0111\u1ea1t c\u1ef1c \u0111\u1ea1i, \u0111\u00f3 l\u00e0 to\u00e1n t\u1eed Laplacian, tuy nhi\u00ean to\u00e1n t\u1eed n\u00e0y r\u1ea5t d\u1ec5 b\u1ecb \u1ea3nh h\u01b0\u1edfng b\u1edfi nhi\u1ec5u: ch\u1ec9 m\u1ed9t dao \u0111\u1ed9ng nh\u1ecf trong \u0111\u1ea1o h\u00e0m c\u1ea5p m\u1ed9t c\u0169ng t\u1ea1o th\u00e0nh \u0111i\u1ec3m v\u1ec1 kh\u00f4ng trong \u0111\u1ea1o h\u00e0m c\u1ea5p hai. M\u1ed9t k\u0129 thu\u1eadt kh\u00e1c c\u0169ng gi\u00fap ta gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c v\u1ea5n \u0111\u1ec1 n\u00e0y \u0111\u00f3 l\u00e0 Non-maxima suppression. Non-maxima suppression t\u1eadn d\u1ee5ng th\u00eam m\u1ed9t th\u00f4ng tin n\u1eefa: h\u01b0\u1edbng c\u1ee7a gradient (\u1edf ph\u1ea7n tr\u00ean, ta bi\u1ebft r\u1eb1ng t\u1ea1i m\u1ed7i pixel ta \u0111\u1ec1u t\u00ednh \u0111\u01b0\u1ee3c \u0111\u1ed9 l\u1edbn v\u00e0 h\u01b0\u1edbng c\u1ee7a gradient). T\u1eeb \u0111\u00f3 k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c t\u1eeb non-maxima suppression \u00edt nhi\u1ec5u h\u01a1n k\u1ebft qu\u1ea3 t\u1eeb Laplacian b\u1edfi n\u00f3 kh\u00f4ng l\u1ea5y c\u1ef1c \u0111\u1ea1i c\u1ee7a \u0111\u1ed9 l\u1edbn gradient theo m\u1ecdi h\u01b0\u1edbng m\u00e0 ch\u1ec9 theo m\u1ed9t h\u01b0\u1edbng nh\u1ea5t \u0111\u1ecbnh, hay n\u00f3i c\u00e1ch kh\u00e1c, thu\u1eadt to\u00e1n n\u00e0y kh\u00f4ng l\u1ea5y c\u00e1c \u0111i\u1ec3m v\u1ec1 kh\u00f4ng theo m\u1ecdi h\u01b0\u1edbng m\u00e0 ch\u1ec9 theo m\u1ed9t h\u01b0\u1edbng duy nh\u1ea5t \u0111\u1ed1i v\u1edbi m\u1ed7i pixel: h\u01b0\u1edbng c\u1ee7a gradient, hay h\u01b0\u1edbng vu\u00f4ng g\u00f3c v\u1edbi h\u01b0\u1edbng c\u1ee7a \"c\u1ea1nh\" \u0111ang x\u00e9t.</p> <p>H\u00ecnh d\u01b0\u1edbi \u0111\u00e2y gi\u00fap ta h\u00ecnh dung r\u00f5 h\u01a1n v\u1ec1 thu\u1eadt to\u00e1n n\u00e0y:</p> <p>      H\u00ecnh 16: V\u00ed d\u1ee5 v\u1ec1 thu\u1eadt to\u00e1n non-maxima suppression. Pixel \u1edf v\u1ecb tr\u00ed ch\u1ea5m xanh trong h\u00ecnh th\u1ee9 nh\u1ea5t \u0111\u01b0\u1ee3c gi\u1eef l\u1ea1i \u0111\u1ec3 x\u00e9t c\u1ea1nh, pixel t\u01b0\u01a1ng \u1ee9ng trong h\u00ecnh th\u1ee9 hai th\u00ec kh\u00f4ng. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>\u1ede tr\u01b0\u1eddng h\u1ee3p \u0111\u1ea7u ti\u00ean, pixel ta \u0111ang x\u00e9t c\u00f3 gi\u00e1 tr\u1ecb \u0111\u1ed9 l\u1edbn gradient \u0111\u1ea1t c\u1ef1c \u0111\u1ea1i theo h\u01b0\u1edbng gradient c\u1ee7a pixel n\u00e0y (ta c\u00f3 th\u1ec3 bi\u1ebft gi\u00e1 tr\u1ecb n\u00e0y c\u00f3 \u0111\u1ea1t c\u1ef1c \u0111\u1ea1i hay kh\u00f4ng b\u1eb1ng c\u00e1ch so s\u00e1nh pixel n\u00e0y v\u1edbi hai pixel l\u00e2n c\u1eadn theo h\u01b0\u1edbng t\u01b0\u01a1ng \u1ee9ng). V\u00ec th\u1ebf, ta gi\u1eef l\u1ea1i pixel n\u00e0y, \u0111\u1ec3 \u00fd r\u1eb1ng gi\u00e1 tr\u1ecb t\u1ea1i pixel n\u00e0y kh\u00f4ng \u0111\u1ed5i tr\u01b0\u1edbc v\u00e0 sau nonmaxima suppression. \u1ede tr\u01b0\u1eddng h\u1ee3p th\u1ee9 hai, do pixel n\u00e0y c\u00f3 gi\u00e1 tr\u1ecb \u0111\u1ed9 l\u1edbn gradient nh\u1ecf h\u01a1n pixel b\u00ean ph\u1ea3i c\u1ee7a n\u00f3 theo h\u01b0\u1edbng gradient, n\u00ean ta kh\u00f4ng gi\u1eef l\u1ea1i pixel n\u00e0y cho b\u01b0\u1edbc ti\u1ebfp theo. Sau non-maxima suppression, gi\u00e1 tr\u1ecb c\u1ee7a pixel n\u00e0y \u0111\u01b0\u1ee3c \u0111\u1eb7t v\u1ec1 0. L\u01b0u \u00fd r\u1eb1ng thu\u1eadt to\u00e1n non-maxima suppression ch\u1ec9 cho \u0111\u1ea7u ra l\u00e0 li\u1ec7u m\u1ed9t pixel c\u00f3 \u0111\u01b0\u1ee3c ti\u1ebfp t\u1ee5c gi\u1eef l\u1ea1i \u0111\u1ec3 x\u00e9t l\u00e0 c\u1ea1nh hay kh\u00f4ng, n\u00f3 kh\u00f4ng \u0111\u1ea3m b\u1ea3o c\u00e1c pixel \u0111\u01b0\u1ee3c gi\u1eef l\u1ea1i th\u00ec ch\u1eafc ch\u1eafn l\u00e0 c\u1ea1nh.</p> <p>      H\u00ecnh 17: \u1ea2nh thu \u0111\u01b0\u1ee3c tr\u01b0\u1edbc v\u00e0 sau non-maxima suppression. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p>"},{"location":"blog/edge-detection/#62-hysteresis-thresholding","title":"6.2. Hysteresis thresholding","text":"<p>Quay l\u1ea1i v\u1ea5n \u0111\u1ec1 ch\u1ecdn ng\u01b0\u1ee1ng: vi\u1ec7c ch\u1ecdn m\u1ed9t ng\u01b0\u1ee1ng cao hay th\u1ea5p lu\u00f4n l\u00e0 m\u1ed9t s\u1ef1 \u0111\u00e1nh \u0111\u1ed5i gi\u1eefa s\u1ed1 l\u01b0\u1ee3ng c\u1ea1nh l\u00e0 false positives v\u00e0 false negatives m\u00e0 ta s\u1ebd thu \u0111\u01b0\u1ee3c. Thu\u1eadt to\u00e1n hysteresis thresholding \u0111\u01b0\u1ee3c \u0111\u01b0a ra nh\u1eb1m gi\u00fap ta gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 n\u00e0y.</p> <p>\u00dd t\u01b0\u1edfng c\u1ee7a hysteresis thresholding l\u00e0 ta s\u1ebd ch\u1ecdn 2 ng\u01b0\u1ee1ng thay v\u00ec 1 ng\u01b0\u1ee1ng: c\u00e1c pixel c\u00f3 gi\u00e1 tr\u1ecb n\u1eb1m tr\u00ean ng\u01b0\u1ee1ng cao th\u00ec ch\u1eafc ch\u1eafn l\u00e0 c\u1ea1nh (sure-edge pixel), c\u00e1c pixel c\u00f3 gi\u00e1 tr\u1ecb n\u1eb1m d\u01b0\u1edbi ng\u01b0\u1ee1ng th\u1ea5p th\u00ec ch\u1eafc ch\u1eafn kh\u00f4ng l\u00e0 c\u1ea1nh. Sau \u0111\u00f3 ta b\u1eaft \u0111\u1ea7u x\u00e9t c\u00e1c pixel n\u1eb1m gi\u1eefa hai ng\u01b0\u1ee1ng n\u00e0y, nh\u1eefng pixel n\u00e0o l\u00e0 l\u00e2n c\u1eadn c\u1ee7a c\u00e1c \"sure-edge pixel\" s\u1ebd \u0111\u01b0\u1ee3c nh\u1eadn di\u1ec7n l\u00e0 c\u1ea1nh, \u0111i\u1ec1u ng\u01b0\u1ee3c l\u1ea1i \u0111\u1ed1i v\u1edbi nh\u1eefng pixel kh\u00f4ng l\u00e0 l\u00e2n c\u1eadn c\u1ee7a \"sure-edge pixel\" n\u00e0o c\u1ea3.</p> <p>      H\u00ecnh 18: V\u00ed d\u1ee5 v\u1ec1 thu\u1eadt to\u00e1n hysteresis thresholding. \u1ea2nh tham kh\u1ea3o t\u1ea1i \u0111\u00e2y.     </p> <p>Quan s\u00e1t h\u00ecnh tr\u00ean, ta th\u1ea5y r\u1eb1ng A v\u00e0 B ch\u1eafc ch\u1eafn l\u00e0 c\u1ea1nh v\u00ec gi\u00e1 tr\u1ecb c\u1ee7a ch\u00fang cao h\u01a1n ng\u01b0\u1ee1ng tr\u00ean, D ch\u1eafc ch\u1eafn kh\u00f4ng l\u00e0 c\u1ea1nh v\u00ec gi\u00e1 tr\u1ecb c\u1ee7a n\u00f3 th\u1ea5p h\u01a1n ng\u01b0\u1ee1ng d\u01b0\u1edbi, C v\u00e0 E c\u00f3 gi\u00e1 tr\u1ecb n\u1eb1m gi\u1eefa hai ng\u01b0\u1ee1ng, tuy nhi\u00ean C l\u00e0 c\u1ea1nh do n\u00f3 l\u00e0 l\u00e2n c\u1eadn c\u1ee7a B, c\u00f2n E th\u00ec kh\u00f4ng.</p> <p>Ch\u00fang ta c\u00f9ng so s\u00e1nh thu\u1eadt to\u00e1n hysteresis thresholding v\u1edbi c\u00e1ch ch\u1ecdn ng\u01b0\u1ee1ng th\u00f4ng th\u01b0\u1eddng:</p> <p>      H\u00ecnh 19: H\u00ecnh 1: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c khi \u00e1p d\u1ee5ng hysteresis thresholding. H\u00ecnh 2: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c khi ch\u1ec9 d\u00f9ng m\u1ed9t ng\u01b0\u1ee1ng cao. H\u00ecnh 3: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c khi ch\u1ec9 d\u00f9ng m\u1ed9t ng\u01b0\u1ee1ng th\u1ea5p.     </p> <p>Ta th\u1ea5y r\u1eb1ng thu\u1eadt to\u00e1n hysteresis thresholding v\u1eeba \u0111\u1ea3m b\u1ea3o kh\u00f4ng b\u1ecb nh\u1eadn di\u1ec7n thi\u1ebfu qu\u00e1 nhi\u1ec1u c\u1ea1nh nh\u01b0 trong h\u00ecnh th\u1ee9 hai, v\u1eeba gi\u1ea3m thi\u1ec3u nhi\u1ec5u nh\u01b0 trong h\u00ecnh th\u1ee9 ba.</p>"},{"location":"blog/edge-detection/#63-thuat-toan-canny-edge-detection","title":"6.3. Thu\u1eadt to\u00e1n Canny Edge Detection","text":"\ud83d\udcd0 Thu\u1eadt to\u00e1n: <ol> <li>D\u00f9ng b\u1ed9 l\u1ecdc Gaussian \u0111\u1ec3 l\u1ecdc nhi\u1ec5u</li> <li>X\u1ea5p x\u1ec9 \u0111\u1ed9 l\u1edbn gradient v\u00e0 h\u01b0\u1edbng c\u1ee7a gradient cho t\u1eebng pixel trong \u1ea3nh</li> <li>D\u00f9ng non-maxima suppresion \u0111\u1ec3 \"l\u00e0m m\u1ecfng\" c\u1ea1nh, t\u1ee9c lo\u1ea1i b\u1ecf c\u00e1c pixel kh\u00f4ng c\u1ea7n thi\u1ebft</li> <li>D\u00f9ng hysteresis thresholding \u0111\u1ec3 nh\u1eadn di\u1ec7n c\u1ea1nh</li> </ol> <p>      H\u00ecnh 20: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c c\u1ee7a thu\u1eadt to\u00e1n Canny Edge Detection \u0111\u1ed1i v\u1edbi c\u00e1c c\u00e1ch ch\u1ecdn ng\u01b0\u1ee1ng kh\u00e1c nhau. H\u00ecnh 1: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c sau khi t\u00ednh to\u00e1n \u0111\u1ed9 l\u1edbn gradient (\u1ea3nh \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 ph\u00e2n ng\u01b0\u1ee1ng). H\u00ecnh 2, 3, 4: K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c sau khi \u00e1p d\u1ee5ng hysteresis thresholding v\u1edbi c\u00e1c ng\u01b0\u1ee1ng \\(T_1\\), \\(T_2\\) kh\u00e1c nhau.     </p> <p>M\u1ed9t s\u1ed1 l\u01b0u \u00fd:</p> <ul> <li>\u0110\u1ec3 c\u00f3 th\u1ec3 ti\u1ebfn h\u00e0nh b\u01b0\u1edbc 4: d\u00f9ng m\u1ed9t ng\u01b0\u1ee1ng \u0111\u1ec3 quy\u1ebft \u0111\u1ecbnh xem m\u1ed9t pixel b\u1ea5t k\u00ec c\u00f3 l\u00e0 c\u1ea1nh hay kh\u00f4ng, ta c\u1ea7n gi\u1eef l\u1ea1i gi\u00e1 tr\u1ecb \u0111\u1ed9 l\u1edbn gradient c\u1ee7a m\u1ed7i pixel \u1edf b\u01b0\u1edbc 3.</li> <li>\u0110\u1ebfn \u0111\u00e2y n\u1ebfu b\u1ea1n \u0111\u1ecdc th\u1eafc m\u1eafc r\u1eb1ng v\u00ec sao l\u1ea1i c\u1ea7n b\u01b0\u1edbc thresholding \u1edf cu\u1ed1i khi b\u1ee9c \u1ea3nh thu \u0111\u01b0\u1ee3c sau khi d\u00f9ng non-maxima suppression \u0111\u00e3 cho ra h\u00ecnh d\u00e1ng c\u1ea1nh nh\u01b0 ta mong mu\u1ed1n, b\u1ea1n c\u00f3 th\u1ec3 xem l\u1ea1i ph\u1ea7n 2 m\u1ee5c 3, kh\u00f4ng nh\u1eefng th\u1ebf, k\u0129 thu\u1eadt \u0111\u1eb7c bi\u1ec7t c\u1ee7a hysteresis thresholding c\u00f2n g\u00f3p ph\u1ea7n gi\u1ea3m \u0111i m\u1ed9t l\u01b0\u1ee3ng nhi\u1ec5u \u0111\u00e1ng k\u1ec3 (do nhi\u1ec5u th\u01b0\u1eddng r\u1eddi r\u1ea1c, kh\u00f4ng \"k\u1ebft n\u1ed1i\" v\u1edbi nhau).</li> <li>\u1ede thu\u1eadt to\u00e1n non-maxima suppression, thay v\u00ec so s\u00e1nh gi\u00e1 tr\u1ecb \u0111\u1ed9 l\u1edbn gradient m\u1ed9t pixel v\u1edbi hai pixel l\u00e2n c\u1eadn \u0111\u1ec3 gi\u1eef l\u1ea1i pixel c\u00f3 gi\u00e1 tr\u1ecb c\u1ef1c \u0111\u1ea1i, ta c\u0169ng c\u00f3 th\u1ec3 d\u00f9ng to\u00e1n t\u1eed Laplacian nh\u01b0ng theo m\u1ed9t chi\u1ec1u (chi\u1ec1u c\u1ee7a gradient) ho\u1eb7c \u0111\u1ea1o h\u00e0m c\u1ea5p hai c\u00f3 h\u01b0\u1edbng v\u00e0 gi\u1eef l\u1ea1i pixel \u1edf v\u1ecb tr\u00ed v\u1ec1 kh\u00f4ng.</li> </ul>"},{"location":"blog/edge-detection/#7-ket-luan","title":"7. K\u1ebft lu\u1eadn","text":"<p>\u1ede ph\u1ea7n tr\u00ean, ch\u00fang ta c\u00f3 \u0111\u1ec1 c\u1eadp \u0111\u1ebfn m\u1ed9t kh\u00e1i ni\u1ec7m kh\u00f4ng d\u00f9ng nhi\u1ec1u trong b\u00e0i vi\u1ebft n\u00e0y, \u0111\u00f3 l\u00e0 contour. \u0110\u1ea7u ra c\u1ee7a b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh ch\u1ec9 l\u00e0 c\u00e1c \u0111i\u1ec3m c\u1ea1nh r\u1eddi r\u1ea1c, m\u1eb7c d\u00f9 nh\u00ecn b\u1eb1ng m\u1eaft th\u01b0\u1eddng, ta c\u00f3 c\u1ea3m gi\u00e1c nh\u01b0 \u0111\u00f3 \u0111\u00e3 l\u00e0 c\u00e1c \u0111\u01b0\u1eddng bi\u00ean, v\u00ec m\u1eaft ta c\u00f3 kh\u1ea3 n\u0103ng t\u1ef1 \"k\u1ebft n\u1ed1i\" c\u00e1c \u0111i\u1ec3m r\u1eddi r\u1ea1c n\u00e0y v\u1edbi nhau. N\u1ebfu mu\u1ed1n thu \u0111\u01b0\u1ee3c \u0111\u01b0\u1eddng bi\u00ean li\u00ean t\u1ee5c c\u1ee7a v\u1eadt th\u1ec3, ho\u1eb7c \u0111\u00e1nh th\u1ee9 t\u1ef1 c\u00e1c \u0111i\u1ec3m c\u1ea1nh n\u00e0y \u0111\u1ec3 c\u00f3 th\u1ec3 n\u1ed1i ch\u00fang l\u1ea1i v\u1edbi nhau t\u1ea1o th\u00e0nh m\u1ed9t \u0111\u01b0\u1eddng li\u1ec1n m\u1ea1ch, ta s\u1ebd \u0111\u1ebfn v\u1edbi m\u1ed9t b\u00e0i to\u00e1n m\u1edbi, \u0111\u00f3 l\u00e0 b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n \u0111\u01b0\u1eddng bi\u00ean (Boundary detection).</p> <p>Qua b\u00e0i vi\u1ebft n\u00e0y, b\u1ea1n \u0111\u1ecdc \u0111\u00e3 c\u00f3 m\u1ed9t c\u00e1i nh\u00ecn to\u00e0n c\u1ea3nh v\u1ec1 b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh, c\u00e1c kh\u00e1i ni\u1ec7m li\u00ean quan \u0111\u1ebfn b\u00e0i to\u00e1n, c\u00e1c v\u1ea5n \u0111\u1ec1 th\u01b0\u1eddng g\u1eb7p \u0111\u1ed1i v\u1edbi b\u00e0i to\u00e1n n\u00e0y c\u0169ng nh\u01b0 c\u00e1c thu\u1eadt to\u00e1n th\u00f4ng d\u1ee5ng \u0111\u01b0\u1ee3c \u0111\u1ec1 xu\u1ea5t. Vi\u1ec7c c\u1ed1 g\u1eafng bao tr\u00f9m v\u1ec1 b\u1ec1 r\u1ed9ng ch\u1eafc ch\u1eafn s\u1ebd khi\u1ebfn b\u00e0i vi\u1ebft kh\u00f4ng \u0111\u1ea1t \u0111\u01b0\u1ee3c ti\u00eau ch\u00ed v\u1ec1 b\u1ec1 s\u00e2u, \u0111\u1ed3ng th\u1eddi m\u1ed9t v\u00e0i kh\u00e1i ni\u1ec7m c\u0169ng \u0111\u01b0\u1ee3c \u0111\u01a1n gi\u1ea3n h\u00f3a, nh\u01b0ng qua \u0111\u00f3 m\u00ecnh mong r\u1eb1ng b\u00e0i vi\u1ebft s\u1ebd cung c\u1ea5p cho b\u1ea1n \u0111\u1ecdc m\u1ed9t h\u00ecnh dung nh\u1ea5t \u0111\u1ecbnh v\u1ec1 b\u00e0i to\u00e1n n\u00e0y, \u0111\u1ed3ng th\u1eddi kh\u01a1i g\u1ee3i s\u1ef1 t\u00f2 m\u00f2, h\u1ee9ng th\u00fa c\u1ee7a b\u1ea1n \u0111\u1ecdc, \u0111\u1ec3 t\u1eeb \u0111\u00f3 vi\u1ec7c \u0111\u00e0o s\u00e2u v\u00e0o c\u00e1c ch\u1ee7 \u0111\u1ec1 n\u00e2ng cao v\u00e0 th\u00fa v\u1ecb kh\u00e1c c\u0169ng tr\u1edf n\u00ean d\u1ec5 d\u00e0ng h\u01a1n (m\u1ed9t s\u1ed1 ch\u1ee7 \u0111\u1ec1 c\u0169ng th\u00fa v\u1ecb kh\u00f4ng k\u00e9m trong b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh m\u00e0 b\u1ea1n \u0111\u1ecdc c\u00f3 th\u1ec3 ti\u1ebfp t\u1ee5c t\u00ecm hi\u1ec3u nh\u01b0: s\u1ef1 k\u1ebft h\u1ee3p gi\u1eefa Fourier Transform v\u00e0 High-pass filter trong b\u00e0i to\u00e1n nh\u1eadn di\u1ec7n c\u1ea1nh, hay k\u0129 thu\u1eadt Image approximation \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh v\u1ecb tr\u00ed c\u1ea1nh \u1edf m\u1ee9c \u0111\u1ed9 subpixel,...)</p>"},{"location":"blog/edge-detection/#8-tham-khao","title":"8. Tham kh\u1ea3o","text":"<ol> <li>Jain, R., Kasturi, R., &amp; Schunck, B. G. (1995). Machine vision (Vol. 5, pp. 309-364). New York: McGraw-hill.</li> <li> <p>First Principles of Computer Vision. (n.d.). https://fpcv.cs.columbia.edu/ </p> </li> <li> <p>6.2. Ph\u00e9p T\u00edch ch\u1eadp cho \u1ea2nh \u2014 \u0110\u1eafm m\u00ecnh v\u00e0o H\u1ecdc S\u00e2u 0.14.4 documentation. (n.d.). https://d2l.aivivn.com/chapter_convolutional-neural-networks/conv-layer_vn.html </p> </li> <li> <p>Nalwa, V. S. (1994). A guided tour of computer vision. Addison-Wesley Longman Publishing Co., Inc.</p> </li> <li>OpenCV: Canny Edge Detection. (n.d.). https://docs.opencv.org/3.4/da/d22/tutorial_py_canny.html</li> </ol>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/","title":"[RL series - Part 1] Reinforcement Learning - or The Art of Decision Making in an Uncertain World","text":"<p>This article serves as a guide to help you enter the realm of Reinforcement Learning. The concepts in the \"Theory\" section are presented in an intuitive way, so you can build a solid understanding without unnecessary struggle. If you're more of a hands-on person, you can check out the notebook in the \"Case Study\" section, which experiments on the algorithms that we'll mention.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#section-1-theory","title":"Section 1: Theory","text":""},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#1-introduction","title":"1. Introduction","text":""},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#11-inspiration","title":"1.1 Inspiration","text":"<p>Let\u2019s start with a fascinating question: How do we learn? From the first breadth you tooth until this very moment when you\u2019re sitting here reading this article is a never-ending process of learning. So how exactly do you do this? How do you know that in front of you is a \u201cdog\u201d and not a \u201ccat\u201d or a \u201cdinosaur\u201d? Or how do you know that in order to walk properly, you have to first shift your weight onto one foot, lift the other foot, move it forward, place it onto the ground, and then repeat with the others? The answer is through multiple trial-and-errors and feedbacks of others and of the environment. Remember the moment you mistakenly called a dog a cat? If you were with your parents, they would kindly correct you, pointing out that it's actually a dog. However, if it happened in front of your friends, they might laugh hard at you, leaving you puzzled. These reactions serve as valuable feedback, helping you learn the difference between a dog and a cat for next time (unless you want to risk another round of laughter!). The same goes with learning how to walk, you may have experimented with tilting your body in the wrong direction or not alternating your feet properly to end up finding out that these approaches were not really successful, so you stopped doing that and tried some other techniques to the point when you fell comfortable. Reinforcement Learning algorithms try to do just that: an agent explores different actions, evaluates their outcomes, and adjusts its strategies to maximize rewards and minimize penalties in future interactions. Besides Deep Learning, which seeks to mimic the intricacies of human brain functions such as neural transmission and pattern recognition, Reinforcement Learning is a stepping stone in the effort of imitating human in terms of learning and making decisions.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#12-what-is-reinforcement-learning","title":"1.2 What is Reinforcement Learning?","text":"<p>It\u2019s a type of Machine Learning, where an agent learn to make decision so as to maximize the accumulative reward. To do this, the agent has to interact with the environment, and adjust its actions base on feedbacks in terms of rewards and penalties.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#13-why-do-we-need-reinforcement-learning","title":"1.3 Why do we need Reinforcement Learning?","text":"<p>There appears to be a category of problems where the labels for our data is not available. Although most of the time we can do the labeling on our own, or with the help of an expert (e.g: segmenting the \u201ctumor\u201d region and \u201cnon-tumor\u201d region in a medical image, or classifying images with respect to different people in a face-recognition task,\u2026), but sometimes there are problems where the correct labeling is simply unattainable. Haven\u2019t heard of those kinds of problem before?</p> <ol> <li>Imagine your task is to find the fastest way out of a maze, from the starting state, should you take the left move, which will take you closer to the exit, or the right move, which will take you further away, in either move, you\u2019ll face a possibility of being stuck in a death end.</li> <li>Imagine that your task is to maximize your happiness. You have an exam tomorrow. Should you start studying now or watch another episode of \u201cThe Boys\u201d?</li> </ol> <p>In either circumstances, the goal you\u2019re trying to reach is not the short-term reward (moving closer to the exit, or a little extra satisfaction from watching another episode of your favorite series), but rather a long-term reward (actually find an exit, or maximize your happiness in the long run). Remember that these 2 are not the same, and sometimes, if not most of the time, you have to sacrifice one to achieve another (choosing a longer path with the goal of actually reaching the exit, not just moving closer to it, or sacrifice your instant gratification for a more sustainable fulfillment of academic success). This highlights the first property of Reinforcement Learning, which is delayed reward (the \u201cbest\u201d action is defined to be the action that produce the biggest long-term reward, meaning the reward accumulated through time, not the reward received immediately). But the story doesn\u2019t end here, how do we know, from our first examples, that the longer but not shorter path with eventually leads you to the exit, or you\u2019ll actually earn a good grade after a whole night studying for the exam. The answer is, you have to discover it yourself! Maybe you\u2019ll find out that going right is, indeed, the action that take you to the death end, or maybe you\u2019ll end up flunking the exam even though you\u2019ve been cramming for it the whole night before, and realize that binge watching your favorite TV series would be, sadly, be a better choice. Interesting? We face those decisions our whole life, the only thing we could do is try one, observe the result, and change our worldview and action according to that experience in case we\u2019ll be in that same circumstance in the future. This pinpoint the second property of Reinforcement Learning, which is trial and error (discover the \u201clabel\u201d on our own to maximize the objective function, which is the accumulative reward).</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#14-and-i-i-took-the-road-less-taken-by","title":"1.4 \u201c\u2026 And I \u2014 I took the road less taken by\u2026\u201d","text":"<p>Are those examples enough whet your appetite? This article includes all the key points we\u2019ve gathered after reading Part I of this book, plus our own intuition and understanding about different concepts presented. We hope that this will pave a smoother path for you on your journey into the realm of Reinforcement Learning.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#2-key-elements-of-reinforcement-learning","title":"2. Key Elements of Reinforcement Learning","text":"<p>We\u2019ll start off learning some fundamental concepts that\u2019ll be crucial for you to take a grasp of what the people in this area is talking about.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#1-episodic-task-and-continuing-task","title":"1. Episodic task and Continuing task","text":"<p>Episodic task is a task where the agent-environment interaction breaks naturally into subsequences. In episodic tasks, there are starting states (\\(S_0\\)), and terminal states (\\(S_T\\)- where \\(T\\) is the terminal timestep), between which is a sequence of state-action transitions. Each sequence of timesteps from a particular starting state to a particular terminal state is called an episode. It\u2019s pretty much like video games, where the starting state is the initial position of your character, with zero point, and the terminal state is when you lose, which also denote the end of an episode, in which case you have to start the game again with the same position and zero point. In an episodic task, the succeeding states must not, by any mean, be affected by the preceding states.</p> <p>Continuing task is a task where \\(T \\rightarrow \\infty\\). It\u2019s a task that lasts for the agent\u2019s lifetime (you can see the definition of \u201cagent\u201d below).</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#2-policy-pi","title":"2. Policy (\\(\\pi\\))","text":"<p>A policy is a mapping from situations (or states, which is more widely use in Reinforcement Learning literatures) to actions. A policy can either be deterministic, meaning that it only return one action for each state, or stochastic, meaning that it will return an action sampled from a probability distribution for each state.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#3-reward-r","title":"3. Reward (\\(R\\))","text":"<p>The reward denotes whether a state, or an action in a particular state, is good or bad.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#4-return-g","title":"4. Return (\\(G\\))","text":"<p>It\u2019s the accumulated reward that you\u2019ve collected along the way from the starting state to the terminal state (in episodic tasks) or for the agent\u2019s lifetime (in continuing tasks).</p> <p>So how do we formulate this \\(G_t\\) for a specific timestep? Our first idea would be to sum all the rewards:</p> \\[ G_t = R_{t+1} + R_{t+2} + R_{t+3} + \u00b7\u00b7\u00b7 + R_T \\] <p>This works just fine in simple cases where \\(T\\) is small, but if \\(T\\) is large or approach \\(\\infty\\), this definition of return would be problematic, since \\(G_t\\) will either be really big or approaches infinity. To deal with this, we use a discounting factor (\\(0 \\leq \\lambda \\leq 1\\)) and reformulate \\(G_t\\) as:</p> \\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\] <p>Phewww! Now \\(G_t\\) can converged no matter how large \\(T\\) is. Having reach to this point, I think we should have a deeper discussion about the discount factor \\(\\lambda\\). This variable denotes how much you want to emphasize the importance of future reward. The implementation of \\(\\lambda\\) into this formula gives a beautiful effect in which rewards from states encountered are gradually fading away, and this fading manifest more clearly in states that are more further away in the future. In extreme case where \\(\\lambda = 0\\), we only care about immediate reward (\\(G_t = R_t\\)); and where \\(\\lambda = 1\\), we consider rewards received in every state the same degree of importance.</p> <p>Formulate \\(G_t\\) this way also allows us to rewrite the formula in a recursive way:</p> \\[ G_t = R_{t+1} + \\gamma G_{t+1} \\]"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#5-value-function","title":"5. Value function","text":"<p>Remember when we say that the goal of Reinforcement Learning is to maximize the long-term reward? But what is the long-term reward anyway? We use the term \u201cvalue function\u201d to denote this. \u201cWait a minute\u201d, you may say, \u201cHow about the Return? Isn\u2019t that definition of Return perfectly fit our definition of long-term reward?\u201d Yeah you\u2019re right, in a sense. Let\u2019s make this clear.</p> <p>The \u201cvalue function\u201d is the mapping from a state to its value. The \u201cvalue\u201d is defined as the expected return, which has something to do with the stochastic of the environment dynamics and the actions being taken. Continue with our previous example, what do you think the long-term reward of studying for the exam be? The answer is, it depends. Suppose that you decide to study for the exam, this can cause different scenarios to happen, you could finish studying in a short amount of time and go to bed early, or you could end up being distracted and have to stay up late. Suppose you end up in the second scenario, the next morning, you have to choose either to wake up for exam, or sleep in for an extra 10 minutes, which in turn, can lead you to a situation of you being late for the exam\u2026 You see, the future is full of uncertainties, so the long-term reward should be computed carefully, after considering all possibilities of what can happen and what action you can take, not just one sample of those. Hence for the evaluation of long-term reward, we should aim at the expected return instead of the return itself.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#6-agent","title":"6. Agent","text":"<p>The agent is our decision maker, it interacts with the environment to receive reward, calculate the value of that state, change the value function, and change the policy if necessary and make decision according to that policy.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#7-environment-and-model","title":"7. Environment and Model","text":"<p>The environment gives the agent the reward signals. A model is the simulation of the environment, meaning that it\u2019s built based on our data distribution collected from the environment, and is used to inferred about how the environment may behave. A model allows the agent to learn without interacting with the environment, which might be costly. Much like training to fly a helicopter in MSFS without breaking the bank!</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#8-markov-decision-process","title":"8. Markov Decision Process","text":"<p>Figure 1: The interaction between agent and environment in a MDP   </p> <p>A Markov Decision Process (MDP) is a model that describe the interaction between the agent and the environment, throughout the sequence of decision-makings. In a MDP, at timestep \\(t\\), the agent is in state \\(S_t\\) and receive a reward \\(R_t\\), the agent takes an action \\(A_t\\), to which the environment returns with the next state \\(S_{t+1}\\) and the reward from that next state \\(R_{t+1}\\).</p> <p>The most important property of a MDP is that the next state and reward only depend on the current state and action, not the history of states and actions. Given the previous state and action, the probability distribution of two random variable \\(R_t\\) and \\(S_t\\) is denoted by \\(p\\):</p> \\[ p(s', r|s,a) \\doteq Pr\\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\\} \\] <p>\\(p\\) is often called the dynamic of the environment. Notice that the sum of probabilities of a particular pair of next state - reward over all next states and rewards possible from a particular action in a given current state, is \\(1\\):</p> \\[ \\sum_{s' \\in S}\\sum_{r \\in R} p(s',r|s,a) = 1, \\forall s \\in S, a \\in A(s) \\] <p>At this point you may ask, why we have to take the sum over all \\(r\\)? Doesn\u2019t it mean that the reward can varies even in the same state? The answer is yes, every state may has its own probability distribution of reward, although this isn\u2019t the case most of the time.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#9-the-bellman-equation","title":"9. The Bellman equation","text":"<p>First of all we rewrite our previous definition of the state value function in form of a mathematical formula:</p> \\[ v_\\pi(s) = E[G_t | S_t = s], \\forall s \\in S \\qquad\\qquad (1) \\] <p>I\u2019m supposed to give you the Bellman equation for the state value now. But in an attempt of a clear explanation, I found it easier to introduce you to the Bellman equation for the action value function first. Similar to the definition of the value of a state, the value of a state-action pair \\((s,a)\\) is the expected return the agent will get when it\u2019s in state \\(s\\) and take action \\(a\\):</p> \\[ q_\\pi(s,a) = E[G_t|S_t = s, A_t = a] \\] <p>With some substitution and transformation, we can rewrite the equation as:</p> \\[ \\begin{align*} q_\\pi(s,a) &amp; = E [\\textcolor{red} {R_{t+1} + \\gamma G_{t+1}} | S_t = s, A_t = a] \\\\ &amp; = \\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)\\left(r + \\gamma \\mathbb E[G_{t+1}|S_{t+1}=s']\\right) \\\\ &amp; =\\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)\\left(r + \\gamma \\textcolor{orange}{v_\\pi(s')}\\right)\\qquad\\qquad(2) \\\\ \\end{align*} \\] <p>The first line is obtained by substituting \\(G_t\\) with its recursive form - \\(R_{t+1} + \\gamma G_{t+1}\\). Now to find the expectation of the red term, you have to consider all possibilities of the next state and next reward, given the current state and action. To do this, we take the probability of reaching the next state \\(s\u2019\\) and receive the reward \\(r\\), multiply with its corresponding value of the red term, then sum over all possibilities of next states and rewards from our current state-action pair. You should notice that even after the next state is given, the return of the next state \\(G_{t+1}\\) isn\u2019t deterministic yet, because it still depends on the next action taken the the environment dynamic, hence we still have to take the expectation of that random variable. Now according to \\((1)\\), we substitute the expectation in the second line with the value of the next state to reach the third line.</p> <p>Now that we know that the formula for \\(q_\\pi(s,a)\\) can be derived as \\((2)\\). How about rewriting the value function to obtain the same structure. The value of a state is simply the same as the value of a state-action pair, but the action here is not determined yet. Hence the value of a state equals to the value of a state-action pair, weighted by the probability of taking that action according to the policy, and then sum over all possible actions:</p> \\[ \\begin{align*} v_\\pi(s) &amp; = \\sum_{a \\in A}\\pi(a|s) q_\\pi(s,a) \\qquad\\qquad (3)\\\\ &amp; = \\sum_{a \\in A}\\pi(a|s)\\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)[r + \\gamma v_\\pi(s')], \\quad \\forall s \\in S \\end{align*} \\] <p>Wow! We obtain the formula for \\(v_\\pi(s)\\) in such a beautiful recursive form. This is the Bellman Equation of the state value function.</p> <p>So what about the Bellman equation of the action value function? Let\u2019s return back to our equation \\((2)\\), this isn\u2019t in recursive form yet. But we just have to do one more step: plug the formula at \\((3)\\) into the orange term in \\((2)\\), and\u2026voila!</p> \\[ \\begin{align*} q_{\\pi}(s,a) &amp; =\\sum_{s' \\in S}\\sum_{r \\in R}p(s',r|s,a)\\left(r + \\gamma \\textcolor{orange}{\\sum_{a \\in A}\\pi(a'|s') q_\\pi(s',a')}\\right)\\\\ \\end{align*} \\]"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#10-optimal-policy-and-optimal-value-function","title":"10. Optimal policy and optimal value function","text":"<p>The optimal policy is the policy that for every state, the value of that state is higher than those yielded by other policy:</p> \\[ v_*(s) = \\max_\\pi v_\\pi(s) ,\\quad \\forall s \\in S \\] <p>The optimal value function is the value function under the optimal policy, which is \\(v_*(s)\\). Intuitively, we know the value of a state under the optimal policy must equal the expected return for the best action taken from that state, hence we can write the Bellman Equation for the optimal state value function as following:</p> \\[ \\begin{align*}  v_*(s) &amp; = \\max_{a \\in A} q_{\\pi_*}(s,a) \\\\ &amp; = \\max_a \\mathbb E_{\\pi_*}[R_{t+1} + \\gamma G_{t+1}|S_t = s, A_t = a] \\\\  &amp; = \\max_a\\sum_{s',r}p(s',r|s,a)(r + \\gamma E_{\\pi_*}[G_{t+1}| S_{t+1} = s']) \\\\    &amp; = \\max_a\\sum_{s',r}p(s',r|s,a)(r + \\gamma v_*(s')) \\\\ \\end{align*} \\] <p>The deriving steps are just the same as those from the formulation of the Bellman Equation for state value function above.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#3-dynamic-programming","title":"3. Dynamic Programming","text":"<p>Dynamic Programming (DP) is not a particular algorithm, but a collection of algorithms that computes the optimal policy after being given a perfect model of the environment as a Markov Decision Process (meaning that \\(p(s\u2019,r | s,a)\\), or the environment dynamic, is known for all tuples \\((s,a,s\u2019,r)\\)). One example of such environment is the simulation environment where the agent is a robot navigating in a grid world. In such environment, the dynamic is completely known in the sense that if you control the robot to go to the left, it will not slip to the opposite direction. It\u2019s useful to understand that such environment can still be a stochastic environment, such as a similar environment to the above but this time, when you control the robot to go to the left, there\u2019s 70% of its going to the left and the remaining 30% to the other direction. The universal idea of DP algorithms is that they are obtained by turning Bellman equation into assignments, that is into update rules to improve the approximation of the value functions.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#31-policy-evaluation","title":"3.1 Policy Evaluation","text":"<p>Policy Evaluation is the process of computing the value function \\(v_\\pi(s),s \\in \\mathcal S\\) for an arbitrary policy \\(\\pi\\).</p> <p>To achieve the value function, our first approach is to write the Bellman equation for every state \\(s \\in \\mathcal S\\) (\\(\\mathcal S\\) is the state space, by the way). By doing this obtain a system of \\(|\\mathcal S|\\) linear equations in \\(|\\mathcal S|\\) unknowns, each unknown is the value of a state - \\(v_\\pi(s), s \\in \\mathcal S\\). However, solving for \\(v_\\pi(s)\\) for each \\(s \\in \\mathcal S\\) is a difficult task (even impossible if \\(\\mathcal S\\) is large).</p> <p>The second approach is not to obtain the true value function right away, but to update the value function iteratively:</p> \\[ v_{k+1}(s) = \\sum_{a \\in A}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r + \\gamma v_k(s')\\right] \\] <p>The value of state \\(s\\) at \\((k+1)^{th}\\) iteration is estimated by the values of its succeeding states at \\(k^{th}\\) iteration. Each iteration includes an update of every state in the state space. And when \\(k \\rightarrow \\infty\\), the approximated value of each state approaches that state\u2019s true value. Here is an algorithm for this approach:</p> <p></p> <p>The assignment made in the red box is exactly the same as our above update formula. The inequality in the orange box is the stopping condition, which is when the absolute error of the old and new value of a state, maximum over all states, is below a specific threshold. Notice that the value of each state in this algorithm is updated \u201cinline\u201d, which means for each sweep through the state space, we use the immediately new value of preceding states as the update value of the succeeding states. This method brings about faster learning, and less memory requirement to store the old state values.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#32-policy-improvement","title":"3.2 Policy Improvement","text":"<p>We are able to approach the true value function for an arbitrary policy, but our goal doesn\u2019t just end there. We need to find a better policy based on the values computed. The way we\u2019re going to achieve this is pretty straightforward - for each state, we change the current action to the action that produces the best action value:</p> \\[ \\begin{align*} \\pi'(s) &amp; = \\arg \\max_a q_\\pi(s,a) \\\\ &amp; = \\arg \\max_a \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_\\pi(s')] \\qquad (4) \\end{align*} \\] <p>According to \\((4)\\), we have the value function of the new policy \\(\\pi\u2019\\) as follow:</p> \\[ v_{\\pi'}(s) = \\max_a \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_\\pi(s')] \\qquad (5) \\] <p>If the new policy happens to be as good as, but no better than the old policy (\\(v_{\\pi'}(s) = v_\\pi(s)\\)), then \\((5)\\) will turn into the Bellman optimality equation:</p> \\[ v_{\\pi'}(s) = \\max_a \\sum_{s',r}p(s',r|s,a)[r + \\gamma v_{\\pi'}(s')] \\] <p>The fact that this is the Bellman optimality equation means that our strategy for improving the policy actually works. Policy improvement always gives us a strictly better policy than the one before, except when our original policy is already optimal. In this case \\(\\pi\u2019(s)\\) and \\(\\pi(s)\\) are both optimal policies.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#33-policy-iteration","title":"3.3 Policy Iteration","text":"<p>Policy Iteration is the combination of Policy Evaluation and Policy Improvement, in which we calculate the value function given the current policy, and then change our policy greedily with respect to the calculated value function, repeatedly until our policy and value function is (approximately) stable.</p> <p>One can also imagine the Policy Iteration process as an effort to simultaneously satisfy two goals: obtaining a value function consistent with a given policy and obtaining a better (more greedy) policy. To try to reach one goal means to diverge from the other (by approaching the true value function of a policy, we find that our policy is not as greedy as we think, and by changing our policy to a better one, the value function calculated doesn\u2019t remain true anymore). But by continuously repeating these steps, we will eventually converge to the point of the optimal policy and optimal value function.</p> <p></p> <p>Figure 2: Diagram of Policy Iteration   </p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#34-value-iteration","title":"3.4 Value Iteration","text":"<p>One question arises: If Policy Evaluation is done iteratively, then convergence exactly to \\(v_\\pi\\) occurs only in the limit (we can never find the true value function by iterative method). Must we wait for the exact convergence, or can we stop short of that?</p> <p>The Value Iteration algorithm stems from the idea that we should truncate the Policy Evaluation process, which will speed up the learning process, but still guarantees the convergence to the optimal value function:</p> <p></p> <p>Notice that the formula for updating \\(v_\\pi(s)\\) is now a combination of Policy Improvement (it is now computed as a maximized value, not an expected value) and truncated Policy Evaluation (\\(v_\\pi(s)\\) for every state is now calculated once for every \u201cupdated\u201d policy, not repeatedly). One can also think of this updating formula as an application of the Bellman optimality equation. By updating \\(v_\\pi(s)\\) this way, we expect that \\(v_\\pi(s)\\) will eventually approach \\(v_*(s)\\), and the optimal policy is obtained simply by choosing the best action in each state according to the optimal value function.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#4-monte-carlo-methods","title":"4. Monte Carlo Methods","text":"<p>\u201cif you\u2019re going to try, go all the way. otherwise, don\u2019t even start\u2026\u201d</p> <p>\u2014 \u201cRoll the dice\u201d - Charles Bukowski</p> <p>Dynamic Programming algorithms requires the environment dynamic is completely no, which obviously not be the case for a lot of problems. Imagine the game of Tic-tac-toe, you may know all possible next state given the current state causes by all possible moves that your opponent can make. But the probabilities of reaching those next states is unknown, because you don\u2019t know which strategy your opponent is using, it\u2019s even more difficult to predict if your opponent is an irrational one. This example can be extended to almost all multi-player games, such as poker, blackjack, chess, go,\u2026 In these cases the expected return (true value) of a state can only be calculated by reaching the end of the game multiple times to calculate actual returns and averaging those returns. This is the idea of Monte Carlo methods.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#41-monte-carlo-prediction-or-monte-carlo-estimation-of-state-value","title":"4.1 Monte Carlo Prediction (or Monte Carlo Estimation of State Value)","text":"<p>The algorithm for Monte Carlo (MC) Prediction is presented below:</p> <p></p> <p>This is the \u201cFirst-visit\u201d MC prediction, meaning that we estimate \\(v_\\pi(s)\\) by averaging out returns followed by first visit of \\(s\\) in each episode. As opposed to \u201cEvery visit\u201d MC prediction, where we estimate \\(v_\\pi(s)\\) by averaging out returns followed by every visit of \\(s\\) in every episode. In MC prediction, we first initialize a return array for each state, append the return of that state every time we first encounter it in an episode, and update the value of a state by averaging out all the returns of that state sampled so far.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#42-monte-carlo-with-exploring-start-or-monte-carlo-estimation-of-action-value","title":"4.2 Monte Carlo with Exploring Start (or Monte Carlo Estimation of Action Value)","text":"<p>We\u2019ve obtain the state value function of an arbitrary policy, the next step is to find a better policy according to that. But difficulty arise since we know we know which state is better, but don\u2019t know the environment dynamics, thus we don\u2019t know which action to take that will lead us to the \u201cbetter\u201d state. This necessitates the action value function, according to which we can choose the best action of each state by choosing the action with the largest action value (\\(\\arg \\max_a q_\\pi(s,a)\\)).</p> <p>This approach sounds ideal, but we have to face a problem: Because our policy is deterministic to this point, some state-action pairs can never be experienced, thus we cannot approximate the action value for those pairs. This is problematic since the purpose of RL is to find the best action for each state from all possible actions from that state. There are two ideas for addressing this issue:</p> <ol> <li>Specifying that an episode start from a certain state-action pair, and ensure that every state-action pair has non-zero probability of being chosen in the start</li> <li>Consider only policies that are stochastic, with a non-zero possibility of selecting all actions in each state</li> </ol> <p>This algorithm is implemented following the first idea:</p> <p></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#43-on-policy-monte-carlo","title":"4.3 On-policy Monte Carlo","text":"<p>The method of Exploring Start inherits certain drawbacks: firstly, it\u2019s (almost) unachievable for problems with really large state and action space; secondly, there are certain state-action pairs that are so infrequently encountered or simply cannot happen, that trying to calculate their value turns out to be unnecessary.</p> <p>To solve for this problem, we come up with a policy that isn\u2019t strictly deterministic, but reserves a probability for choosing a random action. One example of such policy is \\(\\epsilon -\\)greedy policy, in which the non-greedy actions are selected with probability \\(\\frac{\\epsilon}{|\\mathcal A(s)|}\\), whereas the greedy one is selected with probability \\(1-\\epsilon + \\frac{\\epsilon}{|\\mathcal A(s)|}\\) (\\(1-\\epsilon\\) chance being chosen as a greedy action and \\(\\frac{\\epsilon}{|\\mathcal A(s)|}\\) chance being chosen as a non-greedy one). Apart from that, the algorithm for the control problem is exactly the same. Notice that when you repeatedly changing the policy to a better one according to the updated action value function, and then generate another episode according to that changed policy, the policy and value function will converge to the optimal policy and optimal value function respectively:</p> <p></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#44-off-policy-monte-carlo","title":"4.4 Off-policy Monte Carlo","text":"<p>HOWEVER, the \\(\\epsilon-\\)soft policy approach still faces a drawback: it\u2019s inherently inferior than a deterministic one (we still have a probability of choosing a non-greedy action). This is the classic problem of exploration vs exploitation tradeoff. The idea of an off-policy method is that we will use 2 policies: a behavior policy which agent uses to generate learning data, and a target policy which agent try to optimize. The target policy should a deterministic optimal policy while the behavior policy remain stochastic and more exploratory (like \\(\\epsilon\\)-greedy policy).</p> <p>But there\u2019s one problem: because the returns we receive following \\(b\\) has different expectation compared to \\(\\pi\\): \\(\\mathbb E_b[G_t| S_t = s] = v_b(s) \\neq v_\\pi(s)\\), so they cannot be averaged to obtain \\(v_\\pi(s)\\). To address this, we use the method of Importance Sampling, in which the returns are weighted base on the relative probability of the trajectory according to the 2 policies.</p> <p>To understand this concept intuitively, we begin with the formula for calculating probability of a specific trajectory - \\(S_t, A_t, S_{t+1}, A_{t+1}, ..., S_{T-1}, A_{T-1}, S_{T}\\) - according to a specific policy - \\(\\pi\\):</p> \\[ \\begin{align*}  &amp; \\Pr \\{S_t, A_t, S_{t+1}, A_{t+1},..., S_T|S_t, A_{t:T-1} \\sim \\pi \\} \\\\ = &amp; \\textcolor{green}{\\Pr \\{S_t, A_t, S_{t+1}}\\} \\times \\textcolor{blue}{\\Pr\\{S_{t+1}, A_{t+1}, S_{t+2}\\}} \\times... \\times \\textcolor{purple}{\\Pr \\{S_{T-1}, A_{T-1}, S_T \\}} \\\\ = &amp; \\textcolor{green} {\\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)} \\textcolor{blue}{\\pi(A_{t+1}|S_{t+1})p(S_{t+2}| S_{t+1}, A_{t+1})}...\\textcolor{purple}{\\pi(A_{t-1}|S_{t-1})p(S_t|S_{t-1},A_{t-1})} \\\\ = &amp; \\prod_{k=t}^{T-1} \\pi(A_k | S_k)p(S_{k+1}|S_k, A_k) \\end{align*} \\] <p>Thus the relative probability of that same trajectory but follow 2 different policies:</p> \\[ \\rho_{t:T-1} = \\frac{\\prod_{k=t}^{T-1} \\pi(A_k | S_k) \\textcolor{magenta}{p(S_{k+1}|S_k, A_k)}}{\\prod_{k=t}^{T-1} b(A_k | S_k)\\textcolor{magenta}{p(S_{k+1}|S_k, A_k)}} = \\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)}{\\prod_{k=1}^{T-1}b(A_k|S_k)} \\] <p>Because the dynamics of the environment is the same no matter which policy we follow, the terms \\(\\prod_{k=t}^{T-1}p(S_{k+1}|S_k, A_k)\\) are cancelled out.</p> <p>We can use this ratio to transform the returns to have the right expected value. The intuition here is that, if a trajectory more frequently appears in the target policy than in it does in the behavior policy, then \\(\\rho_{t:T-1} &gt; 1\\)</p> \\[ \\mathbb E_\\pi[G_t|S_t = s] = \\mathbb E_b[\\rho_{t:T-1}G_t|S_t=s] \\] <p>It\u2019s easy to derive our update formula of the value function as:</p> \\[ v_\\pi(s) = \\frac{\\sum_{t \\in \\mathcal J(s)}\\rho_{t:T(t)-1}G_t}{|\\mathcal J(s)|} \\] <p>This kind of Importance Sampling is called Ordinary Importance Sampling, there\u2019s also an alternative one, called Weighted Importance Sampling, which use a weighted average instead of the average of weighted returns:</p> \\[ v_\\pi(s) = \\frac{\\sum_{t \\in \\mathcal J(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t \\in \\mathcal J(s)}\\rho_{t:T(t)-1}} \\] <p>We finally arrive at our algorithm for Off-policy MC Control:</p> <p></p> <p>Okay, there\u2019re a lot that need explaining here. Firstly, what are those weird updating formula? They are exactly the update formula using weighted importance sampling, but in an incremental way. Here\u2019s the proof for those incrementally implemented formulas. If you\u2019re not that curious, it\u2019s totally fine to just skip this part and accept the formulas as they are.</p> <p>Hypothesis:</p> <p>Suppose that we have a sequence of returns that start from one state \\(G_1, G_2,..., G_{n-1}\\) with their corresponding weights \\(W_i (i: 1 \\rightarrow n-1)\\), the value of that state at the \\(n^{th}\\) update is:</p> \\[ V_n = \\frac{\\sum_{k=1}^{n-1}W_kG_k}{\\sum_{k=1}^{n-1}W_k} \\] <p>Then the updating formula would be:</p> \\[ \\begin{align*} V_{n+1} &amp; = V_n + \\frac{W_n}{C_n}(G_n - V_n) \\\\  \\text{in which} \\qquad C_n &amp; = C_{n-1} + W_n\\end{align*} \\] <p>Proof:</p> \\[ \\begin{align*} V_{n+1} &amp; = \\frac{\\sum_{k=1}^{n}W_kG_k}{\\sum_{k=1}^{n}W_k}\\\\ &amp; = \\frac{\\sum_{k=1}^{n-1}W_kG_k + W_{n}G_n}{\\sum_{k=1}^{n-1}W_k}\\cdot\\frac{\\sum_{k=1}^{n-1}W_k}{\\sum_{k=1}^{n}W_k} \\\\       &amp; = V_n \\cdot \\frac{\\sum_{k=1}^{n-1}W_k}{\\sum_{k=1}^{n}W_k}  + \\frac{W_nG_n}{\\sum_{k=1}^{n}W_k} \\\\      &amp; = V_n \\left( 1 - \\frac{W_n}{\\sum_{k=1}^{n}W_k}\\right) + G_n\\frac{W_n}{\\sum_{k=1}^{n}W_k} \\\\       &amp; = V_n + \\frac{W_n}{\\textcolor{red}{\\sum_{k=1}^{n}W_n}}(G_n-V_n)      \\end{align*} \\] <p>We can implement the red-colored sum recursively by defining \\(C_n = C_{n-1} + W_n\\) and \\(C_0 = 0\\). By doing so, we obtain the updating formulas exactly like those in the hypothesis.</p> <p>To this point you might be confusing about what the algorithm is trying to do in the orange and yellow box. Isn\u2019t the update formula for \\(W_t\\) supposed to be \\(W_{t-1}\\frac{\\pi(A_t|S_t)}{b(A_t | S_t)}\\), not \\(W_{t-1}\\frac{1}{b(A_t | S_t)}\\)? You\u2019re right, remember that our target policy \\(\\pi\\) is deterministic? Thus the probability of taking a certain action, in a certain state is either 1 or 0. If it\u2019s a 1, the update formula for \\(W_t\\) is \\(W_t = W_{t-1}\\frac{1}{b(A_t | S_t)}\\) (hence the term in the yellow box). If it\u2019s a 0, the update formula for \\(W_t\\) is \\(W_t = 0\\), the action value of the previous state-action pair remains the same; and the same happen to all states preceding that state in the episode, since if just one transition step in a trajectory cannot happen when using the target policy, the whole trajectory also cannot happen.</p> <p>Off-policy methods are often of greater variance and slower to converge, but is more powerful and general: on-policy methods are off-policy methods with target policy and behavior policy being the same.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#5-temporal-difference-learning","title":"5. Temporal Difference Learning","text":"<p>It turns out that there\u2019s a universal form for update formula in RL algorithm:</p> \\[ \\text{NewEstimate} \\leftarrow \\text{OldEstimate} + \\alpha[\\text{NewEstimate} - \\text{OldEstimate}] \\] <p>If we want to represent the estimate value of a state following MC method, which is the average of returns from that state, the update formula will be:</p> \\[ V(S_t) \\leftarrow V(S_t) + \\frac{1}{n}[G_t - V(S_t)] \\] <p>with \\(n\\) representing the number of returns calculated from that state so far (you can prove this update formula by yourself, or substitute \\(n\\) with \\(1, 2, 3,\u2026\\) to see what you\u2019ll receive). Now if \\(\\alpha\\) is constant, we obtain a method called Constant-\\(\\alpha\\) MC, which will not return the exact average of returns, but is convenient computationally. The idea of Temporal Difference (TD) learning is that instead of setting the \\(\\text{NewEstimate}\\) to \\(G_t\\), whose value we have to wait till the end of every episode to calculate, we set it to \\(R_{t+1} + \\gamma V(S_t)\\), whose value can be calculate as soon as a transition is made to the next state:</p> \\[ V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)] \\]"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#51-td-prediction","title":"5.1 TD Prediction","text":"<p>The algorithm for estimating \\(v_\\pi\\) based on the above update formula is called TD(0), or one-step TD:</p> <p></p> <p>By updating the value of each state as soon as we transit to the next state, the updates are distributed uniformly over the time interval of an episode, there are more information contains in each update (we use the updated value of preceding states to calculate for the target estimate for the succeeding states within a single episode), and we don\u2019t need to reach the end of an episode to be able to update the value function (which is really beneficial in continuing tasks or episodic tasks with long episodes). These reasons make TD methods converge faster than MC methods in practice.</p> <p>Getting confused? I feel you\u2026 Let me provide you another intuition to get you to take a grasp of the bigger picture. Here is the definition of the value function we\u2019ve known:</p> \\[ \\begin{align*} v_\\pi(s) &amp; = \\mathbb E[G_t | S_t = s] \\qquad \\qquad \\qquad \\qquad \\text{ }(5)\\\\ &amp; = \\mathbb E[R_{t+1} + \\gamma G_{t+1}|S_t = s] \\\\ &amp; = \\mathbb E[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s] \\qquad (6)\\end{align*} \\] <p>MC uses the estimate of \\((5)\\) as the target for updating the value function, while TD learning uses the estimate of \\((6)\\) as a target. The target of MC is an estimate because it only bases on a single sample, not a whole distribution of possible states and reward subsequent to that state (just the return, not the expected return). The target of DP is also an estimate but not for the same reason, indeed it\u2019s because the \\(v_\\pi(S_{t+1})\\) is just a current estimate of the value - \\(V(S_{t+1})\\). While the target of TD learning is an estimate for both reasons above.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#52-sarsa-on-policy-td-control","title":"5.2 Sarsa: On-policy TD Control","text":"<p>The same story goes for TD learning methods: If we want to improve the policy somehow, we should estimate action value function instead of state value function. The same as MC control algorithm: If you try to update the value function according to the current policy, and then choose actions according a policy derived from that updated value function, then the value function will converge to the optimal value function (\\(Q \\rightarrow q_*\\)):</p> <p></p> <p>Fun fact: the algorithm is called Sarsa since each update requires a quintuple of events \\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\\).</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#53-q-learning-off-policy-td-control","title":"5.3 Q-learning: Off-policy TD Control","text":"<p>In Q-learning algorithm, the action used to calculate the target in the update formula, is the one that maximize \\(Q(S\u2019,a)\\), not the one selected by a greedy policy derived by our current action value function (like in Sarsa). The target policy, which can be a deterministic one, is learned dependently from that policy derived from our current value function, which is required to be more explorative. Hence this algorithm is an Off-policy one.</p> <p></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#54-maximization-bias-and-double-learning","title":"5.4 Maximization Bias and Double Learning","text":"<p>The update formula for Q-learning has a maximization operation. In this algorithm, the maximum of estimated values is used implicitly as an estimate of the maximum (of true) value, causes something called maximization bias (consider a state with \\(q(s,a)\\) for all \\(a\\) is zero but the estimated value fluctuate around \\(0\\), the maximum of the true values is zero while the maximum of the estimated values is positive). Take a look at this example:</p> <p></p> <p>Figure 3: Comparison between Q-learning and Double Q-learning on a 4-state episodic MDP.   </p> <p>This is a simple MDP. The agent always starts from \\(A\\) and can choose between 2 actions: left or right. The right action transit it to a terminal state which gives a reward of 0. The left action transit it to another terminal state - \\(B\\), from which it can take many actions, all of which transit the it to the terminal state with a reward sampled from a normal distribution with \\(\\mu = -0.1\\) and \\(\\sigma = 1\\). In this example, it\u2019s optimal to always choose right. But as you can see Q-learning algorithm initially take the left action more often than the right action, and always choose it with probability much more than \\(5\\%\\), which is the probability enforced by the \\(\\epsilon-\\)greedy action selection with \\(\\epsilon = 0.1\\). Why does this happen, to be specific? It\u2019s reasonable to expect that \\(Q(A, r) = 0\\) and \\(Q(A, l) = -0.1\\). And you\u2019re right for the \\(Q(A, r)\\), but for \\(Q(A,l)\\), something else\u2019s happening here: because the \\(Q(B, a)\\) for \\(a \\in \\mathcal A(B)\\) fluctuate around \\(-0.1\\), there\u2019s a high chance that some of the action values from state \\(B\\) are positive, thus the maximum of \\(Q(B,a)\\) is positive. And because Q-learning always use the maximum action value as the target for value function update, \\(Q(A, l)\\) will be positive initially (\\(Q(A, l) \\leftarrow Q(A, l) + \\alpha [R_{B} + \\gamma \\max_aQ(B,a) - Q(A,l)]\\)), thus the agents always choose left in the beginning. Not until after a plenty of steps that the action values from state \\(B\\) approaches their true value, which is \\(-0.1\\), because of which the agent starts to choose right more often.</p> <p>The cause to this problem is using the same samples for both determining the maximizing action and to estimating its value. The solution proposed in Double Q-learning algorithm is using 2 value functions and updating them independently. In each timestep, we would use one for action selection: \\(A_* = \\max_a Q_1(s,a)\\) and one for action evaluation: \\(Q_2(A_*) = Q_2(s, \\max_a Q_1(s,a))\\), and by a probability of \\(0.5\\), we switch the role of those value functions: \\(Q_1(A_*) = Q_1(s, \\max_a Q_2(s,a))\\).</p> <p></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#6-n-step-bootstrapping","title":"6. n-step Bootstrapping","text":"<p>Congrats! You\u2019ve come such a long way. At this point, you can imagine the space of methods form a spectrum with one end is the one-step TD learning, the other end is MC, and the rest of them are methods whose update rules based on a number of rewards ranging from more than one, but less than the rest of them until termination. Those \u201cintermediate methods\u201d are called n-step TD learning. Below is the backup diagrams of n-step methods. A backup diagram shows how a state (or state-action pair) is updated by information transferred back from its successor states (or state-action pair), with white nodes represent states and black nodes represent actions.</p> <p></p> <p>Figure 4: Spectrum of RL algorithms, ranging from one-step TD to Monte Carlo.   </p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#61-n-step-td-prediction","title":"6.1 n-step TD Prediction","text":"<p>The update rule of n-step TD Prediction is just as we expect: instead of accumulating the reward all the way to terminal state, we do that up to timestep \\(t+n\\), then the rest is truncated to the value of state \\(S_{t+n}\\):</p> \\[ G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1}R_{t+n}+ \\gamma^n \\textcolor{orange}{V_{t+n-1}(S_{t+n})} \\] <p>Here\u2019s the complete algorithm:</p> <p></p> <p>It turns out that intermediate methods perform much better than extreme ones:</p> <p></p> <p>Figure 5: Performance of n-step TD methods varies with different values of \\(\\alpha\\), for different values of n, when applied to a random walk task with 19 states.   </p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#62-n-step-sarsa","title":"6.2 n-step Sarsa","text":"<p>To change the method from solving a prediction problem to a control problem, we simply switch the state value to action value, and then use an \\(\\epsilon-\\)greedy policy:</p> <p></p> <p>Notice that whether we keep the policy unchanged or change it with respect to \\(Q\\) depends on our purpose: whether we want to approximate the optimal value function and optimal policy, or just the value function for some arbitrary policy. Although our usual goal would be the former.</p> <p>For more intuition about the difference between one-step and n-step Sarsa, consider this Gridworld example:</p> <p></p> <p>Figure 6: Comparison between one-step Sarsa and 10-step Sarsa on how much each algorithm learns in a single episode using a Gridworld example.  </p> <p>This Gridworld example illustrates the enhance in speed of policy improvement in 2 algorithms: one-step Sarsa and 10-step Sarsa. In this problem, reward of all states are zero instead of the terminal state mark by G, which contains a positive reward. Suppose we initialize the value of all states to zero, and the agent take the path presented in the first panel. In one-step Sarsa, only the action immediately prior to the terminal state is strengthened, whereas in 10-step Sarsa, 10 last actions of the episode is strengthened, which leads to much faster learning.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#63-n-step-off-policy-learning","title":"6.3 n-step Off-policy Learning","text":"<p>The off-policy form of n-step TD is quite straightforward: we just have to add the importance sampling ratio in order to used the data generated from \\(b\\). But since in n-step methods, returns are constructed over n steps, so we\u2019re only interested in the relative probability up in just those n actions. The update formula for the off-policy version of n-step TD:</p> \\[ V_{t+n}(S_t) = V_{t+n-1}(S_t) + \\alpha \\rho_{t:t+n-1}(G_{t:t+n} - V_{t+n-1}(S_t)) \\\\ \\text{with } \\quad \\rho_{t:h} = \\prod_{k = t}^{\\min(h, T-1)} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)} \\] <p>Similarly, the off-policy form for our n-step Sarsa update formula is:</p> \\[ Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \\alpha \\textcolor{green}{\\rho_{t+1:t+n}} (G_{t:t+n} - Q_{t+n-1}(S_t, A_t)) \\] <p>Notice that in this formula the importance sampling ratio starts and ends 1 step later than in n-step TD. It\u2019s because here, the action whose value is estimated has already been chosen, so the trajectory probability up to that action for the behavior policy and target policy is both 1. Furthermore, the last value we need for bootstrapping is the value of a state-action pair, not of a state, so we have to include the probability of taking that last action in the importance sampling ratio. Here is pseudocode for the full algorithm, the only difference it makes from on-policy n-step Sarsa is the addition of the importance sampling ratio:</p> <p></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#64-off-policy-learning-without-importance-sampling-n-step-tree-backup-algorithm","title":"6.4 Off-policy learning without Importance Sampling: n-step Tree Backup Algorithm","text":"<p>Is there an off-policy method without importance sampling? We\u2019ve seen these methods, they are Q-learning and Expected Sarsa, but they bootstrap after just one timestep. Turns out that there is a corresponding algorithm but for multiple-step case, it\u2019s the n-step Tree Backup algorithm. How to compute the target for the update formula in this algorithm is best illustrated via a backup diagram:</p> <p>If in n-step Sarsa, the target is calculated by combining the rewards collected from timestep \\(t+1\\) to timestep \\(t+n\\), and the value of the state-action pair \\((S_{t+n}, A_{t+n})\\), then in n-step Tree Backup algorithm, the target is exactly that, plus the value of the actions that are not taken, at every timestep, which are the actions left hanging in the backup tree beside.</p> <p>We\u2019ll derive the formula for the update target from this backup tree. If our algorithm is just 1-step Tree Backup, the target is the same as one in Expected Sarsa:</p> <p></p> <p>Figure 7: Comparison between 3-step Sarsa and 3-step Tree Backup using a backup diagram. Nodes on the lines represent states and actions account for the value of the state-action pair at the roots.  </p> \\[ G_{t:t+1} = R_{t+1} + \\gamma \\sum_a \\pi(a | S_{t+1})Q(S_{t+1}, a) \\] <p>If our algorithm is a 2-step one, each action \\(a\\) at the first level contribute in calculating the target with the weight of \\(\\pi(a | S_{t+1})\\), except for action actually taken. Instead, its probability, \\(\\pi(A_{t+1} | S_{t+1})\\) is used to calculate the weight of action values at the second level. Thus each action \\(a'\\) at this level contributes with the weight of \\(\\pi(A_{t+1}|S_{t+1})\\pi(a\u2019|S_{t+2})\\):</p> \\[ \\begin{align*} G_{t:t+2} &amp; = R_{t+1} + \\gamma \\sum_{a\\neq A_{t+1}} \\pi(a | S_{t+1})Q(S_{t+1}, a) + \\gamma \\pi(A_{t+1} | S_{t+1}) \\left [R_{t+2} + \\gamma \\sum_{a'}\\pi(a'|S_{t+2})Q(S_{t+2}, a')\\right] \\\\ &amp; = R_{t+1} + \\gamma \\sum_{a\\neq A_{t+1}} \\pi(a | S_{t+1})Q(S_{t+1}, a) + \\gamma \\pi(A_{t+1} | S_{t+1})G_{t+1: t+2} \\end{align*} \\] <p>This recursive form is really helpful since we can form a more general update target formula for n-step Tree Backup algorithm, which is:</p> \\[ G_{t:t+n} = R_{t+1} + \\gamma \\sum_{a\\neq A_{t+1}} \\pi(a | S_{t+1})Q(S_{t+1}, a) + \\gamma \\pi(A_{t+1} | S_{t+1})G_{t+1: t+n} \\] <p>The complete pseudocode for the algorithm is presented in the box below, it\u2019s pretty similar to n-step Sarsa, only the update target is computed differently:</p> <p></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#7-problems-and-extension-regarding-tabular-methods","title":"7. Problems and Extension regarding Tabular Methods","text":""},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#71-dyna-integration-of-learning-and-planning","title":"7.1 Dyna: Integration of Learning and Planning","text":"<p>If we categorized all of the above algorithms, we\u2019ve got model-based (Dynamic Programming, in which we need the model of the environment to be able to update the value function) and model-free (Monte Carlo, Temporal Difference, in which we just need the data sampled directly from the environment) reinforcement learning. The process of updating the value function and improving the policy via interactions with the real environment or the model is called learning and planning respectively.</p> <p>When planning is done online (while interacting with the environment), there are a lot of problems need considering: how to update the model according to the continuously collected data from interaction with the real environment, how to choose state-action to update when interacting with the model,\u2026 Dyna-Q is a simple model demonstrates how the 2 processes of learning and planning is done in every timestep:</p> <p></p> <p>You can observe that for Dyna-Q, real experience serves 2 purposes: update value function and/or policy (direct learning) and update the model (the model is later used for planning, which is call indirect learning). The algorithm used for direct learning is Q-learning, and for planning is also Q-learning, but the state-action pairs are randomly sampled from seen ones.</p> <p>This model fit well to problems that the interaction between the agent and environment is limited, or that there\u2019s a high cost doing so. Integrating planning into the process of learning will probably takes the agent far fewer episodes to reach the optimal performance.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#72-when-the-model-is-wrong","title":"7.2 When the model is wrong","text":"<p>One problem arises when planning is that sometime the model is wrong. This can be caused by several reasons: the environment is stochastic and a small number of samples are observed; the model is learned by function approximation in which the function updated is either chosen to be too simple that it cannot represent the environment accurately, or too complex so that it failed to generalize; the environment changes over time;\u2026 This failure in constructing the model most of the time will lead to suboptimal policies due to planning. The fixing of the environment will lead to 2 scenarios - either the environment becomes better or worse for the current policy. Let\u2019s take a look at these example:</p> <p></p> <p>Figure 8: Examples of when the model changes for the worse (Blocking maze) and changes for the better (Shortcut maze)  </p> <p>The task of the agent is to find the shortest way from S to G. The first panel describes the scenario where the optimal path is blocked after the model is relearned (aka environment changes to become worse). The second describes the scenario where there\u2019s an additional shortcut when the model is relearned. Dyna-Q+ is fundamentally Dyna-Q with additional bonus in behavior policy that encourage exploration. In the former scenario, both our Dyna-Q agents can relearn a better policy after they\u2019ve been wandering around the right corner (the part where the graph flattens out). This happens because following a policy that they are so optimistic about leads the agents to new discoveries and updates that help them in finding out that the current policy is \u201cnot that optimal\u201d so that they work hard to find an optimal one. But in the latter scenario, the issue becomes complicated: the Dyna-Q agent doesn\u2019t realize that there\u2019s a better policy since the cost for discovering new policy is so large that it\u2019s safer for the agent to stay with the current one (in order to discover the optimal policy, the agent has to make a bunch of exploration steps into low-value states), however the Dyna-Q+ manages to find such policy since the bonus it\u2019s given for exploration at some point outweighs the accumulative reward it receives for sticking with the current policy.</p> <p>What gives Dyna-Q+ this exploration ability is that it\u2019s given a bonus reward for each state-action pair encountered:</p> \\[ r' = r+k\\sqrt{\\tau} \\] <p>where \\(\\tau\\) is the number of time steps that the \\(s-a\\) pair hasn't been tried.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#73-prioritize-sweeping","title":"7.3 Prioritize Sweeping","text":"<p>Prioritize Sweeping is an updating strategy in planning, in which the states being updated are prioritized base on there predicted change in value. One way to achieving this is to work backward from goal states, but to be more general, it is to work backward from states whose values are just changed. The full algorithm is presented below:</p> <p></p> <p>This approach is call backward focusing. There is another approach called forward focusing where priority is determined based on how easily a state can be reached from states that are frequently visited under our current policy.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#74-trajectory-sampling","title":"7.4 Trajectory Sampling","text":"<p>Trajectory Sampling is a strategy in distributing updates where instead of exhaustive sweep through the entire state space (like in DP), you update them according to an on-policy distribution, which is the distribution of states and actions encountered when following the current policy. This is beneficial for problems where the state space is too big that it\u2019s computationally expensive to sweep through all of them. Instead, there are some states that need more focus on, such as those being visited frequently following our current policy.</p> <p>Here is the result of a small experiment. The environment is designed so that each state-action pair can lead to \\(b\\) possible next state equally (\\(b\\) is called the branching factor). On all transitions there is a 0.1 chance of reaching the terminal state. And the expected reward of all transition is sampled from a Gaussian distribution with mean 0 and variance 1.</p> <p></p> <p>Figure 9: Relative performance between uniform sampling and on-policy sampling for a 1,000-state MDP with different branching factors.  </p> <p>While being beneficial for problems with a large state space in which just a small subset is visited frequently, trajectory sampling can be detrimental in the sense that it can end up causing the same states to be updated again and again. This drawback can be seen from the graph where the performance of uniform sampling is better than on-policy sampling asymptotically.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#section-2-case-study","title":"Section 2: Case Study","text":"<p>Here\u2019s a notebook that contains the code for all the algorithms mentioned above. In these experiments, we used 4 toy environments (Frozen Lake - deterministic, Frozen Lake - slippery, Cliff Walking and Taxi) provided by <code>gymnasium</code> - a library that provides API for lots of useful Reinforcement Learning environments. <code>Frozen Lake</code> and <code>Cliff Walking</code> are environments where the agent has to reach the goal (a present/cookie) without breaking the ice/falling off the grid, while <code>Taxi</code> is an environment where our agent (a taxi) navigates to passengers, picks them up and delivers them to the building.</p> <p>If there\u2019s a mistake that I\u2019ve made, or there is something in the notebook that confuses you, feel free to reach out to me!</p> <p>There are a few things that you should notice:</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#1-sarsa-vs-q-learning-in-cliff-walking","title":"1. Sarsa vs Q-learning in Cliff Walking","text":"<p>Sarsa chose the safer path than Q-learning. This happens because the state-action values calculated by Sarsa take into account all possible states and actions from the policy and the environment dynamics, this results in this algorithm concerning about both the reward of reaching the goal and the penalty of falling off the cliff, while the state-action values calculated by Q-learning only takes into account the actions that maximize the reward, hence the agent chose the shortest path to get to the goal. In other words, agent learned with Sarsa algorithm chose the path that maximized expected return, while agent learned with Q-learning algorithm chose the path that maximized greatest possible return.</p> <p>Figure 10: The optimal policy found by SARSA in Cliff Walking environment.  </p> <p>Figure 11: The optimal policy found by Q-learning in Cliff Walking environment.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#2-value-iteration-vs-policy-iteration","title":"2. Value Iteration vs Policy Iteration","text":"<p>It\u2019s easy to observe that Value Iteration yields better result than Policy Iteration, since it cancels out a lot of redundant iterative steps of approaching the truth value of states following a suboptimal policy.</p> <p></p> <p>Figure 12: Number of steps using Value Iteration and Policy Iteration, average over 5 experiments</p> <p></p> <p>Figure 13: The Frozen Lake environment and the corresponding Q-table learned by the Value Iteration algorithm. The direction of the arrow and the color of each state in the above graph represent the optimal action (action which has the highest value) and its corresponding value in each state.  </p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#3-how-many-steps-are-optimal-for-n-step-bootstrapping-algorithms","title":"3. How many steps are optimal for n-step bootstrapping algorithms?","text":"<p>The below plot shows the performance of n-step SARSA and n-step Tree Backup algorithms with different number of bootstrap steps. We can see that the step n set at 4 or 8 is optimal for both algorithms, in the Cliff Walking environment</p> <p></p> <p>Figure 14: Performance of n-step Tree Backup with n = [2, 4, 8, 16] in Cliff Walking environment.  </p> <p></p> <p>Figure 15: Performance of n-step Sarsa with n = [2, 4, 8, 16] in Cliff Walking environment.  </p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#4-the-best-algorithm-for-each-environment-not-include-n-step-bootstrapping-algorithms","title":"4. The best algorithm for each environment (not include n-step bootstrapping algorithms)","text":"<p>We can see that all the algorithms have comparative performance to one another in our toy environments such as Cliff Walking or Taxi, except for Monte Carlo. We can see from the graph \u201cSteps until terminated\u201d that Q-Learning is the fastest to converge, and Monte Carlo is the slowest. However, in different hyperparameter and environment settings, this may not remain true. Double Q-Learning algorithm may converge faster in more complex environment since it addresses the issue of positive bias made by Q-Learning. But in easy environments, it performs worse since there are two Q tables to be updated.</p> <p></p> <p>Figure 16: Comparison between different Reinforcement Learning algorithms in Cliff Walking environment.  </p> <p></p> <p>Figure 17: Comparison between different Reinforcement Learning algorithms in the Taxi environment.  </p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#5-the-strange-case-of-monte-carlos-gamma-fine-tuning","title":"5. The strange case of Monte Carlo\u2019s gamma fine-tuning","text":"<p>While running the experiment, we\u2019ve observed Monte Carlo\u2019s strange behaviors: its inability to converge although the greedy hyperparameter \\(\\epsilon\\) was set to be very high. This happened because the discounted factor (\\(\\gamma\\)) was so small (\\(0.9\\)) that the accumulated penalty for exploring around (the agent had greater risk of falling off the cliff, which gave a penalty of -100 in return) was much worse than standing at the same state or repeatedly visiting the same states (the maximum number of timesteps is 1000, so the accumulative penalty of standing at the same state was \\(\\frac{0.9^{1000}\\times(-1) - 0.9^0 \\times (-1)}{0.9 - 1} \\approx -10\\)), which was approximately the same as wandering around about 60 timesteps, and then entering the terminal state?!  To address this problem, we had to increase the gamma (\\(\\gamma\\)) hyperparameter to \\(0.99\\) or \\(1\\), and the algorithm converged perfectly.</p> <p></p> <p>Figure 18: Monte Carlo cannot converge in Cliff Walking environment when discount factor (\\(\\gamma\\)) is set to \\(0.9\\).  </p> <p></p> <p>Figure 19: Monte Carlo algorithm converges in Cliff Walking environment when discount factor (\\(\\gamma\\)) is set to \\(1\\).  </p> <p>This is also one of the biggest problems of Reinforcement Learning, that is the time-consuming task of hyperparameter tuning: just a small change in hyperparameter can make the algorithm to be extremely unstable.</p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#conclusion","title":"Conclusion","text":"<p>We\u2019ve gone a pretty far way through different classical Reinforcement Learning algorithms, and applied them to some toy environments like Frozen Lake, Cliff Walking and Taxi. We\u2018ve also learned about the trade-offs between those algorithms, between on-policy and off-policy methods, and between exploration and exploitation in finding the optimal solution; about different strategies to improve the efficiency of exploration and updating of state/action values; and about the drawbacks of Reinforcement Learning algorithms in general and how to avoid (well, a part of) them. We hope that this article will invoke your enthusiasm and eagerness to explore more about the subject matter.</p> <p>If you want to discuss about certain parts of the article, or interesting RL ideas, feel free to contact me via email <code>tramdang7907129@gmail.com</code></p>"},{"location":"blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/#references","title":"References","text":"<p>[1] R. S. Sutton and A. Barto, Reinforcement learning : An introduction. London: The Mit Press, 2018.</p> <p>[2] Gymnasium documentation. (n.d.). Retrieved November 7, 2024, from https://gymnasium.farama.org</p>"},{"location":"challenges/json-in-cpp/","title":"JSON in C++","text":"","tags":["JSON","C++","Build from scratch"]},{"location":"challenges/json-in-cpp/#introduction","title":"Introduction","text":"<p>JSON (JavaScript Object Notation) is a lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate. It is often used for data transmission between a server and a web application, or between different parts of an application. JSON is language-independent, which means it can be used with virtually any programming language.</p> <p>You can find more information about JSON here.</p> <p>Parsing and generating JSON is supported by most programming languages through built-in libraries or third-party packages, making it easy to work with JSON data in various software applications.</p> <p>There are many JSON libraries available for C++. To explore how they work, let's build a simple JSON library from scratch.</p>","tags":["JSON","C++","Build from scratch"]},{"location":"challenges/json-in-cpp/#learning-objectives","title":"Learning Objectives","text":"<p>Through this challenge, you will:</p> <ul> <li>Interpret and understand the JSON data format.</li> <li>Learn how to encode (serialize) and decode (deserialize) JSON data.</li> <li>Overload operators to make your code more readable and intuitive.</li> <li>Smart pointers and memory management.</li> <li>Differentiate between static typing and dynamic typing in programming languages (not only C++).</li> <li>Move semantics and rvalue references.</li> </ul>","tags":["JSON","C++","Build from scratch"]},{"location":"challenges/json-in-cpp/#your-tasks","title":"Your Tasks","text":"<p>You must create a class named <code>Json</code> that can:</p> <ul> <li> Construct a JSON object from a JSON string literal.</li> <li> Have a function named <code>toString</code> that returns a JSON string literal that represents the JSON object.</li> <li> Can get/set JSON object properties using the <code>[]</code> operator.</li> <li> Make sure your code is memory-safe and does not have memory leaks.</li> <li> Define copy constructor, move constructor and assignment operators.</li> </ul> <p>There are some examples of how your code should work:</p> <pre><code>Json json = Json::parse(R\"({\"name\": \"John\", \"age\": 30})\");\nstd::cout &lt;&lt; json.toString() &lt;&lt; std::endl;// {\"name\": \"John\", \"age\": 30}\n\njson[\"name\"] = \"Jane\";\njson[\"age\"] = 20;\nstd::cout &lt;&lt; json.toString() &lt;&lt; std::endl; // {\"name\": \"Jane\", \"age\": 20}\n\nstd::cout &lt;&lt; json[\"name\"] &lt;&lt; std::endl; // Jane\n</code></pre> <p>Don't use any third-party libraries. You can only use the standard library and the STL.</p>","tags":["JSON","C++","Build from scratch"]},{"location":"challenges/personal-finance-app/","title":"Personal Finance Management Application","text":"<p>In this challenge, you will create a personal finance management application. The primary objective is to improve your coding skills, so you can adapt the project to suit your needs and preferences. You should create a mobile app, maybe cross-platform, depending on your coding experience and preferences.</p> <p>Here are some guidelines to help you get started.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#define-your-project-scope","title":"Define Your Project Scope","text":"<p>Clearly outline the objectives of your project. Decide what specific features you want to include in your app. Since your primary goal is to improve your coding skills and learn new things, focus on functionality and not the visual aspects.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#choose-a-development-platform","title":"Choose a Development Platform","text":"<p>Select a development platform that you're comfortable with (and which type of smartphone you are using ). You can opt for mobile development using a framework like React Native, Flutter, or a platform-specific language like Swift (iOS) or Kotlin (Android).</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#core-features","title":"Core Features","text":"<p>Concentrate on the essential functionalities:</p> <ul> <li>Expense Tracking: Implement a system to input and store your daily expenses.</li> <li>Budget Management: Create a feature that allows you to set and manage budgets.</li> <li>Income Tracking: Add functionality to record your sources of income.</li> <li>Savings Goals: Include a simple savings goal tracker.</li> </ul> <p>Nice-to-have features:</p> <ul> <li>Reports: Create a basic reporting system to display your financial data.</li> <li>Reminders: Add a reminder system to help you stay on track with your budget.</li> </ul>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#storing-data","title":"Storing Data","text":"<p>Set up a database using SQLite to store your financial data.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#user-interface-ui","title":"User Interface (UI)","text":"<p>While the focus is on coding skills, design a basic and functional user interface. This doesn't need to be overly complex or visually appealing; it should allow you to interact with your app effectively.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#testing-and-debugging","title":"Testing and Debugging","text":"<p>Test your app thoroughly and fix any bugs you encounter. This is a valuable part of the learning process.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#security","title":"Security","text":"<p>Even if the app is just for your personal use, practice secure coding. Sanitize inputs, protect against SQL injection, and secure your database.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#documentation","title":"Documentation","text":"<p>Document your code and create comments to make it easier for another developers to understand and maintain. As your coding skills grow, you can continuously enhance your app. You might add more features, improve the user interface, or even consider implementing additional technologies and tools.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#learning-resources","title":"Learning Resources","text":"<p>Utilize online tutorials, documentation, and coding forums to help you overcome challenges and learn new coding techniques.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#version-control","title":"Version Control","text":"<p>Use <code>git</code> and <code>Github</code> to keep track of changes in your code and collaborate with yourself, especially if you plan to take this challenge with your team.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#time-management","title":"Time Management","text":"<p>Set aside dedicated time for your coding project. According to project size, it should take less than 75 days. Consistency and regular practice will help you improve your coding skills faster. Don't rely on motivation alone; discipline is essential.</p>","tags":["Mobile","SQL"]},{"location":"challenges/personal-finance-app/#keep-it-personal","title":"Keep It Personal","text":"<p>Remember that this is your personal project for learning and improving. Don't worry too much about making it perfect. The primary objective is to learn and grow.</p> <p>By following these guidelines, you can create a personal finance management app for self-improvement while gaining valuable coding experience. As your skills improve, you can adapt and expand the app to meet your evolving needs and explore new coding challenges.</p>","tags":["Mobile","SQL"]},{"location":"challenges/csv-query-tool/","title":"Building the <code>exql</code> program","text":"","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#introduction","title":"Introduction","text":"<p>Greetings, collaborators ! Welcome to an engaging programming challenge that will introduce you to the creation of a program capable of querying data from CSV files using the SQL query language. This task will not only enhance your programming skills but also provide insights into structured data manipulation.</p>","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#key-concepts","title":"Key Concepts","text":"<p>Before we delve into the details, let's cover some important concepts:</p> <ul> <li>CSV (Comma-Separated Values): CSV is a simple file format used to store tabular data. Each row in the file corresponds to a row in the table, and data fields within each row are separated by commas.</li> <li>SQL (Structured Query Language): SQL is a powerful language used for managing and manipulating structured data in databases. It enables various operations like querying, inserting, updating, and deleting data.</li> </ul>","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#your-tasks","title":"Your tasks","text":"<p>Your task is to develop a program named <code>exql</code> that can read CSV files and execute SQL-like queries on the data within. When executed with a specific query and CSV file name as parameters, the program should display the queried data on the screen. Here's an example of usage:</p> <pre><code>exql \"SELECT * FROM data.csv\"\n</code></pre> <p>In this example, the <code>exql</code> program will read the <code>data.csv</code> file and execute the <code>SELECT *</code> query, fetching all data from the file. The program will then display the retrieved data on the terminal.</p> <p>To make it easier, we can assume that CSV files is always a table <code>M</code>x<code>N + 1</code>, with the first row contains column names, <code>M</code> is number of columns and <code>N</code> rows of data. All column names have <code>snake_case</code> format. For instance:</p> id first_name last_name age major #1 Vinh Nguyen Phuc 22 Computer Science #2 Quan Nguyen Hong 20 Computer Engineering #1 Phu Nguyen Ngoc 20 Mechanical Engineering","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#requirements","title":"Requirements","text":"<p>Your program should have these components:</p> <ul> <li>Command-line interface: The program should be run from the command line, accepting the CSV file name and SQL-like query as parameters.</li> <li>Query parsing: Implement a mechanism to parse the provided query parameter. The program should understand basic SQL-like syntax, including <code>SELECT</code>/<code>INSERT</code> statements. <code>DELETE</code>/<code>UPDATE</code> statements are not required.</li> <li>CSV Reading: Design the program to read and interpret CSV files. Assume that the CSV files are properly formatted and include header rows.</li> <li>Query Execution: Process the query and retrieve appropriate data from the CSV file.</li> </ul> <p>The executable file <code>exql</code> must be:</p> <ul> <li>Run and compiled on Linux operating system</li> <li>Use C++ standard 17</li> <li>NO MEMORY LEAKS after program exit</li> <li>Pass all test cases</li> </ul> <p>The language description is at here.</p>","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#learning-objectives","title":"Learning Objectives","text":"<p>Through this challenge, you will:</p> <ul> <li>Learn the fundamentals of SQL and its application for data querying.</li> <li>Enhance your programming skills by building a data manipulation tool.</li> <li>Gain practical experience with I/O operations for reading and processing CSV files.</li> <li>This assignment is designed to gradually introduce you to these concepts. If you are new to SQL or databases, don't worry - you'll have access to various resources and guidance to help you successfully complete the task.</li> </ul> <p>Furthermore, you will have ideas with:</p> <ul> <li>The difference between high level programming languages (<code>C#</code>, <code>Java</code>, <code>Javascript</code>, <code>Python</code> ...) and low level ones (<code>C/C++</code>, <code>Rust</code> ...)</li> <li>How computers understand programming/scripting languages</li> </ul>","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#timelines","title":"Timelines","text":"<p>The challenge will last for 8 weeks but may extend or conclude sooner based on your ability.</p> <ul> <li>Week 1: Set up working environment and find out more about this challenges</li> <li>Week 2: Introduce to CLI, query validating &amp; query parsing</li> <li>Week 3 &amp; Week 4: Construct a query tree</li> <li>Week 5: Validate query commands against a file definition.</li> <li>Week 6: Comming soon ...</li> </ul>","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/#documents-keywords","title":"Documents / Keywords","text":"<ul> <li>Learn CPP: https://www.learncpp.com/</li> <li>CPP Roadmap for developers (advanced): https://roadmap.sh/cpp</li> <li>RAII </li> <li>Rule of zero, rules of three, rules of five</li> <li>Cpp Core Guidelines: https://isocpp.github.io/CppCoreGuidelines/CppCoreGuidelines#main</li> </ul>","tags":["SQL","C++","Principle of programming languages (PPL)"]},{"location":"challenges/csv-query-tool/language-description/","title":"CSV Query Language Description","text":"<p>This language is similar to SQL and is designed for manipulating CSV files. It supports two main types of operations: <code>SELECT</code> and <code>INSERT</code>.</p>"},{"location":"challenges/csv-query-tool/language-description/#insert-statement","title":"<code>INSERT</code> Statement","text":"<p>The INSERT statement in your language is similar to the native SQL INSERT statement, but with a twist tailored for CSV files. </p> <p>It allows you to add new rows of data to a CSV file. The syntax is as follows:</p> <pre><code>INSERT INTO &lt;file_name&gt; [(column1, column2, ...)] VALUES (value1, value2, ...);\n</code></pre> <p>Here's a breakdown of the components:</p> <ul> <li><code>&lt;file_name&gt;</code>: Specifies the name of the CSV file where you want to insert the data.</li> <li><code>(column1, column2, ...)</code>: Lists the columns into which you want to insert data. If not defined, use list of table columns by default.</li> <li><code>(value1, value2, ...)</code>: Provides the values to be inserted into the specified columns</li> </ul>"},{"location":"challenges/csv-query-tool/language-description/#select-statement","title":"<code>SELECT</code> Statement:","text":"<p>The SELECT statement in your language resembles the native SQL SELECT statement, adapted for CSV file querying. </p> <p>While it lacks support for GROUP BY, HAVING, and aggregate functions, it offers other essential features like WHERE condition and ORDER BY options.</p> <p>The syntax is as follows:</p> <pre><code>SELECT [* | (column1, column2, ...)]\nFROM (&lt;file_name&gt; | (select_statement))\nWHERE &lt;condition&gt;\nORDER BY column_name [ASC | DESC];\n</code></pre> <p>SELECT statement also can have nest one as describe above. For instance:</p> <pre><code>SELECT id, name, email, phone FROM (\n  SELECT id, name, email, age, phone\n  FROM member.csv\n  WHERE age &gt; 22\n)\nWHERE name LIKE '%Vinh%'\n</code></pre>"},{"location":"challenges/csv-query-tool/language-description/#condition-syntax-rule","title":"<code>&lt;condition&gt;</code> syntax rule","text":"<ul> <li> <p>A <code>&lt;condition&gt;</code> can be relational or logical expression.</p> </li> <li> <p>A relational expression have syntax rule as follows: <code>&lt;col_name&gt; (= | &gt; | &lt; | &gt;= | &lt;= | LIKE) &lt;value&gt;</code>;</p> </li> <li>A logical expression (combination of two or more relational conditions) have syntax rule as follows: <code>[\"&lt;relational_condition&gt; [(AND | OR) &lt;relational_condition&gt;]</code> and can be wrapped by a pair of round brackets.</li> </ul>"},{"location":"challenges/csv-query-tool/language-description/#filename-requirements","title":"Filename Requirements","text":"<ul> <li><code>&lt;file_name&gt;</code> must possess the .csv file extension</li> </ul>"},{"location":"challenges/csv-query-tool/language-description/#value","title":"<code>&lt;value&gt;</code>","text":"<p>A <code>&lt;value&gt;</code> can be:</p> <ul> <li>Text: Text is a sequence of characters enclosed in double quotes (\"). For example, \"This is a text.\" is a valid text.</li> <li>Number: A number can be an integer or a float. Integers are whole numbers, such as 1, 5, and 100. Floats are numbers that contain a decimal point, such as 3.14, -5.67, and 1.234567e8.</li> <li>Boolean: A boolean value is either true or false.</li> </ul>"},{"location":"challenges/csv-query-tool/timelines/week1/","title":"Week 1","text":""},{"location":"challenges/csv-query-tool/timelines/week1/#goals","title":"Goals","text":"<ol> <li>Set up working environment</li> <li>Install Ubuntu (recommend version &gt;= 22.04) or other Linux distributions as your OS </li> <li><code>gcc/g++</code> v11.4.0</li> <li><code>git</code> latest version</li> <li>Create Github account</li> <li>Install a text editor like <code>VS Code</code>, <code>Atom</code> ... (depend on your choice)</li> <li>Install Valgrind. This tool will help you detect memory leaks in your program</li> <li>Set up your own repository on Github to store solution for this challenge.</li> <li>Try SQLite</li> <li>Read the challenge described in README.md carefully.</li> </ol>"},{"location":"challenges/csv-query-tool/timelines/week1/#question","title":"Question","text":"<p>Regarding to requirements, there will be 4 parts in your program:</p> <ul> <li>Command-line interface</li> <li>Query parsing</li> <li>CSV Reading</li> <li>Query Execution</li> </ul> <p>Explain your idea about this.</p>"},{"location":"challenges/csv-query-tool/timelines/week2/","title":"Week 2","text":"<p>In this week, we will implement:</p> <ul> <li>Command-Line Interface</li> <li>Query Parsing (For INSERT statement only)</li> </ul>"},{"location":"challenges/csv-query-tool/timelines/week2/#command-line-interface","title":"Command-Line Interface","text":"<p>Below is a basic \"Hello World\" program in C++:</p> <pre><code>#include &lt;iostream&gt;\n\nint main() {\n    std::cout &lt;&lt; \"Hello World!\" &lt;&lt; std::endl;\n    return 0;\n}\n</code></pre> <p>To execute this code, we need to compile it into an executable binary file (let's assume its name is <code>hello</code>) and then run it from the terminal:</p> <pre><code>./hello\n</code></pre> <p>However, our goal is to enhance this CLI to print customized messages, not limited to \"Hello, World!\". For instance:</p> <p>Now, I need to print any words whatever I want, not just \"Hello world !\". For examples:</p> <pre><code>&gt; ./hello \"Hello!\"\nHello!\n&gt; ./hello \"We are at TickLab!\"\nWe are at TickLab!\n</code></pre> <p>Let's explore how to achieve this functionality !</p>"},{"location":"challenges/csv-query-tool/timelines/week2/#query-parsing","title":"Query Parsing","text":""},{"location":"challenges/csv-query-tool/timelines/week2/#why-query-strings","title":"Why query strings ?","text":"<p>Why do we use query strings (or more broadly, programming languages)? Imagine you're communicating with your program, instructing it to:</p> <ul> <li>Retrieve specific data for you</li> <li>Recognize that you only require the age and name columns</li> <li>Retrieve this data from a file named abc.csv</li> </ul> <p>However, your program might struggle to comprehend human language. Amidst all your words, it only needs key information and keywords like \"read,\" \"age,\" \"name,\" and \"abc.csv.\" Consequently, you could distill crucial details into something like exql \"read\", \"age\", \"name\", \"abc.csv\". But from a human perspective, this command might be challenging to grasp.</p> <p>Programming/scripting languages emerged to facilitate mutual understanding between humans and computers.</p> <p>Thanks to tools such as compilers and interpreters that act as translators, we are spared from manually translating our code into machine language. So, please code for human reading, not computer reading.</p>"},{"location":"challenges/csv-query-tool/timelines/week2/#validating-and-analyzing-queries","title":"Validating and Analyzing Queries","text":"<p>Let's consider an example command line instruction:</p> <pre><code>exql \"SELECT id, first_name, last_name, email FROM personnel.csv\"\n</code></pre> <p>The structure of the query string resembles SQL, which makes it relatively easy to discern that we're instructing the program to:</p> <ul> <li>Retrieve personnel data from a file named personnel.csv</li> <li>Extract specific attributes: IDs, first names, last names, and emails</li> </ul> <p>From the computer's perspective, the pertinent details consist of:</p> <ul> <li><code>SELECT</code></li> <li><code>id</code>, <code>first_name</code>, <code>last_name</code>, <code>email</code></li> <li><code>personnel.csv</code></li> </ul> <p>The term \"FROM\" doesn't carry meaning for the computer, so we should focus on extracting only the relevant information within our program. However, consider the query string \"ABCDEF SELECT 123 id, first_name Hello world abcd.txt.\" How can we extract crucial information from an unstructured string like this?</p> <p>This is where query validation becomes important.</p> <p>Query validation serves as a way to ensure that the provided query adheres to a specific syntax or pattern that the program can understand. It aids in distinguishing valuable components from noise in the query string. By validating the query, we can reliably extract essential elements.</p>"},{"location":"challenges/csv-query-tool/timelines/week2/#goals","title":"Goals","text":"<p>Follow the language description:</p> <ul> <li>Implement a CLI (command-line interface) that receive query string as parameter and then print it to console</li> <li>Validate and extract data from <code>INSERT</code> statement</li> </ul>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/","title":"Week 3 &amp; Week 4","text":"<p>In week 2, we took significant steps by implementing a simple CLI and validating and extracting data from <code>INSERT</code> statements.</p> <p>Now, our journey progresses to a more intricate aspect of our language: handling `SELECT`` statements.</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#convert-query-string-into-a-data-structure","title":"Convert query string into a data structure","text":"<p>In our journey to process and understand queries more effectively, we introduced the concept of a <code>query command</code>. This structured data representation serves as a bridge between the raw query and the eventual execution process. While our <code>INSERT</code> statement could be elegantly encapsulated using a simple C++ struct, the complexities of the <code>SELECT</code> statement necessitate a more sophisticated approach.</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#representing-insert-with-a-struct","title":"Representing <code>INSERT</code> with a Struct","text":"<p>We can see that the data extracted from an <code>INSERT</code> statement could be neatly captured within a C++ struct:</p> <pre><code>struct InsertCommand {\npublic:\n    string filename;\n    vector&lt;string&gt; columnNames;\n    vector&lt;string&gt; values;\n};\n</code></pre> <p>This straightforward structure aligns well with the simplicity of the <code>INSERT</code> statement.</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#challenges-of-representing-select","title":"Challenges of Representing <code>SELECT</code>","text":"<p>However, the <code>SELECT</code> statement introduces a new set of challenges. Its syntax supports subqueries and multiple conditions, making it impossible to represent the query's structure using a predefined struct. For instance:</p> <pre><code>SELECT id, name, email\nFROM (\n    SELECT id, name, email, age\n    FROM members.csv\n    WHERE age &gt; 22\n)\nWHERE name LIKE '%Vinh%' AND email LIKE '%@hcmut.edu.vn';\n</code></pre> <p>As seen in the above example, the structure of a <code>SELECT</code> query depends on how it's expressed, and it can involve subqueries and multiple logical conditions.</p> <p>To tackle the complexities introduced by the <code>SELECT</code> statement, we need a dynamic data structure (e.g tree, linked list ...). This hierarchical structure allows us to represent the intricate relationships present in a query. The data structure will help us:</p> <ul> <li>Organize Logic: Hierarchically organize the various clauses, subqueries, conditions, and relationships in the query.</li> <li>Understand Semantics: Reflect the semantics of the query, ensuring that we correctly interpret the programmer's intentions.</li> <li>Optimize Execution: Serve as a foundation for query optimization, enabling us to choose the most efficient execution plan.</li> </ul>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#constructing-the-query-command","title":"Constructing the Query Command","text":"<p>Constructing the query command involves two main stages: tokenization using a lexer and syntactic analysis using a parser. These processes work together to break down the raw query into manageable tokens and understand their syntactic and semantic relationships.</p> <ul> <li>Lexer (Tokenization): Transform the raw query string into a sequence of tokens, categorizing them by type and lexeme. These tokens form the basis for further analysis.</li> <li>Parser (Syntactic Analysis): The parser processes the stream of tokens generated by the lexer. It constructs a structured representation of the query, often called a parse tree or syntax tree. The parser ensures that the token sequence adheres to the language's grammar rules.</li> <li>Query Command Construction: The parse tree serves as a blueprint for building the query command. This higher-level representation captures the query's semantics and removes unnecessary syntactic details.</li> </ul> <p>By constructing and utilizing a query command, we bridge the gap between a programmer's query and its execution. The command provides a structured, semantically meaningful representation, essential for optimizing, understanding, and executing queries effectively.</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#requirements","title":"Requirements","text":"<p>Because our CSV query language is not </p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#lexer-tokenization","title":"Lexer (Tokenization)","text":"<p>The Lexer phase is responsible for breaking down the input query string into tokens, each representing a meaningful component of the language. Each token will have a type and a value. Here's what's expected from the Lexer.</p> <ol> <li>Token Definition<ul> <li>Define a set of token types that capture the different components of the language. This includes keywords, identifiers, values, punctuation symbols, and other relevant elements. For example:<ul> <li>ACTION: Represents actions like \"INSERT\" or \"SELECT\".</li> <li>INTO: Represents the keyword \"INTO\".</li> <li>FROM: Represents the keyword \"FROM\".</li> <li>VALUES: Represents the keyword \"VALUES\".</li> <li>ID: Represents identifiers like filenames, values, or column names.</li> <li>...</li> </ul> </li> </ul> </li> <li>Token Extraction</li> <li>Implement the mechanism to identify and extract tokens from the query string based on defined patterns.</li> <li>Regular expressions (using <code>std::regex</code> or similar libraries) can be highly effective for this purpose.</li> <li>Token Classification</li> <li>Assign the appropriate token type to each extracted token based on the identified patterns.</li> <li>Ensure that each token is categorized accurately.</li> <li>Token Creation</li> <li>Create a Token struct that encapsulates the type and value of each token. For instance:         <pre><code>enum TokenType { ... };\nstruct Token {\n    TokenType type;\n    string value;\n};\n</code></pre></li> <li>Handling Whitespace<ul> <li>Skip over unnecessary whitespace and comments during tokenization. These elements don't contribute to the semantics of the language. </li> </ul> </li> <li>Error Handling<ul> <li>Implement appropriate error handling mechanisms. If the lexer encounters an unrecognized sequence of characters, it should generate an error and halt the process.</li> </ul> </li> </ol> <p>For example, give an <code>INSERT</code> statement: </p> <pre><code>INSERT INTO example.csv VALUES (1, 2, \"hello world\");\n</code></pre> <p>The statement above can be seperated into: <code>INSERT</code>, <code>INTO</code>, <code>example.csv</code>, <code>VALUES</code>, <code>(</code>, <code>1</code>, <code>2</code>, <code>hello world</code>, <code>)</code>.</p> <p>Note that you can define token type by yourself. Depend on your definition, seperated tokens can be different from the example.</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#parser-syntactic-analysis","title":"Parser (Syntactic Analysis)","text":"<p>In the previous Lexer phase, we transformed the raw query string into a sequence of tokens. However, having recognized tokens doesn't necessarily ensure that the sequence is a valid and meaningful query. For example: <code>data.csv SELECT VALUES *</code> is not a valid query.</p> <p>The Parser phase ensures that the sequence of tokens adheres to the grammar rules of our CSV Query Language, constructing a syntactic representation of the query's structure.</p> <p>Your responsibilities:</p> <ol> <li>Token Validation: Check the sequence of tokens to confirm if they adhere to the expected grammar rules. Validate that the tokens form a valid query according to the language's syntax.</li> <li>Detect Invalid Sequences: Detect and report invalid token sequences. If the sequence of tokens doesn't adhere to the language's grammar, generate informative error messages that highlight the issue and potentially its location.</li> <li>Construct Parse Tree: From list of tokens, construct a parse tree that captures the syntactic structure of the query. Each node in the parse tree should correspond to a meaningful grammatical element.</li> </ol> <p>To illustrate a parse tree, let's consider the <code>SELECT</code> below:</p> <pre><code>SELECT id, name, email, phone FROM (\n  SELECT id, name, email, age, phone\n  FROM member.csv\n  WHERE age &gt; 22\n)\nWHERE name LIKE '%Vinh%'\n</code></pre> <p>This query can be parsed into a tree like this:</p> <pre><code>graph TD;\n    SelectCommand--&gt;SELECT;\n    SelectCommand--&gt;ColumnNames;\n    SelectCommand--&gt;FROM;\n    SelectCommand--&gt;DataSource;\n    SelectCommand--&gt;Condition;\n    ColumnNames--&gt;id[ID];\n    ColumnNames--&gt;name[ID];\n    ColumnNames--&gt;email[ID];\n    ColumnNames--&gt;phone[ID];\n\n    DataSource--&gt;subselect[SelectCommand];\n    subselect[SelectCommand]--&gt;subSELECT[SELECT];\n    subselect[SelectCommand]--&gt;subColumnNames[ColumnNames];\n    subselect[SelectCommand]--&gt;subFROM[FROM];\n    subselect[SelectCommand]--&gt;subDataSrc[DataSource];\n    subselect[SelectCommand]--&gt;subCond[Condition];\n    subColumnNames[ColumnNames]--&gt;subColumnNames_id[ID];\n    subColumnNames[ColumnNames]--&gt;subColumnNames_name[ID];\n    subColumnNames[ColumnNames]--&gt;subColumnNames_email[ID];\n    subColumnNames[ColumnNames]--&gt;subColumnNames_phone[ID];\n    subDataSrc[DataSource]--&gt;FILENAME;\n    subCond[Condition]--&gt;subCompareColAge[ID];\n    subCond[Condition]--&gt;subSelectOP[OPERATOR];\n    subCond[Condition]--&gt;subVals[Value];\n    subVals[Value]--&gt;INT;\n\n    Condition--&gt;compareColumn[ID];\n    Condition--&gt;OPERATOR;\n    Condition--&gt;Value;\n    Value--&gt;1st[SINGLE_QUOTE];\n    Value--&gt;STR;\n    Value--&gt;2nd[SINGLE_QUOTE];</code></pre> <p>With nodes that have name in uppercase format are terminal nodes (contain token data), and nodes that have name in Pascal case are non-terminal node (contain multiple terminal nodes).</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#query-command","title":"Query Command","text":"<p>In the context of query command construction, this phase focuses on elevating the query representation to a higher level than the parse tree. While a parse tree can include syntactic elements like brackets, <code>INTO</code>/<code>FROM</code> words, commas, single quotes, and other extraneous details, this phase aims to capture the query's semantics and remove unnecessary syntactic details.</p> <p>In the previous phases (Lexer and Parser), developers were given the flexibility to define their own rules. However, in this phase, to ensure consistency and uniformity in the output, you MUST utilize specific classes for constructing query commands:</p> <pre><code>class Expr {\n    virtual string toString() const = 0;\n};\n\nclass RelationalExpr : public Expr {\npublic:\n    string colName;\n    short int operator;\n    string value;\npublic:\n    string toString() const {\n        // TODO\n    }\n};\n\nclass LogicalExpr : public Expr {\npublic:\n    RelationalExpr* expr1;\n    short int operator;\n    RelationalExpr* expr2;\npublic:\n    string toString() const {\n        // TODO\n    }\n};\n\nclass SortBy {\npublic:\n    string columnName;\n    short int order;\npublic:\n    string toString() const {\n        // TODO\n    }\n};\n\nclass Command {\npublic:\n    virtual string toString() const = 0;\n};\n\nclass InsertCommand : public Command {\npublic:\n    string filename;\n    vector&lt;string&gt; columnNames;\n    vector&lt;string&gt; values;\npublic:\n    string toString() const {\n        // TODO\n    }\n};\n\nclass SelectCommand : public Command {\npublic:\n    vector&lt;string&gt; columnNames;\n    void* dataSource;\n    Expr* expr;\n    SortBy* sort;\n\npublic:\n    string toString() const {\n        // TODO\n    }\n};\n</code></pre> <p>To apply testing, <code>toString</code> functions are used to stringify a query command. You need to write implementations for them to satisfy stringify rules.</p>"},{"location":"challenges/csv-query-tool/timelines/week3_n_week4/#stringify-a-query-command","title":"Stringify a query command","text":"<p>Stringify rules are defined as follows:</p> <ul> <li>Insert command:<ul> <li>In case list of column names is empty     <pre><code>// Attribute values of an insert command\nfilename: \"data.csv\"\ncolumnNames: []\nvalues: [\"1\", \"Hello world\"]\n\n// Will convert to string\n\"[1,hello world]&gt;&gt;data.csv\"\n</code></pre></li> <li>In case list of column names is specified     <pre><code>// Attribute values of an insert command\nfilename: \"data.csv\"\ncolumnNames: [\"name\", \"ID\"]\nvalues: [\"Hello world\", \"1\"]\n\n// Will convert to string\n\"[hello world,ID]&gt;&gt;data.csv[name,ID]\"\n</code></pre></li> </ul> </li> <li>Select command:<ul> <li>In case simple select command:     <pre><code>// Attribute values of an insert command\ndatasource: \"data.csv\"\ncolNames: [\"name\", \"ID\"]\nsortBy:\n    columnName: \"name\"\n    order: 0\n\n// Will convert to string\n\"[name,ID]&lt;&lt;data.csv\"\n</code></pre></li> </ul> </li> </ul>"},{"location":"challenges/csv-query-tool/timelines/week5/","title":"Week 5","text":"<p>This week's challenge: Write a program to validate query commands against a file definition.</p> <p>In previous weeks, we constructed query commands from raw query strings and checked their syntactic rules. However, we did not check whether the query commands are valid for the target CSV file.</p>"},{"location":"challenges/csv-query-tool/timelines/week5/#file-definition","title":"File definition","text":"<p>A CSV file has its definition, which is specified in the first row of the file. The file definition specifies the column names, data types and not N/A constraints. A column definition has syntax like this <code>&lt;col_name&gt;:&lt;datatype&gt;[!]</code>.</p> <ul> <li><code>&lt;col_name&gt;</code> has <code>snake_case</code> format and no whitespaces</li> <li><code>&lt;datatype&gt;</code> can be <code>text</code>, <code>number</code>, <code>bool</code></li> <li><code>!</code> indicates that the column is required (no N/A values on it)</li> </ul> <p>For example, the file definition for the <code>BKStudent.csv</code> file is as follows:</p> stid:number! first_name:text! last_name:text age:number major:text graduated:bool 1 Vinh Nguyen Phuc 22 Computer Science true 2 Quan 20 Computer Engineering false 3 Phu Mechanical Engineering true <p>From this definition, we know the <code>BKStudent.csv</code> file has six columns: <code>stid</code>, <code>first_name</code>, <code>last_name</code>, <code>age</code>, <code>major</code>, and <code>graduated</code>. The <code>stid</code> and <code>first_name</code> columns are required, and the <code>stid</code> column must be a number ...</p> <p>Now we can check a query command is valid or not. For instances:</p> <ul> <li><code>SELECT name FROM BKStudent.csv</code> is not valid because there is no column named <code>name</code>.</li> <li><code>INSERT INTO BKStudent.csv (stid, first_name, last_name) VALUES (\"abcd\", \"John\", \"Doe\")</code> is not valid because <code>stid</code>'s datatype is number.</li> <li><code>INSERT INTO BKStudent.csv (first_name, age) VALUES (\"John\", 22)</code> is not valid because <code>stid</code> is required.</li> <li>...</li> </ul>"},{"location":"challenges/csv-query-tool/timelines/week5/#validate-query-command","title":"Validate query command","text":"<p>To validate a query command against a file's definition, you need to check the following:</p> <ul> <li>Does the target CSV file exist?</li> <li>Does the query command specify all of the required columns?</li> <li>Are the data types of the columns in the query command the same as the data types of the columns in the table definition?</li> <li>Keep in mind that query target can be a subquery.</li> </ul> <p>If any of these checks fail, the query command is invalid. In this case, print an appropriate error message to the console and terminate the program.</p>"},{"location":"challenges/csv-query-tool/timelines/week5/#testing","title":"Testing","text":"<p>Because we haven't a DDL (data definition language) yet, you can create CSV files manually for testing. For example, you could create a file named <code>BKStudentTest.csv</code> with the following contents:</p> stid:number! first_name:text! last_name:text age:number major:text graduated:bool 1 Vinh Nguyen Phuc 22 Computer Science true 2 Quan 20 Computer Engineering false 3 Phu Mechanical Engineering true <p>Test your program with the following queries:</p> <pre><code>-- Invalid query command (column `name` does not exist)\nSELECT name FROM BKStudentTest.csv\n\n-- Invalid query command (stid's data type is number)\nINSERT INTO BKStudentTest.csv (stid, first_name, last_name) VALUES (\"abcd\", \"John\", \"Doe\")\n\n- Invalid query command (stid is required)\nINSERT INTO BKStudentTest.csv (first_name, age) VALUES (\"John\", 22)\n</code></pre> <p>If your program is working correctly, it should print the following error messages for the invalid query commands:</p> <pre><code>$ Column \"name\" does not exist\n$ stid's data type is number\n$ stid is required\n</code></pre> <p>Good luck!</p>"},{"location":"challenges/csv-query-tool/timelines/week5/#data-definition-language-optional","title":"Data Definition Language (Optional)","text":"<p>To create and delete CSV file, we can use SQL-liked DDL. For example, the <code>BKStudent.csv</code> file can be create by this command:</p> <pre><code>CREATE FILE BKStudent.csv (\n    stid NUMBER NOT NULL\n    first_name TEXT NOT NULL\n    last_name TEXT\n    age NUMBER\n    major TEXT\n    graduated BOOLEAN\n);\n</code></pre> <p>To delete <code>BKStudent.csv</code> file, just use:</p> <pre><code>DROP FILE BKStudent.csv;\n</code></pre> <p>This part is not compulsory, so try if you want.</p>"},{"location":"challenges/csv-query-tool/timelines/week6/","title":"Week 6","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/research/","title":"Research","text":""},{"location":"blog/category/engineering/","title":"Engineering","text":""},{"location":"blog/page/2/","title":"Blog","text":""},{"location":"challenges/","title":"TickLab Challenge","text":"<p>\ud83d\udce3 TickLab Challenge l\u00e0 chu\u1ed7i c\u00e1c th\u1eed th\u00e1ch v\u1edbi nhi\u1ec1u ch\u1ee7 \u0111\u1ec1 tr\u00ean c\u00e1c l\u0129nh v\u1ef1c trong ba h\u01b0\u1edbng ph\u00e1t tri\u1ec3n c\u1ee7a TickLab.</p> <p>\ud83d\udce3 TickLab Challenge h\u01b0\u1edbng \u0111\u1ebfn vi\u1ec7c t\u1ea1o c\u01a1 h\u1ed9i ph\u00e1t tri\u1ec3n cho c\u00e1c b\u1ea1n sinh vi\u00ean, gi\u00fap c\u00e1c b\u1ea1n c\u00f3 th\u1ec3 ti\u1ebfp c\u1eadn t\u00ecm hi\u1ec3u c\u00e1c ch\u1ee7 \u0111\u1ec1 m\u1edbi, c\u0169ng nh\u01b0 c\u00f3 c\u01a1 h\u1ed9i th\u1eed th\u00e1ch b\u1ea3n th\u00e2n. Tham gia v\u00e0o c\u00e1c th\u1eed th\u00e1ch c\u00e1c b\u1ea1n s\u1ebd c\u00f3 mentor v\u00e0 c\u00e1c thi\u1ebft b\u1ecb c\u1ea7n thi\u1ebft \u0111\u1ec3 c\u00f3 th\u1ec3 ho\u00e0n th\u00e0nh \u0111\u01b0\u1ee3c th\u1eed th\u00e1ch.</p>"},{"location":"challenges/#cac-chu-e","title":"C\u00e1c ch\u1ee7 \u0111\u1ec1","text":""},{"location":"challenges/#build-from-scratch","title":"Build from scratch","text":"<ul> <li>JSON in C++</li> </ul>"},{"location":"challenges/#c","title":"C++","text":"<ul> <li>JSON in C++</li> <li>Building the exql program</li> </ul>"},{"location":"challenges/#json","title":"JSON","text":"<ul> <li>JSON in C++</li> </ul>"},{"location":"challenges/#mobile","title":"Mobile","text":"<ul> <li>Personal Finance App</li> </ul>"},{"location":"challenges/#principle-of-programming-languages-ppl","title":"Principle of programming languages (PPL)","text":"<ul> <li>Building the exql program</li> </ul>"},{"location":"challenges/#sql","title":"SQL","text":"<ul> <li>Personal Finance App</li> <li>Building the exql program</li> </ul>"}]}