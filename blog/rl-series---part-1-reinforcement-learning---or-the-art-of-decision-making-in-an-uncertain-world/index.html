
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Summary of knowledge-sharing content during the research and project development process across departments at TickLab.">
      
      
        <meta name="author" content="TickLab Viet Nam">
      
      
        <link rel="canonical" href="https://ticklabvn.github.io/knowledge/blog/rl-series---part-1-reinforcement-learning---or-the-art-of-decision-making-in-an-uncertain-world/">
      
      
        <link rel="prev" href="../colmap-a-software-for-3d-spare--dense-reconstruction/">
      
      
      
      <link rel="icon" href="../../assets/TickLab-logo.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.44">
    
    
      
        <title>[RL series - Part 1] Reinforcement Learning - or The Art of Decision Making in an Uncertain World - TickLab - Start here, go anywhere</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.0253249f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
<link rel="stylesheet" href="../../assets/stylesheets/custom.00c04c01.min.css">

  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#rl-series-part-1-reinforcement-learning-or-the-art-of-decision-making-in-an-uncertain-world" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="TickLab - Start here, go anywhere" class="md-header__button md-logo" aria-label="TickLab - Start here, go anywhere" data-md-component="logo">
      
  <img src="../../assets/TickLab-logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            TickLab - Start here, go anywhere
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              [RL series - Part 1] Reinforcement Learning - or The Art of Decision Making in an Uncertain World
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="teal" data-md-color-accent="teal"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/TickLabVN/knowledge" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    TickLabVN/knowledge
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
    
  
  Blog

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../challenges/" class="md-tabs__link">
          
  
    
  
  Challenges

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="TickLab - Start here, go anywhere" class="md-nav__button md-logo" aria-label="TickLab - Start here, go anywhere" data-md-component="logo">
      
  <img src="../../assets/TickLab-logo.svg" alt="logo">

    </a>
    TickLab - Start here, go anywhere
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/TickLabVN/knowledge" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    TickLabVN/knowledge
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Blog
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Blog
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../archive/2024/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../archive/2023/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2023
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Categories
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../category/engineering/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Engineering
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../category/research/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../challenges/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    Challenges
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Challenges
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Engineering (CS)
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            Engineering (CS)
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_1" >
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../challenges/csv-query-tool/" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    CSV Query Tool
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3_2_1" id="__nav_3_2_1_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_3_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_1">
            <span class="md-nav__icon md-icon"></span>
            CSV Query Tool
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/csv-query-tool/language-description/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Language description
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2_1_3" >
        
          
          <label class="md-nav__link" for="__nav_3_2_1_3" id="__nav_3_2_1_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Timelines
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="4" aria-labelledby="__nav_3_2_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2_1_3">
            <span class="md-nav__icon md-icon"></span>
            Timelines
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/csv-query-tool/timelines/week1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/csv-query-tool/timelines/week2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/csv-query-tool/timelines/week3_n_week4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 3 &amp; Week 4
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/csv-query-tool/timelines/week5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 5
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/csv-query-tool/timelines/week6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Week 6
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/personal-finance-app/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Personal Finance App
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../challenges/json-in-cpp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    JSON in C++
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
    <a href="#section-1-theory" class="md-nav__link">
      <span class="md-ellipsis">
        Section 1: Theory
      </span>
    </a>
  
    <!-- Table of contents list -->
    
      <nav class="md-nav" aria-label="Section 1: Theory">
        <ul class="md-nav__list">
          
          
            <li class="md-nav__item">
    <a href="#1-introduction" class="md-nav__link">
      <span class="md-ellipsis">
        1. Introduction
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#2-key-elements-of-reinforcement-learning" class="md-nav__link">
      <span class="md-ellipsis">
        2. Key Elements of Reinforcement Learning
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#3-dynamic-programming" class="md-nav__link">
      <span class="md-ellipsis">
        3. Dynamic Programming
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#4-monte-carlo-methods" class="md-nav__link">
      <span class="md-ellipsis">
        4. Monte Carlo Methods
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#5-temporal-difference-learning" class="md-nav__link">
      <span class="md-ellipsis">
        5. Temporal Difference Learning
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#6-n-step-bootstrapping" class="md-nav__link">
      <span class="md-ellipsis">
        6. n-step Bootstrapping
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#7-problems-and-extension-regarding-tabular-methods" class="md-nav__link">
      <span class="md-ellipsis">
        7. Problems and Extension regarding Tabular Methods
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
        </ul>
      </nav>
    
  </li>
      
        <li class="md-nav__item">
    <a href="#section-2-case-study" class="md-nav__link">
      <span class="md-ellipsis">
        Section 2: Case Study
      </span>
    </a>
  
    <!-- Table of contents list -->
    
      <nav class="md-nav" aria-label="Section 2: Case Study">
        <ul class="md-nav__list">
          
          
            <li class="md-nav__item">
    <a href="#1-sarsa-vs-q-learning-in-cliff-walking" class="md-nav__link">
      <span class="md-ellipsis">
        1. Sarsa vs Q-learning in Cliff Walking
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#2-value-iteration-vs-policy-iteration" class="md-nav__link">
      <span class="md-ellipsis">
        2. Value Iteration vs Policy Iteration
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#3-how-many-steps-are-optimal-for-n-step-bootstrapping-algorithms" class="md-nav__link">
      <span class="md-ellipsis">
        3. How many steps are optimal for n-step bootstrapping algorithms?
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#4-the-best-algorithm-for-each-environment-not-include-n-step-bootstrapping-algorithms" class="md-nav__link">
      <span class="md-ellipsis">
        4. The best algorithm for each environment (not include n-step bootstrapping algorithms)
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
          
            <li class="md-nav__item">
    <a href="#5-the-strange-case-of-monte-carlos-gamma-fine-tuning" class="md-nav__link">
      <span class="md-ellipsis">
        5. The strange case of Monte Carlo’s gamma fine-tuning
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
          
          
        </ul>
      </nav>
    
  </li>
      
        <li class="md-nav__item">
    <a href="#conclusion" class="md-nav__link">
      <span class="md-ellipsis">
        Conclusion
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
      
        <li class="md-nav__item">
    <a href="#references" class="md-nav__link">
      <span class="md-ellipsis">
        References
      </span>
    </a>
  
    <!-- Table of contents list -->
    
  </li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
<div class="md-content md-content--post" data-md-component="content">

    <!-- Sidebar -->
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
        <div class="md-sidebar__scrollwrap">
            <div class="md-sidebar__inner md-post">
                <nav class="md-nav md-nav--primary">

                    <!-- Back to overview link -->
                    <div class="md-post__back">
                        <div class="md-nav__title md-nav__container">
                            <a href="../" class=" md-nav__link">
                                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                                <span class="md-ellipsis">
                                    Back to index
                                </span>
                            </a>
                        </div>
                    </div>

                    <!-- Post authors -->
                    
                    <div class="md-post__authors md-typeset">
                        
                        <div class="md-profile md-post__profile">
                            <span class="md-author md-author--long">
                                <a href="https://github.com/baotram153">
                                    <img src="https://avatars.githubusercontent.com/u/56596545?v=4" alt="Đặng Ngọc Bảo Trâm" />
                                    </a>
                            </span>
                            <span class="md-profile__description">
                                <strong>
                                    <a href="https://github.com/baotram153">
                                        Đặng Ngọc Bảo Trâm
                                        </a>
                                </strong><br />
                                Scientific researcher
                            </span>
                        </div>
                        
                    </div>
                    

                    <!-- Post metadata -->
                    <ul class="md-post__meta md-nav__list">
                        <li class="md-nav__item md-nav__item--section">
                            <div class="md-post__title">
                                <span class="md-ellipsis">
                                    Metadata
                                </span>
                            </div>
                            <nav class="md-nav">
                                <ul class="md-nav__list">

                                    <!-- Post date -->
                                    <li class="md-nav__item">
                                        <div class="md-nav__link">
                                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                                            <time datetime="2024-11-06 00:00:00" class="md-ellipsis">November 6, 2024</time>
                                        </div>
                                    </li>

                                    <!-- Post date updated -->
                                    

                                    <!-- Post categories -->
                                    
                                    <li class="md-nav__item">
                                        <div class="md-nav__link">
                                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                                            <span class="md-ellipsis">
                                                in
                                                
                                                <a href="../category/research/">Research</a></span>
                                        </div>
                                    </li>
                                    

                                    <!-- Post readtime -->
                                    
                                    
                                    <li class="md-nav__item">
                                        <div class="md-nav__link">
                                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                                            <span class="md-ellipsis">
                                                
                                                42 min read
                                                
                                            </span>
                                        </div>
                                    </li>
                                    
                                </ul>
                            </nav>
                        </li>
                    </ul>
                </nav>

                <!-- Table of contents, if integrated -->
                
            </div>
        </div>
    </div>

    <!-- Page content -->
    <article class="md-content__inner md-typeset">
        
        

  
  


<h1 id="rl-series-part-1-reinforcement-learning-or-the-art-of-decision-making-in-an-uncertain-world">[RL series - Part 1] Reinforcement Learning - or The Art of Decision Making in an Uncertain World<a class="headerlink" href="#rl-series-part-1-reinforcement-learning-or-the-art-of-decision-making-in-an-uncertain-world" title="Permanent link">&para;</a></h1>
<style>
.md-typeset h4 {
    font-weight: 400;
    letter-spacing: -.01em;
    font-size: 1.25em;
    line-height: 1.5;
    margin: 1.6em 0 .8em
}
.md-typeset h3 {
    font-weight: 500;
    letter-spacing: -0.02em;
    font-size: 1.5em;
    line-height: 1.6;
    margin: 1.8em 0 0.9em;
}
.md-typeset h2 {
    font-weight: 600;
    letter-spacing: -0.03em;
    font-size: 1.75em;
    line-height: 1.7;
    margin: 2em 0 1em;
}
</style>

<figure>
<p><img alt="rl-intro" src="../research/rl-series-part-1/images/RECRUITMENT.png" width="300" /></p>
</figure>
<p>This article serves as a guide to help you enter the realm of Reinforcement Learning. The concepts in the "Theory" section are presented in an intuitive way, so you can build a solid understanding without unnecessary struggle. If you're more of a hands-on person, you can check out the notebook in the "Case Study" section, which experiments on the algorithms that we'll mention.</p>
<!-- more -->

<h2 id="section-1-theory">Section 1: Theory<a class="headerlink" href="#section-1-theory" title="Permanent link">&para;</a></h2>
<h3 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h3>
<h4 id="11-inspiration">1.1 Inspiration<a class="headerlink" href="#11-inspiration" title="Permanent link">&para;</a></h4>
<p>Let’s start with a fascinating question: How do we learn? From the first breadth you tooth until this very moment when you’re sitting here reading this article is a never-ending process of learning. So how exactly do you do this? How do you know that in front of you is a “dog” and not a “cat” or a “dinosaur”? Or how do you know that in order to walk properly, you have to first shift your weight onto one foot, lift the other foot, move it forward, place it onto the ground, and then repeat with the others? The answer is through multiple trial-and-errors and feedbacks of others and of the environment. Remember the moment you mistakenly called a dog a cat? If you were with your parents, they would kindly correct you, pointing out that it's actually a dog. However, if it happened in front of your friends, they might laugh hard at you, leaving you puzzled. These reactions serve as valuable feedback, helping you learn the difference between a dog and a cat for next time (unless you want to risk another round of laughter!). The same goes with learning how to walk, you may have experimented with tilting your body in the wrong direction or not alternating your feet properly to end up finding out that these approaches were not really successful, so you stopped doing that and tried some other techniques to the point when you fell comfortable. Reinforcement Learning algorithms try to do just that: an agent explores different actions, evaluates their outcomes, and adjusts its strategies to maximize rewards and minimize penalties in future interactions. Besides Deep Learning, which seeks to mimic the intricacies of human brain functions such as neural transmission and pattern recognition, Reinforcement Learning is a stepping stone in the effort of imitating human in terms of learning and making decisions.</p>
<h4 id="12-what-is-reinforcement-learning">1.2 What is Reinforcement Learning?<a class="headerlink" href="#12-what-is-reinforcement-learning" title="Permanent link">&para;</a></h4>
<p>It’s a type of Machine Learning, where an agent learn to make decision so as to maximize the accumulative reward. To do this, the agent has to interact with the environment, and adjust its actions base on feedbacks in terms of rewards and penalties.</p>
<h4 id="13-why-do-we-need-reinforcement-learning">1.3 Why do we need Reinforcement Learning?<a class="headerlink" href="#13-why-do-we-need-reinforcement-learning" title="Permanent link">&para;</a></h4>
<p>There appears to be a category of problems where the labels for our data is not available. Although most of the time we can do the labeling on our own, or with the help of an expert (e.g: segmenting the “tumor” region and “non-tumor” region in a medical image, or classifying images with respect to different people in a face-recognition task,…), but sometimes there are problems where the correct labeling is simply unattainable. Haven’t heard of those kinds of problem before?</p>
<ol>
<li>Imagine your task is to find the fastest way out of a maze, from the starting state, should you take the left move, which will take you closer to the exit, or the right move, which will take you further away, in either move, you’ll face a possibility of being stuck in a death end.</li>
<li>Imagine that your task is to maximize your happiness. You have an exam tomorrow. Should you start studying now or watch another episode of “The Boys”?</li>
</ol>
<p>In either circumstances, the goal you’re trying to reach is not the short-term reward (moving closer to the exit, or a little extra satisfaction from watching another episode of your favorite series), but rather a long-term reward (actually find an exit, or maximize your happiness in the long run). Remember that these 2 are not the same, and sometimes, if not most of the time, you have to sacrifice one to achieve another (choosing a longer path with the goal of actually reaching the exit, not just moving closer to it, or sacrifice your instant gratification for a more sustainable fulfillment of academic success). This highlights the first property of Reinforcement Learning, which is <strong><em>delayed reward</em></strong> (the “best” action is defined to be the action that produce the biggest long-term reward, meaning the reward accumulated through time, not the reward received immediately). But the story doesn’t end here, how do we know, from our first examples, that the longer but not shorter path with eventually leads you to the exit, or you’ll actually earn a good grade after a whole night studying for the exam. The answer is, you have to discover it yourself! Maybe you’ll find out that going right is, indeed, the action that take you to the death end, or maybe you’ll end up flunking the exam even though you’ve been cramming for it the whole night before, and realize that binge watching your favorite TV series would be, sadly, be a better choice. Interesting? We face those decisions our whole life, the only thing we could do is try one, observe the result, and change our worldview and action according to that experience in case we’ll be in that same circumstance in the future. This pinpoint the second property of Reinforcement Learning, which is <strong><em>trial and error</em></strong> (discover the “label” on our own to maximize the objective function, which is the accumulative reward).</p>
<h4 id="14-and-i-i-took-the-road-less-taken-by">1.4 “… And I — I took the road less taken by…”<a class="headerlink" href="#14-and-i-i-took-the-road-less-taken-by" title="Permanent link">&para;</a></h4>
<p>Are those examples enough whet your appetite? This article includes all the key points we’ve gathered after reading Part I of <a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank">this book</a>, plus our own intuition and understanding about different concepts presented. We hope that this will pave a smoother path for you on your journey into the realm of Reinforcement Learning.</p>
<h3 id="2-key-elements-of-reinforcement-learning">2. Key Elements of Reinforcement Learning<a class="headerlink" href="#2-key-elements-of-reinforcement-learning" title="Permanent link">&para;</a></h3>
<p>We’ll start off learning some fundamental concepts that’ll be crucial for you to take a grasp of what the people in this area is talking about.</p>
<h4 id="1-episodic-task-and-continuing-task">1. Episodic task and Continuing task<a class="headerlink" href="#1-episodic-task-and-continuing-task" title="Permanent link">&para;</a></h4>
<p>Episodic task is a task where the agent-environment interaction breaks naturally into subsequences. In episodic tasks, there are starting states (<span class="arithmatex">\(S_0\)</span>), and terminal states (<span class="arithmatex">\(S_T\)</span>- where <span class="arithmatex">\(T\)</span> is the terminal timestep), between which is a sequence of state-action transitions. Each sequence of timesteps from a particular starting state to a particular terminal state is called an episode. It’s pretty much like video games, where the starting state is the initial position of your character, with zero point, and the terminal state is when you lose, which also denote the end of an episode, in which case you have to start the game again with the same position and zero point. In an episodic task, the succeeding states must not, by any mean, be affected by the preceding states.</p>
<p>Continuing task is a task where <span class="arithmatex">\(T \rightarrow \infty\)</span>. It’s a task that lasts for the agent’s lifetime (you can see the definition of “agent” below).</p>
<h4 id="2-policy-pi">2. Policy (<span class="arithmatex">\(\pi\)</span>)<a class="headerlink" href="#2-policy-pi" title="Permanent link">&para;</a></h4>
<p>A policy is a mapping from situations (or states, which is more widely use in Reinforcement Learning literatures) to actions. A policy can either be deterministic, meaning that it only return one action for each state, or stochastic, meaning that it will return an action sampled from a probability distribution for each state.</p>
<h4 id="3-reward-r">3. Reward (<span class="arithmatex">\(R\)</span>)<a class="headerlink" href="#3-reward-r" title="Permanent link">&para;</a></h4>
<p>The reward denotes whether a state, or an action in a particular state, is good or bad.</p>
<h4 id="4-return-g">4. Return (<span class="arithmatex">\(G\)</span>)<a class="headerlink" href="#4-return-g" title="Permanent link">&para;</a></h4>
<p>It’s the accumulated reward that you’ve collected along the way from the starting state to the terminal state (in episodic tasks) or for the agent’s lifetime (in continuing tasks).</p>
<p>So how do we formulate this <span class="arithmatex">\(G_t\)</span> for a specific timestep? Our first idea would be to sum all the rewards:</p>
<div class="arithmatex">\[
G_t = R_{t+1} + R_{t+2} + R_{t+3} + ··· + R_T
\]</div>
<p>This works just fine in simple cases where <span class="arithmatex">\(T\)</span> is small, but if <span class="arithmatex">\(T\)</span> is large or approach <span class="arithmatex">\(\infty\)</span>, this definition of return would be problematic, since <span class="arithmatex">\(G_t\)</span> will either be really big or approaches infinity. To deal with this, we use a discounting factor (<span class="arithmatex">\(0 \leq \lambda \leq 1\)</span>) and reformulate <span class="arithmatex">\(G_t\)</span> as:</p>
<div class="arithmatex">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\]</div>
<p>Phewww! Now <span class="arithmatex">\(G_t\)</span> can converged no matter how large <span class="arithmatex">\(T\)</span> is. Having reach to this point, I think we should have a deeper discussion about the discount factor <span class="arithmatex">\(\lambda\)</span>. This variable denotes how much you want to emphasize the importance of future reward. The implementation of <span class="arithmatex">\(\lambda\)</span> into this formula gives a beautiful effect in which rewards from states encountered are gradually fading away, and this fading manifest more clearly in states that are more further away in the future. In extreme case where <span class="arithmatex">\(\lambda = 0\)</span>, we only care about immediate reward (<span class="arithmatex">\(G_t = R_t\)</span>); and where <span class="arithmatex">\(\lambda = 1\)</span>, we consider rewards received in every state the same degree of importance.</p>
<p>Formulate <span class="arithmatex">\(G_t\)</span> this way also allows us to rewrite the formula in a recursive way:</p>
<div class="arithmatex">\[
G_t = R_{t+1} + \gamma G_{t+1}
\]</div>
<h4 id="5-value-function">5. Value function<a class="headerlink" href="#5-value-function" title="Permanent link">&para;</a></h4>
<p>Remember when we say that the goal of Reinforcement Learning is to maximize the long-term reward? But what is the long-term reward anyway? We use the term “value function” to denote this. “Wait a minute”, you may say, “How about the Return? Isn’t that definition of Return perfectly fit our definition of long-term reward?” Yeah you’re right, in a sense. Let’s make this clear.</p>
<p>The “value function” is the mapping from a state to its value. The “value” is defined as the <em>expected</em> return, which has something to do with the stochastic of the environment dynamics and the actions being taken. Continue with our previous example, what do you think the long-term reward of studying for the exam be? The answer is, it <em>depends</em>. Suppose that you decide to study for the exam, this can cause different scenarios to happen, you could finish studying in a short amount of time and go to bed early, or you could end up being distracted and have to stay up late. Suppose you end up in the second scenario, the next morning, you have to choose either to wake up for exam, or sleep in for an extra 10 minutes, which in turn, can lead you to a situation of you being late for the exam… You see, the future is full of uncertainties, so the long-term reward should be computed carefully, after considering <em>all possibilities</em> of what can happen and what action you can take, not just <em>one sample</em> of those. Hence for the evaluation of long-term reward, we should aim at the <em>expected return</em> instead of the <em>return</em> itself.</p>
<h4 id="6-agent">6. Agent<a class="headerlink" href="#6-agent" title="Permanent link">&para;</a></h4>
<p>The agent is our decision maker, it interacts with the environment to receive reward, calculate the value of that state, change the value function, and change the policy if necessary and make decision according to that policy.</p>
<h4 id="7-environment-and-model">7. Environment and Model<a class="headerlink" href="#7-environment-and-model" title="Permanent link">&para;</a></h4>
<p>The environment gives the agent the reward signals. A model is the simulation of the environment, meaning that it’s built based on our data distribution collected from the environment, and is used to inferred about how the environment may behave. A model allows the agent to learn without interacting with the environment, which might be costly. Much like training to fly a helicopter in MSFS without breaking the bank!</p>
<h4 id="8-markov-decision-process">8. Markov Decision Process<a class="headerlink" href="#8-markov-decision-process" title="Permanent link">&para;</a></h4>
<figure>
<p><img alt="Figure 1: The interaction between agent and environment in a MDP" src="../research/rl-series-part-1/images/Untitled.png" width="700" /></p>
<figcaption>
<p>Figure 1: The interaction between agent and environment in a MDP
  </p>
</figcaption>
</figure>
<p>A Markov Decision Process (MDP) is a model that describe the interaction between the agent and the environment, throughout the sequence of decision-makings. In a MDP, at timestep <span class="arithmatex">\(t\)</span>, the agent is in state <span class="arithmatex">\(S_t\)</span> and receive a reward <span class="arithmatex">\(R_t\)</span>, the agent takes an action <span class="arithmatex">\(A_t\)</span>, to which the environment returns with the next state <span class="arithmatex">\(S_{t+1}\)</span> and the reward from that next state <span class="arithmatex">\(R_{t+1}\)</span>.</p>
<p>The most important property of a MDP is that the next state and reward only depend on the current state and action, not the history of states and actions. Given the previous state and action, the probability distribution of two random variable <span class="arithmatex">\(R_t\)</span> and <span class="arithmatex">\(S_t\)</span> is denoted by <span class="arithmatex">\(p\)</span>:</p>
<div class="arithmatex">\[
p(s', r|s,a) \doteq Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\}
\]</div>
<p><span class="arithmatex">\(p\)</span> is often called the dynamic of the environment. Notice that the sum of probabilities of a particular pair of next state - reward over all next states and rewards possible from a particular action in a given current state, is <span class="arithmatex">\(1\)</span>:</p>
<div class="arithmatex">\[
\sum_{s' \in S}\sum_{r \in R} p(s',r|s,a) = 1, \forall s \in S, a \in A(s)
\]</div>
<p>At this point you may ask, why we have to take the sum over all <span class="arithmatex">\(r\)</span>? Doesn’t it mean that the reward can varies even in the same state? The answer is yes, every state may has its own probability distribution of reward, although this isn’t the case most of the time.</p>
<h4 id="9-the-bellman-equation">9. The Bellman equation<a class="headerlink" href="#9-the-bellman-equation" title="Permanent link">&para;</a></h4>
<p>First of all we rewrite our previous definition of the state value function in form of a mathematical formula:</p>
<div class="arithmatex">\[
v_\pi(s) = E[G_t | S_t = s], \forall s \in S \qquad\qquad (1)
\]</div>
<p>I’m supposed to give you the Bellman equation for the state value now. But in an attempt of a clear explanation, I found it easier to introduce you to the Bellman equation for the action value function first. Similar to the definition of the value of a state, the value of a state-action pair <span class="arithmatex">\((s,a)\)</span> is the expected return the agent will get when it’s in state <span class="arithmatex">\(s\)</span> and take action <span class="arithmatex">\(a\)</span>:</p>
<div class="arithmatex">\[
q_\pi(s,a) = E[G_t|S_t = s, A_t = a]
\]</div>
<p>With some substitution and transformation, we can rewrite the equation as:</p>
<div class="arithmatex">\[
\begin{align*} q_\pi(s,a) &amp; = E [\textcolor{red} {R_{t+1} + \gamma G_{t+1}} | S_t = s, A_t = a] \\
&amp; = \sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma \mathbb E[G_{t+1}|S_{t+1}=s']\right) \\
&amp; =\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma \textcolor{orange}{v_\pi(s')}\right)\qquad\qquad(2) \\
\end{align*}
\]</div>
<p>The first line is obtained by substituting <span class="arithmatex">\(G_t\)</span> with its recursive form - <span class="arithmatex">\(R_{t+1} + \gamma G_{t+1}\)</span>. Now to find the expectation of the red term, you have to consider all possibilities of the next state and next reward, given the current state and action. To do this, we take the probability of reaching the next state <span class="arithmatex">\(s’\)</span> and receive the reward <span class="arithmatex">\(r\)</span>, multiply with its corresponding value of the red term, then sum over all possibilities of next states and rewards from our current state-action pair. You should notice that even after the next state is given, the return of the next state <span class="arithmatex">\(G_{t+1}\)</span> isn’t deterministic yet, because it still depends on the next action taken the the environment dynamic, hence we still have to take the expectation of that random variable. Now according to <span class="arithmatex">\((1)\)</span>, we substitute the expectation in the second line with the value of the next state to reach the third line.</p>
<p>Now that we know that the formula for <span class="arithmatex">\(q_\pi(s,a)\)</span> can be derived as <span class="arithmatex">\((2)\)</span>. How about rewriting the value function to obtain the same structure. The value of a state is simply the same as the value of a state-action pair, but the action here is not determined yet. Hence the value of a state equals to the value of a state-action pair, weighted by the probability of taking that action according to the policy, and then sum over all possible actions:</p>
<div class="arithmatex">\[
\begin{align*} v_\pi(s) &amp; = \sum_{a \in A}\pi(a|s) q_\pi(s,a) \qquad\qquad (3)\\
&amp; = \sum_{a \in A}\pi(a|s)\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)[r + \gamma v_\pi(s')], \quad \forall s \in S \end{align*}
\]</div>
<p>Wow! We obtain the formula for <span class="arithmatex">\(v_\pi(s)\)</span> in such a beautiful recursive form. This is the <strong><em>Bellman Equation of the state value function</em></strong>.</p>
<p>So what about the <strong><em>Bellman equation of the action value function</em></strong>? Let’s return back to our equation <span class="arithmatex">\((2)\)</span>, this isn’t in recursive form yet. But we just have to do one more step: plug the formula at <span class="arithmatex">\((3)\)</span> into the orange term in <span class="arithmatex">\((2)\)</span>, and…voila!</p>
<div class="arithmatex">\[
\begin{align*}
q_{\pi}(s,a) &amp; =\sum_{s' \in S}\sum_{r \in R}p(s',r|s,a)\left(r + \gamma \textcolor{orange}{\sum_{a \in A}\pi(a'|s') q_\pi(s',a')}\right)\\
\end{align*}
\]</div>
<h4 id="10-optimal-policy-and-optimal-value-function">10. Optimal policy and optimal value function<a class="headerlink" href="#10-optimal-policy-and-optimal-value-function" title="Permanent link">&para;</a></h4>
<p>The optimal policy is the policy that for every state, the value of that state is higher than those yielded by other policy:</p>
<div class="arithmatex">\[
v_*(s) = \max_\pi v_\pi(s) ,\quad \forall s \in S
\]</div>
<p>The optimal value function is the value function under the optimal policy, which is <span class="arithmatex">\(v_*(s)\)</span>. Intuitively, we know the value of a state under the optimal policy must equal the expected return for the best action taken from that state, hence we can write the Bellman Equation for the optimal state value function as following:</p>
<div class="arithmatex">\[
\begin{align*}  v_*(s) &amp; = \max_{a \in A} q_{\pi_*}(s,a) \\ &amp; = \max_a \mathbb E_{\pi_*}[R_{t+1} + \gamma G_{t+1}|S_t = s, A_t = a] \\  &amp; = \max_a\sum_{s',r}p(s',r|s,a)(r + \gamma E_{\pi_*}[G_{t+1}| S_{t+1} = s']) \\    &amp; = \max_a\sum_{s',r}p(s',r|s,a)(r + \gamma v_*(s')) \\ \end{align*}
\]</div>
<p>The deriving steps are just the same as those from the formulation of the Bellman Equation for state value function above.</p>
<h3 id="3-dynamic-programming">3. Dynamic Programming<a class="headerlink" href="#3-dynamic-programming" title="Permanent link">&para;</a></h3>
<p>Dynamic Programming (DP) is not a particular algorithm, but a collection of algorithms that computes the optimal policy after being given a perfect model of the environment as a Markov Decision Process (meaning that <span class="arithmatex">\(p(s’,r | s,a)\)</span>, or the environment dynamic, is known for all tuples <span class="arithmatex">\((s,a,s’,r)\)</span>). One example of such environment is the simulation environment where the agent is a robot navigating in a grid world. In such environment, the dynamic is completely known in the sense that if you control the robot to go to the left, it will not slip to the opposite direction. It’s useful to understand that such environment can still be a stochastic environment, such as a similar environment to the above but this time, when you control the robot to go to the left, there’s 70% of its going to the left and the remaining 30% to the other direction. The universal idea of DP algorithms is that they are obtained by turning Bellman equation into assignments, that is into update rules to improve the approximation of the value functions.</p>
<h4 id="31-policy-evaluation">3.1 Policy Evaluation<a class="headerlink" href="#31-policy-evaluation" title="Permanent link">&para;</a></h4>
<p>Policy Evaluation is the process of computing the value function <span class="arithmatex">\(v_\pi(s),s \in \mathcal S\)</span> for an arbitrary policy <span class="arithmatex">\(\pi\)</span>.</p>
<p>To achieve the value function, our first approach is to write the Bellman equation for every state <span class="arithmatex">\(s \in \mathcal S\)</span> (<span class="arithmatex">\(\mathcal S\)</span> is the state space, by the way). By doing this obtain a system of <span class="arithmatex">\(|\mathcal S|\)</span> linear equations in <span class="arithmatex">\(|\mathcal S|\)</span> unknowns, each unknown is the value of a state - <span class="arithmatex">\(v_\pi(s), s \in \mathcal S\)</span>. However, solving for <span class="arithmatex">\(v_\pi(s)\)</span> for each <span class="arithmatex">\(s \in \mathcal S\)</span> is a difficult task (even impossible if <span class="arithmatex">\(\mathcal S\)</span> is large).</p>
<p>The second approach is not to obtain the true value function right away, but to update the value function iteratively:</p>
<div class="arithmatex">\[
v_{k+1}(s) = \sum_{a \in A}\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r + \gamma v_k(s')\right]
\]</div>
<p>The value of state <span class="arithmatex">\(s\)</span> at <span class="arithmatex">\((k+1)^{th}\)</span> iteration is estimated by the values of its succeeding states at <span class="arithmatex">\(k^{th}\)</span> iteration. Each iteration includes an update of every state in the state space. And when <span class="arithmatex">\(k \rightarrow \infty\)</span>, the approximated value of each state approaches that state’s true value. Here is an algorithm for this approach:</p>
<figure>
<p><img alt="Untitled" src="../research/rl-series-part-1/images/Untitled1.png" width="700" /></p>
</figure>
<p>The assignment made in the red box is exactly the same as our above update formula. The inequality in the orange box is the stopping condition, which is when the absolute error of the old and new value of a state, maximum over all states, is below a specific threshold. Notice that the value of each state in this algorithm is updated “inline”, which means for each sweep through the state space, we use the immediately new value of preceding states as the update value of the succeeding states. This method brings about faster learning, and less memory requirement to store the old state values.</p>
<h4 id="32-policy-improvement">3.2 Policy Improvement<a class="headerlink" href="#32-policy-improvement" title="Permanent link">&para;</a></h4>
<p>We are able to approach the true value function for an arbitrary policy, but our goal doesn’t just end there. We need to find a better policy based on the values computed. The way we’re going to achieve this is pretty straightforward - for each state, we change the current action to the action that produces the best action value:</p>
<div class="arithmatex">\[
\begin{align*} \pi'(s) &amp; = \arg \max_a q_\pi(s,a) \\ &amp; = \arg \max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')] \qquad (4) \end{align*}
\]</div>
<p>According to <span class="arithmatex">\((4)\)</span>, we have the value function of the new policy <span class="arithmatex">\(\pi’\)</span> as follow:</p>
<div class="arithmatex">\[
v_{\pi'}(s) = \max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')] \qquad (5)
\]</div>
<p>If the new policy happens to be as good as, but no better than the old policy (<span class="arithmatex">\(v_{\pi'}(s) = v_\pi(s)\)</span>), then <span class="arithmatex">\((5)\)</span> will turn into the Bellman optimality equation:</p>
<div class="arithmatex">\[
v_{\pi'}(s) = \max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_{\pi'}(s')]
\]</div>
<p>The fact that this is the Bellman optimality equation means that our strategy for improving the policy actually works. Policy improvement <em>always</em> gives us a strictly better policy than the one before, except when our original policy is already optimal. In this case <span class="arithmatex">\(\pi’(s)\)</span> and <span class="arithmatex">\(\pi(s)\)</span> are both optimal policies.</p>
<h4 id="33-policy-iteration">3.3 Policy Iteration<a class="headerlink" href="#33-policy-iteration" title="Permanent link">&para;</a></h4>
<p>Policy Iteration is the combination of Policy Evaluation and Policy Improvement, in which we calculate the value function given the current policy, and then change our policy greedily with respect to the calculated value function, repeatedly until our policy and value function is (approximately) stable.</p>
<p>One can also imagine the Policy Iteration process as an effort to simultaneously satisfy two goals: obtaining a value function consistent with a given policy and obtaining a better (more greedy) policy. To try to reach one goal means to diverge from the other (by approaching the true value function of a policy, we find that our policy is not as greedy as we think, and by changing our policy to a better one, the value function calculated doesn’t remain true anymore). But by continuously repeating these steps, we will eventually converge to the point of the optimal policy and optimal value function.</p>
<figure>
<p><img _="&quot;" alt="Figure 2: Diagram of Policy Iteration" src="../research/rl-series-part-1/images/Untitled2.png" width="500" /></p>
<figcaption>
<p>Figure 2: Diagram of Policy Iteration
  </p>
</figcaption>
</figure>
<h4 id="34-value-iteration">3.4 Value Iteration<a class="headerlink" href="#34-value-iteration" title="Permanent link">&para;</a></h4>
<p>One question arises: If Policy Evaluation is done iteratively, then convergence exactly to <span class="arithmatex">\(v_\pi\)</span> occurs only in the limit (we can never find the <em>true</em> value function by iterative method). Must we wait for the exact convergence, or can we stop short of that?</p>
<p>The Value Iteration algorithm stems from the idea that we should truncate the Policy Evaluation process, which will speed up the learning process, but still guarantees the convergence to the optimal value function:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled3.png" width="700" /></p>
</figure>
<p>Notice that the formula for updating <span class="arithmatex">\(v_\pi(s)\)</span> is now a combination of Policy Improvement (it is now computed as a maximized value, not an expected value) and truncated Policy Evaluation (<span class="arithmatex">\(v_\pi(s)\)</span> for every state is now calculated once for every “updated” policy, not repeatedly). One can also think of this updating formula as an application of the Bellman optimality equation. By updating <span class="arithmatex">\(v_\pi(s)\)</span> this way, we expect that <span class="arithmatex">\(v_\pi(s)\)</span> will eventually approach <span class="arithmatex">\(v_*(s)\)</span>, and the optimal policy is obtained simply by choosing the best action in each state according to the optimal value function.</p>
<h3 id="4-monte-carlo-methods">4. Monte Carlo Methods<a class="headerlink" href="#4-monte-carlo-methods" title="Permanent link">&para;</a></h3>
<p><em>“if you’re going to try, go all the
way.
otherwise, don’t even start…”</em></p>
<p><em>— “Roll the dice” - Charles Bukowski</em></p>
<p>Dynamic Programming algorithms requires the environment dynamic is completely no, which obviously not be the case for a lot of problems. Imagine the game of Tic-tac-toe, you may know all possible next state given the current state causes by all possible moves that your opponent can make. But the probabilities of reaching those next states is unknown, because you don’t know which strategy your opponent is using, it’s even more difficult to predict if your opponent is an irrational one. This example can be extended to almost all multi-player games, such as poker, blackjack, chess, go,… In these cases the expected return (true value) of a state can only be calculated by reaching the end of the game multiple times to calculate actual returns and averaging those returns. This is the idea of Monte Carlo methods.</p>
<h4 id="41-monte-carlo-prediction-or-monte-carlo-estimation-of-state-value">4.1 Monte Carlo Prediction (or Monte Carlo Estimation of State Value)<a class="headerlink" href="#41-monte-carlo-prediction-or-monte-carlo-estimation-of-state-value" title="Permanent link">&para;</a></h4>
<p>The algorithm for Monte Carlo (MC) Prediction is presented below:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled4.png" width="700" /></p>
</figure>
<p>This is the “First-visit” MC prediction, meaning that we estimate <span class="arithmatex">\(v_\pi(s)\)</span> by averaging out returns followed by <em>first visit</em> of <span class="arithmatex">\(s\)</span> in each episode. As opposed to “Every visit” MC prediction, where we estimate <span class="arithmatex">\(v_\pi(s)\)</span> by averaging out returns followed by <em>every</em> visit of <span class="arithmatex">\(s\)</span> in every episode. In MC prediction, we first initialize a return array for each state, append the return of that state every time we first encounter it in an episode, and update the value of a state by averaging out all the returns of that state sampled so far.</p>
<h4 id="42-monte-carlo-with-exploring-start-or-monte-carlo-estimation-of-action-value">4.2 Monte Carlo with Exploring Start (or Monte Carlo Estimation of Action Value)<a class="headerlink" href="#42-monte-carlo-with-exploring-start-or-monte-carlo-estimation-of-action-value" title="Permanent link">&para;</a></h4>
<p>We’ve obtain the state value function of an arbitrary policy, the next step is to find a better policy according to that. But difficulty arise since we know we know which state is better, but don’t know the environment dynamics, thus we don’t know which action to take that will lead us to the “better” state. This necessitates the action value function, according to which we can choose the best action of each state by choosing the action with the largest action value (<span class="arithmatex">\(\arg \max_a q_\pi(s,a)\)</span>).</p>
<p>This approach sounds ideal, but we have to face a problem: Because our policy is deterministic to this point, some state-action pairs can never be experienced, thus we cannot approximate the action value for those pairs. This is problematic since the purpose of RL is to find the best action for each state from <em>all possible actions</em> from that state. There are two ideas for addressing this issue:</p>
<ol>
<li>Specifying that an episode start from a certain state-action pair, and ensure that every state-action pair has non-zero probability of being chosen in the start</li>
<li>Consider only policies that are stochastic, with a non-zero possibility of selecting all actions in each state</li>
</ol>
<p>This algorithm is implemented following the first idea:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled5.png" width="700" /></p>
</figure>
<h4 id="43-on-policy-monte-carlo">4.3 On-policy Monte Carlo<a class="headerlink" href="#43-on-policy-monte-carlo" title="Permanent link">&para;</a></h4>
<p>The method of Exploring Start inherits certain drawbacks: firstly, it’s (almost) unachievable for problems with really large state and action space; secondly, there are certain state-action pairs that are so infrequently encountered or simply cannot happen, that trying to calculate their value turns out to be unnecessary.</p>
<p>To solve for this problem, we come up with a policy that isn’t strictly deterministic, but reserves a probability for choosing a random action. One example of such policy is <span class="arithmatex">\(\epsilon -\)</span>greedy policy, in which the non-greedy actions are selected with probability <span class="arithmatex">\(\frac{\epsilon}{|\mathcal A(s)|}\)</span>, whereas the greedy one is selected with probability <span class="arithmatex">\(1-\epsilon + \frac{\epsilon}{|\mathcal A(s)|}\)</span> (<span class="arithmatex">\(1-\epsilon\)</span> chance being chosen as a greedy action and <span class="arithmatex">\(\frac{\epsilon}{|\mathcal A(s)|}\)</span> chance being chosen as a non-greedy one). Apart from that, the algorithm for the control problem is exactly the same. Notice that when you repeatedly changing the policy to a better one according to the updated action value function, and then generate another episode according to that changed policy, the policy and value function will converge to the optimal policy and optimal value function respectively:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled6.png" width="700" /></p>
</figure>
<h4 id="44-off-policy-monte-carlo">4.4 Off-policy Monte Carlo<a class="headerlink" href="#44-off-policy-monte-carlo" title="Permanent link">&para;</a></h4>
<p>HOWEVER, the <span class="arithmatex">\(\epsilon-\)</span>soft policy approach still faces a drawback: it’s inherently inferior than a deterministic one (we still have a probability of choosing a non-greedy action). This is the classic problem of exploration vs exploitation tradeoff. The idea of an off-policy method is that we will use 2 policies: a behavior policy which agent uses to generate learning data, and a target policy which agent try to optimize. The target policy should a deterministic optimal policy while the behavior policy remain stochastic and more exploratory (like <span class="arithmatex">\(\epsilon\)</span>-greedy policy).</p>
<p>But there’s one problem: because the returns we receive following <span class="arithmatex">\(b\)</span> has different expectation compared to <span class="arithmatex">\(\pi\)</span>: <span class="arithmatex">\(\mathbb E_b[G_t| S_t = s] = v_b(s) \neq v_\pi(s)\)</span>, so they cannot be averaged to obtain <span class="arithmatex">\(v_\pi(s)\)</span>. To address this, we use the method of <strong><em>Importance Sampling</em></strong>, in which the returns are weighted base on the <em>relative probability of the trajectory</em> according to the 2 policies.</p>
<p>To understand this concept intuitively, we begin with the formula for calculating probability of a specific trajectory - <span class="arithmatex">\(S_t, A_t, S_{t+1}, A_{t+1}, ..., S_{T-1}, A_{T-1}, S_{T}\)</span> - according to a specific policy - <span class="arithmatex">\(\pi\)</span>:</p>
<div class="arithmatex">\[
\begin{align*}  &amp; \Pr \{S_t, A_t, S_{t+1}, A_{t+1},..., S_T|S_t, A_{t:T-1} \sim \pi \} \\
= &amp; \textcolor{green}{\Pr \{S_t, A_t, S_{t+1}}\} \times \textcolor{blue}{\Pr\{S_{t+1}, A_{t+1}, S_{t+2}\}} \times... \times \textcolor{purple}{\Pr \{S_{T-1}, A_{T-1}, S_T \}} \\
= &amp; \textcolor{green} {\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)} \textcolor{blue}{\pi(A_{t+1}|S_{t+1})p(S_{t+2}| S_{t+1}, A_{t+1})}...\textcolor{purple}{\pi(A_{t-1}|S_{t-1})p(S_t|S_{t-1},A_{t-1})} \\
= &amp; \prod_{k=t}^{T-1} \pi(A_k | S_k)p(S_{k+1}|S_k, A_k) \end{align*}
\]</div>
<p>Thus the relative probability of that same trajectory but follow 2 different policies:</p>
<div class="arithmatex">\[
\rho_{t:T-1} = \frac{\prod_{k=t}^{T-1} \pi(A_k | S_k) \textcolor{magenta}{p(S_{k+1}|S_k, A_k)}}{\prod_{k=t}^{T-1} b(A_k | S_k)\textcolor{magenta}{p(S_{k+1}|S_k, A_k)}} = \frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)}{\prod_{k=1}^{T-1}b(A_k|S_k)}
\]</div>
<p>Because the dynamics of the environment is the same no matter which policy we follow, the terms <span class="arithmatex">\(\prod_{k=t}^{T-1}p(S_{k+1}|S_k, A_k)\)</span> are cancelled out.</p>
<p>We can use this ratio to transform the returns to have the right expected value. The intuition here is that, if a trajectory more frequently appears in the target policy than in it does in the behavior policy, then <span class="arithmatex">\(\rho_{t:T-1} &gt; 1\)</span></p>
<div class="arithmatex">\[
\mathbb E_\pi[G_t|S_t = s] = \mathbb E_b[\rho_{t:T-1}G_t|S_t=s]
\]</div>
<p>It’s easy to derive our update formula of the value function as:</p>
<div class="arithmatex">\[
v_\pi(s) = \frac{\sum_{t \in \mathcal J(s)}\rho_{t:T(t)-1}G_t}{|\mathcal J(s)|}
\]</div>
<p>This kind of Importance Sampling is called Ordinary Importance Sampling, there’s also an alternative one, called Weighted Importance Sampling, which use a <em>weighted average</em> instead of the <em>average of weighted returns:</em></p>
<div class="arithmatex">\[
v_\pi(s) = \frac{\sum_{t \in \mathcal J(s)}\rho_{t:T(t)-1}G_t}{\sum_{t \in \mathcal J(s)}\rho_{t:T(t)-1}}
\]</div>
<p>We finally arrive at our algorithm for Off-policy MC Control:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled7.png" width="700" /></p>
</figure>
<p>Okay, there’re a lot that need explaining here. Firstly, what are those weird updating formula? They are exactly the update formula using weighted importance sampling, but in an incremental way. Here’s the proof for those incrementally implemented formulas. If you’re not that curious, it’s totally fine to just skip this part and accept the formulas as they are.</p>
<p><strong><em><u>Hypothesis</u></em></strong>:</p>
<p>Suppose that we have a sequence of returns that start from one state <span class="arithmatex">\(G_1, G_2,..., G_{n-1}\)</span> with their corresponding weights <span class="arithmatex">\(W_i (i: 1 \rightarrow n-1)\)</span>, the value of that state at the <span class="arithmatex">\(n^{th}\)</span> update is:</p>
<div class="arithmatex">\[
V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}
\]</div>
<p>Then the updating formula would be:</p>
<div class="arithmatex">\[
\begin{align*} V_{n+1} &amp; = V_n + \frac{W_n}{C_n}(G_n - V_n) \\  \text{in which} \qquad C_n &amp; = C_{n-1} + W_n\end{align*}
\]</div>
<p><strong><em><u>Proof</u></em></strong>:</p>
<div class="arithmatex">\[
\begin{align*} V_{n+1} &amp; = \frac{\sum_{k=1}^{n}W_kG_k}{\sum_{k=1}^{n}W_k}\\
&amp; = \frac{\sum_{k=1}^{n-1}W_kG_k + W_{n}G_n}{\sum_{k=1}^{n-1}W_k}\cdot\frac{\sum_{k=1}^{n-1}W_k}{\sum_{k=1}^{n}W_k} \\       &amp; = V_n \cdot \frac{\sum_{k=1}^{n-1}W_k}{\sum_{k=1}^{n}W_k}  + \frac{W_nG_n}{\sum_{k=1}^{n}W_k} \\      &amp; = V_n \left( 1 - \frac{W_n}{\sum_{k=1}^{n}W_k}\right) + G_n\frac{W_n}{\sum_{k=1}^{n}W_k} \\       &amp; = V_n + \frac{W_n}{\textcolor{red}{\sum_{k=1}^{n}W_n}}(G_n-V_n)      \end{align*}
\]</div>
<p>We can implement the red-colored sum recursively by defining <span class="arithmatex">\(C_n = C_{n-1} + W_n\)</span> and <span class="arithmatex">\(C_0 = 0\)</span>. By doing so, we obtain the updating formulas exactly like those in the hypothesis.</p>
<p>To this point you might be confusing about what the algorithm is trying to do in the orange and yellow box. Isn’t the update formula for <span class="arithmatex">\(W_t\)</span> supposed to be <span class="arithmatex">\(W_{t-1}\frac{\pi(A_t|S_t)}{b(A_t | S_t)}\)</span>, not <span class="arithmatex">\(W_{t-1}\frac{1}{b(A_t | S_t)}\)</span>? You’re right, remember that our target policy <span class="arithmatex">\(\pi\)</span> is deterministic? Thus the probability of taking a certain action, in a certain state is either 1 or 0. If it’s a 1, the update formula for <span class="arithmatex">\(W_t\)</span> is <span class="arithmatex">\(W_t = W_{t-1}\frac{1}{b(A_t | S_t)}\)</span> (hence the term in the yellow box). If it’s a 0, the update formula for <span class="arithmatex">\(W_t\)</span> is <span class="arithmatex">\(W_t = 0\)</span>, the action value of the previous state-action pair remains the same; and the same happen to all states preceding that state in the episode, since if just one transition step in a trajectory cannot happen when using the target policy, the whole trajectory also cannot happen.</p>
<p>Off-policy methods are often of greater variance and slower to converge, but is more powerful and general: on-policy methods are off-policy methods with target policy and behavior policy being the same.</p>
<h3 id="5-temporal-difference-learning">5. Temporal Difference Learning<a class="headerlink" href="#5-temporal-difference-learning" title="Permanent link">&para;</a></h3>
<p>It turns out that there’s a universal form for update formula in RL algorithm:</p>
<div class="arithmatex">\[
\text{NewEstimate} \leftarrow \text{OldEstimate} + \alpha[\text{NewEstimate} - \text{OldEstimate}]
\]</div>
<p>If we want to represent the estimate value of a state following MC method, which is the average of returns from that state, the update formula will be:</p>
<div class="arithmatex">\[
V(S_t) \leftarrow V(S_t) + \frac{1}{n}[G_t - V(S_t)]
\]</div>
<p>with <span class="arithmatex">\(n\)</span> representing the number of returns calculated from that state so far (you can prove this update formula by yourself, or substitute <span class="arithmatex">\(n\)</span> with <span class="arithmatex">\(1, 2, 3,…\)</span> to see what you’ll receive). Now if <span class="arithmatex">\(\alpha\)</span> is constant, we obtain a method called Constant-<span class="arithmatex">\(\alpha\)</span> MC, which will not return the exact average of returns, but is convenient computationally. The idea of Temporal Difference (TD) learning is that instead of setting the <span class="arithmatex">\(\text{NewEstimate}\)</span> to <span class="arithmatex">\(G_t\)</span>, whose value we have to wait till the end of every episode to calculate, we set it to <span class="arithmatex">\(R_{t+1} + \gamma V(S_t)\)</span>, whose value can be calculate as soon as a transition is made to the next state:</p>
<div class="arithmatex">\[
V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\]</div>
<h4 id="51-td-prediction">5.1 TD Prediction<a class="headerlink" href="#51-td-prediction" title="Permanent link">&para;</a></h4>
<p>The algorithm for estimating <span class="arithmatex">\(v_\pi\)</span> based on the above update formula is called <strong><em>TD(0)</em></strong>, or <strong><em>one-step TD:</em></strong></p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled8.png" width="700" /></p>
</figure>
<p>By updating the value of each state as soon as we transit to the next state, the updates are distributed uniformly over the time interval of an episode, there are more information contains in each update (we use the updated value of preceding states to calculate for the target estimate for the succeeding states within a single episode), and we don’t need to reach the end of an episode to be able to update the value function (which is really beneficial in continuing tasks or episodic tasks with long episodes). These reasons make TD methods converge faster than MC methods in practice.</p>
<p>Getting confused? I feel you… Let me provide you another intuition to get you to take a grasp of the bigger picture. Here is the definition of the value function we’ve known:</p>
<div class="arithmatex">\[
\begin{align*} v_\pi(s) &amp; = \mathbb E[G_t | S_t = s] \qquad \qquad \qquad \qquad \text{ }(5)\\ &amp; = \mathbb E[R_{t+1} + \gamma G_{t+1}|S_t = s] \\ &amp; = \mathbb E[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s] \qquad (6)\end{align*}
\]</div>
<p>MC uses the estimate of <span class="arithmatex">\((5)\)</span> as the target for updating the value function, while TD learning uses the estimate of <span class="arithmatex">\((6)\)</span> as a target. The target of MC is an estimate because it only bases on <em>a single sample</em>, not a whole distribution of possible states and reward subsequent to that state (just the return, not the expected return). The target of DP is also an estimate but not for the same reason, indeed it’s because the <span class="arithmatex">\(v_\pi(S_{t+1})\)</span> is just a current <em>estimate of the value</em> - <span class="arithmatex">\(V(S_{t+1})\)</span>. While the target of TD learning is an estimate for <em>both reasons</em> above.</p>
<h4 id="52-sarsa-on-policy-td-control">5.2 Sarsa: On-policy TD Control<a class="headerlink" href="#52-sarsa-on-policy-td-control" title="Permanent link">&para;</a></h4>
<p>The same story goes for TD learning methods: If we want to improve the policy somehow, we should estimate action value function instead of state value function. The same as MC control algorithm: If you try to update the value function according to the current policy, and then choose actions according a policy derived from that updated value function, then the value function will converge to the optimal value function (<span class="arithmatex">\(Q \rightarrow q_*\)</span>):</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled9.png" width="700" /></p>
</figure>
<p>Fun fact: the algorithm is called Sarsa since each update requires a quintuple of events <span class="arithmatex">\((S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\)</span>.</p>
<h4 id="53-q-learning-off-policy-td-control">5.3 Q-learning: Off-policy TD Control<a class="headerlink" href="#53-q-learning-off-policy-td-control" title="Permanent link">&para;</a></h4>
<p>In Q-learning algorithm, the action used to calculate the target in the update formula, is the one that maximize <span class="arithmatex">\(Q(S’,a)\)</span>, not the one selected by a greedy policy derived by our current action value function (like in Sarsa). The target policy, which can be a deterministic one, is learned dependently from that policy derived from our current value function, which is required to be more explorative. Hence this algorithm is an Off-policy one.</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled10.png" width="700" /></p>
</figure>
<h4 id="54-maximization-bias-and-double-learning">5.4 Maximization Bias and Double Learning<a class="headerlink" href="#54-maximization-bias-and-double-learning" title="Permanent link">&para;</a></h4>
<p>The update formula for Q-learning has a maximization operation. In this algorithm, the maximum of estimated values is used implicitly as an estimate of the maximum (of true) value, causes something called <strong><em>maximization bias</em></strong> (consider a state with <span class="arithmatex">\(q(s,a)\)</span> for all <span class="arithmatex">\(a\)</span> is zero but the estimated value fluctuate around <span class="arithmatex">\(0\)</span>, the <em>maximum of the true values</em> is zero while the <em>maximum of the estimated values</em> is positive). Take a look at this example:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled11.png" width="700" /></p>
<figcaption>
<p>Figure 3: Comparison between Q-learning and Double Q-learning on a 4-state episodic MDP.
  </p>
</figcaption>
</figure>
<p>This is a simple MDP. The agent always starts from <span class="arithmatex">\(A\)</span> and can choose between 2 actions: left or right. The right action transit it to a terminal state which gives a reward of 0. The left action transit it to another terminal state - <span class="arithmatex">\(B\)</span>, from which it can take many actions, all of which transit the it to the terminal state with a reward sampled from a normal distribution with <span class="arithmatex">\(\mu = -0.1\)</span> and <span class="arithmatex">\(\sigma = 1\)</span>. In this example, it’s optimal to always choose right. But as you can see Q-learning algorithm initially take the left action more often than the right action, and always choose it with probability much more than <span class="arithmatex">\(5\%\)</span>, which is the probability enforced by the <span class="arithmatex">\(\epsilon-\)</span>greedy action selection with <span class="arithmatex">\(\epsilon = 0.1\)</span>. Why does this happen, to be specific? It’s reasonable to expect that <span class="arithmatex">\(Q(A, r) = 0\)</span> and <span class="arithmatex">\(Q(A, l) = -0.1\)</span>. And you’re right for the <span class="arithmatex">\(Q(A, r)\)</span>, but for <span class="arithmatex">\(Q(A,l)\)</span>, something else’s happening here: because the <span class="arithmatex">\(Q(B, a)\)</span> for <span class="arithmatex">\(a \in \mathcal A(B)\)</span> fluctuate around <span class="arithmatex">\(-0.1\)</span>, there’s a high chance that some of the action values from state <span class="arithmatex">\(B\)</span> are positive, thus the maximum of <span class="arithmatex">\(Q(B,a)\)</span> is positive. And because Q-learning always use the maximum action value as the target for value function update, <span class="arithmatex">\(Q(A, l)\)</span> will be positive initially (<span class="arithmatex">\(Q(A, l) \leftarrow Q(A, l) + \alpha [R_{B} + \gamma \max_aQ(B,a) - Q(A,l)]\)</span>), thus the agents always choose left in the beginning. Not until after a plenty of steps that the action values from state <span class="arithmatex">\(B\)</span> approaches their true value, which is <span class="arithmatex">\(-0.1\)</span>, because of which the agent starts to choose right more often.</p>
<p>The cause to this problem is using the same samples for both determining the maximizing action and to estimating its value. The solution proposed in Double Q-learning algorithm is <em>using 2 value functions</em> and updating them independently. In each timestep, we would use one for action selection: <span class="arithmatex">\(A_* = \max_a Q_1(s,a)\)</span> and one for action evaluation: <span class="arithmatex">\(Q_2(A_*) = Q_2(s, \max_a Q_1(s,a))\)</span>, and by a probability of <span class="arithmatex">\(0.5\)</span>, we switch the role of those value functions: <span class="arithmatex">\(Q_1(A_*) = Q_1(s, \max_a Q_2(s,a))\)</span>.</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled12.png" width="700" /></p>
</figure>
<h3 id="6-n-step-bootstrapping">6. n-step Bootstrapping<a class="headerlink" href="#6-n-step-bootstrapping" title="Permanent link">&para;</a></h3>
<p>Congrats! You’ve come such a long way. At this point, you can imagine the space of methods form a spectrum with one end is the one-step TD learning, the other end is MC, and the rest of them are methods whose update rules based on a number of rewards ranging from more than one, but less than the rest of them until termination. Those “intermediate methods” are called n-step TD learning. Below is the backup diagrams of n-step methods. A backup diagram shows how a state (or state-action pair) is updated by information transferred back from its successor states (or state-action pair), with white nodes represent states and black nodes represent actions.</p>
<figure>
<p><img alt="Figure 4: Spectrum of RL algorithms, ranging from one-step TD to Monte Carlo." src="../research/rl-series-part-1/images/Untitled13.png" /></p>
<figcaption>
<p>Figure 4: Spectrum of RL algorithms, ranging from one-step TD to Monte Carlo.
  </p>
</figcaption>
</figure>
<h4 id="61-n-step-td-prediction">6.1 n-step TD Prediction<a class="headerlink" href="#61-n-step-td-prediction" title="Permanent link">&para;</a></h4>
<p>The update rule of n-step TD Prediction is just as we expect: instead of accumulating the reward all the way to terminal state, we do that up to timestep <span class="arithmatex">\(t+n\)</span>, then the rest is truncated to the value of state <span class="arithmatex">\(S_{t+n}\)</span>:</p>
<div class="arithmatex">\[
G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n}+ \gamma^n \textcolor{orange}{V_{t+n-1}(S_{t+n})}
\]</div>
<p>Here’s the complete algorithm:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled14.png" width="700" /></p>
</figure>
<p>It turns out that intermediate methods perform much better than extreme ones:</p>
<figure>
<p><img alt="Figure 5: Performance of n-step TD methods varies with different values of \(\alpha\), for different values of n, when applied to a random walk task with 19 states." src="../research/rl-series-part-1/images/Untitled15.png" /></p>
<figcaption>
<p>Figure 5: Performance of n-step TD methods varies with different values of <span class="arithmatex">\(\alpha\)</span>, for different values of n, when applied to a random walk task with 19 states.
  </p>
</figcaption>
</figure>
<h4 id="62-n-step-sarsa">6.2 n-step Sarsa<a class="headerlink" href="#62-n-step-sarsa" title="Permanent link">&para;</a></h4>
<p>To change the method from solving a prediction problem to a control problem, we simply switch the state value to action value, and then use an <span class="arithmatex">\(\epsilon-\)</span>greedy policy:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled16.png" width="700" /></p>
</figure>
<p>Notice that whether we keep the policy unchanged or change it with respect to <span class="arithmatex">\(Q\)</span> depends on our purpose: whether we want to approximate the optimal value function and optimal policy, or just the value function for some arbitrary policy. Although our usual goal would be the former.</p>
<p>For more intuition about the difference between one-step and n-step Sarsa, consider this Gridworld example:</p>
<figure>
<p><img alt="Figure 6: Comparison between one-step Sarsa and 10-step Sarsa on how much each algorithm learns in a single episode using a Gridworld example." src="../research/rl-series-part-1/images/Untitled17.png" /></p>
<figcaption>
<p>Figure 6: Comparison between one-step Sarsa and 10-step Sarsa on how much each algorithm learns in a single episode using a Gridworld example.
 </p>
</figcaption>
</figure>
<p>This Gridworld example illustrates the enhance in speed of policy improvement in 2 algorithms: one-step Sarsa and 10-step Sarsa. In this problem, reward of all states are zero instead of the terminal state mark by G, which contains a positive reward. Suppose we initialize the value of all states to zero, and the agent take the path presented in the first panel. In one-step Sarsa, only the action immediately prior to the terminal state is strengthened, whereas in 10-step Sarsa, 10 last actions of the episode is strengthened, which leads to much faster learning.</p>
<h4 id="63-n-step-off-policy-learning">6.3 n-step Off-policy Learning<a class="headerlink" href="#63-n-step-off-policy-learning" title="Permanent link">&para;</a></h4>
<p>The off-policy form of n-step TD is quite straightforward: we just have to add the importance sampling ratio in order to used the data generated from <span class="arithmatex">\(b\)</span>. But since in n-step methods, returns are constructed over n steps, so we’re only interested in the relative probability up in just those n actions. The update formula for the off-policy version of n-step TD:</p>
<div class="arithmatex">\[
V_{t+n}(S_t) = V_{t+n-1}(S_t) + \alpha \rho_{t:t+n-1}(G_{t:t+n} - V_{t+n-1}(S_t)) \\
\text{with } \quad \rho_{t:h} = \prod_{k = t}^{\min(h, T-1)} \frac{\pi(A_k|S_k)}{b(A_k|S_k)}
\]</div>
<p>Similarly, the off-policy form for our n-step Sarsa update formula is:</p>
<div class="arithmatex">\[
Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \alpha \textcolor{green}{\rho_{t+1:t+n}} (G_{t:t+n} - Q_{t+n-1}(S_t, A_t))
\]</div>
<p>Notice that in this formula the importance sampling ratio starts and ends 1 step later than in n-step TD. It’s because here, the action whose value is estimated has already been chosen, so the trajectory probability up to that action for the behavior policy and target policy is both 1. Furthermore, the last value we need for bootstrapping is the value of a state-action pair, not of a state, so we have to include the probability of taking that last action in the importance sampling ratio. Here is pseudocode for the full algorithm, the only difference it makes from on-policy n-step Sarsa is the addition of the importance sampling ratio:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled18.png" width="700" /></p>
</figure>
<h4 id="64-off-policy-learning-without-importance-sampling-n-step-tree-backup-algorithm">6.4 Off-policy learning without Importance Sampling: n-step Tree Backup Algorithm<a class="headerlink" href="#64-off-policy-learning-without-importance-sampling-n-step-tree-backup-algorithm" title="Permanent link">&para;</a></h4>
<p>Is there an off-policy method without importance sampling? We’ve seen these methods, they are Q-learning and Expected Sarsa, but they bootstrap after just one timestep. Turns out that there is a corresponding algorithm but for multiple-step case, it’s the n-step Tree Backup algorithm. How to compute the target for the update formula in this algorithm is best illustrated via a backup diagram:</p>
<p>If in n-step Sarsa, the target is calculated by combining the rewards collected from timestep <span class="arithmatex">\(t+1\)</span> to timestep <span class="arithmatex">\(t+n\)</span>, and the value of the state-action pair <span class="arithmatex">\((S_{t+n}, A_{t+n})\)</span>, then in n-step Tree Backup algorithm, the target is exactly that, plus the value of the actions that are not taken, at every timestep, which are the actions left hanging in the backup tree beside.</p>
<p>We’ll derive the formula for the update target from this backup tree. If our algorithm is just 1-step Tree Backup, the target is the same as one in Expected Sarsa:</p>
<figure>
<p><img alt="Figure 7: Comparison between 3-step Sarsa and 3-step Tree Backup using a backup diagram. Nodes on the lines represent states and actikjons account for the value of the state-action pair at the roots." src="../research/rl-series-part-1/images/Untitled19.png" width="500" /></p>
<figcaption>
<p>Figure 7: Comparison between 3-step Sarsa and 3-step Tree Backup using a backup diagram. Nodes on the lines represent states and actions account for the value of the state-action pair at the roots.
 </p>
</figcaption>
</figure>
<div class="arithmatex">\[
G_{t:t+1} = R_{t+1} + \gamma \sum_a \pi(a | S_{t+1})Q(S_{t+1}, a)
\]</div>
<p>If our algorithm is a 2-step one, each action <span class="arithmatex">\(a\)</span> at the first level contribute in calculating the target with the weight of <span class="arithmatex">\(\pi(a | S_{t+1})\)</span>, except for action actually taken. Instead, its probability, <span class="arithmatex">\(\pi(A_{t+1} | S_{t+1})\)</span> is used to calculate the weight of action values at the second level. Thus each action <span class="arithmatex">\(a'\)</span> at this level contributes with the weight of <span class="arithmatex">\(\pi(A_{t+1}|S_{t+1})\pi(a’|S_{t+2})\)</span>:</p>
<div class="arithmatex">\[
\begin{align*} G_{t:t+2} &amp; = R_{t+1} + \gamma \sum_{a\neq A_{t+1}} \pi(a | S_{t+1})Q(S_{t+1}, a) + \gamma \pi(A_{t+1} | S_{t+1}) \left [R_{t+2} + \gamma \sum_{a'}\pi(a'|S_{t+2})Q(S_{t+2}, a')\right] \\
&amp; = R_{t+1} + \gamma \sum_{a\neq A_{t+1}} \pi(a | S_{t+1})Q(S_{t+1}, a) + \gamma \pi(A_{t+1} | S_{t+1})G_{t+1: t+2}
\end{align*}
\]</div>
<p>This recursive form is really helpful since we can form a more general update target formula for n-step Tree Backup algorithm, which is:</p>
<div class="arithmatex">\[
G_{t:t+n} = R_{t+1} + \gamma \sum_{a\neq A_{t+1}} \pi(a | S_{t+1})Q(S_{t+1}, a) + \gamma \pi(A_{t+1} | S_{t+1})G_{t+1: t+n}
\]</div>
<p>The complete pseudocode for the algorithm is presented in the box below, it’s pretty similar to n-step Sarsa, only the update target is computed differently:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled20.png" width="700" /></p>
</figure>
<h3 id="7-problems-and-extension-regarding-tabular-methods">7. Problems and Extension regarding Tabular Methods<a class="headerlink" href="#7-problems-and-extension-regarding-tabular-methods" title="Permanent link">&para;</a></h3>
<h4 id="71-dyna-integration-of-learning-and-planning">7.1 Dyna: Integration of Learning and Planning<a class="headerlink" href="#71-dyna-integration-of-learning-and-planning" title="Permanent link">&para;</a></h4>
<p>If we categorized all of the above algorithms, we’ve got <em>model-based</em> (Dynamic Programming, in which we need the model of the environment to be able to update the value function) and <em>model-free</em> (Monte Carlo, Temporal Difference, in which we just need the data sampled directly from the environment) reinforcement learning. The process of updating the value function and improving the policy via interactions with the <em>real environment</em> or the <em>model</em> is called <em>learning</em> and <em>planning</em> respectively.</p>
<p>When planning is done online (while interacting with the environment), there are a lot of problems need considering: how to update the model according to the continuously collected data from interaction with the real environment, how to choose state-action to update when interacting with the model,… Dyna-Q is a simple model demonstrates how the 2 processes of learning and planning is done in every timestep:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled21.png" width="700" /></p>
</figure>
<p>You can observe that for Dyna-Q, real experience serves 2 purposes: update value function and/or policy (direct learning) and update the model (the model is later used for planning, which is call indirect learning). The algorithm used for direct learning is Q-learning, and for planning is also Q-learning, but the state-action pairs are randomly sampled from seen ones.</p>
<p>This model fit well to problems that the interaction between the agent and environment is limited, or that there’s a high cost doing so. Integrating planning into the process of learning will probably takes the agent far fewer episodes to reach the optimal performance.</p>
<h4 id="72-when-the-model-is-wrong">7.2 When the model is wrong<a class="headerlink" href="#72-when-the-model-is-wrong" title="Permanent link">&para;</a></h4>
<p>One problem arises when planning is that sometime the model is wrong. This can be caused by several reasons: the environment is stochastic and a small number of samples are observed; the model is learned by function approximation in which the function updated is either chosen to be too simple that it cannot represent the environment accurately, or too complex so that it failed to generalize; the environment changes over time;… This failure in constructing the model most of the time will lead to suboptimal policies due to planning. The fixing of the environment will lead to 2 scenarios - either the environment becomes <em>better</em> or <em>worse</em> for the current policy. Let’s take a look at these example:</p>
<figure>
<p><img alt="Figure 8: Examples of when the model changes for the worse (Blocking maze) and changes for the better (Shortcut maze)" src="../research/rl-series-part-1/images/Untitled22.png" /></p>
<figcaption>
<p>Figure 8: Examples of when the model changes for the worse (Blocking maze) and changes for the better (Shortcut maze)
 </p>
</figcaption>
</figure>
<p>The task of the agent is to find the shortest way from S to G. The first panel describes the scenario where the optimal path is blocked after the model is relearned (aka environment changes to become worse). The second describes the scenario where there’s an additional shortcut when the model is relearned. Dyna-Q+ is fundamentally Dyna-Q with additional bonus in behavior policy that encourage exploration. In the former scenario, both our Dyna-Q agents can relearn a better policy after they’ve been wandering around the right corner (the part where the graph flattens out). This happens because following a policy that they are so optimistic about leads the agents to new discoveries and updates that help them in finding out that the current policy is “not that optimal” so that they work hard to find an optimal one. But in the latter scenario, the issue becomes complicated: the Dyna-Q agent doesn’t realize that there’s a better policy since the cost for discovering new policy is so large that it’s safer for the agent to stay with the current one (in order to discover the optimal policy, the agent has to make a bunch of exploration steps into low-value states), however the Dyna-Q+ manages to find such policy since the bonus it’s given for exploration at some point outweighs the accumulative reward it receives for sticking with the current policy.</p>
<p>What gives Dyna-Q+ this exploration ability is that it’s given a bonus reward for each state-action pair encountered:</p>
<div class="arithmatex">\[
r' = r+k\sqrt{\tau}
\]</div>
<p>where <span class="arithmatex">\(\tau\)</span> is the number of time steps that the <span class="arithmatex">\(s-a\)</span> pair hasn't been tried.</p>
<h4 id="73-prioritize-sweeping">7.3 Prioritize Sweeping<a class="headerlink" href="#73-prioritize-sweeping" title="Permanent link">&para;</a></h4>
<p>Prioritize Sweeping is an updating strategy in planning, in which the states being updated are prioritized base on there predicted change in value. One way to achieving this is to work backward from goal states, but to be more general, it is to work backward from states whose values are just changed. The full algorithm is presented below:</p>
<figure>
<p><img _="&quot;" alt="Untitled" src="../research/rl-series-part-1/images/Untitled23.png" width="700" /></p>
</figure>
<p>This approach is call backward focusing. There is another approach called forward focusing where priority is determined based on how easily a state can be reached from states that are frequently visited under our current policy.</p>
<h4 id="74-trajectory-sampling">7.4 Trajectory Sampling<a class="headerlink" href="#74-trajectory-sampling" title="Permanent link">&para;</a></h4>
<p>Trajectory Sampling is a strategy in distributing updates where instead of exhaustive sweep through the entire state space (like in DP), you update them according to an on-policy distribution, which is the distribution of states and actions encountered when following the current policy. This is beneficial for problems where the state space is too big that it’s computationally expensive to sweep through all of them. Instead, there are some states that need more focus on, such as those being visited frequently following our current policy.</p>
<p>Here is the result of a small experiment. The environment is designed so that each state-action pair can lead to <span class="arithmatex">\(b\)</span> possible next state equally (<span class="arithmatex">\(b\)</span> is called the branching factor). On all transitions there is a 0.1 chance of reaching the terminal state. And the expected reward of all transition is sampled from a Gaussian distribution with mean 0 and variance 1.</p>
<figure>
<p><img alt="Figure 9: Relative performance between uniform sampling and on-policy sampling for a 1,000-state MDP with different branching factors." src="../research/rl-series-part-1/images/Untitled24.png" /></p>
<figcaption>
<p>Figure 9: Relative performance between uniform sampling and on-policy sampling for a 1,000-state MDP with different branching factors.
 </p>
</figcaption>
</figure>
<p>While being beneficial for problems with a large state space in which just a small subset is visited frequently, trajectory sampling can be detrimental in the sense that it can end up causing the same states to be updated again and again. This drawback can be seen from the graph where the performance of uniform sampling is better than on-policy sampling asymptotically.</p>
<h2 id="section-2-case-study">Section 2: Case Study<a class="headerlink" href="#section-2-case-study" title="Permanent link">&para;</a></h2>
<p>Here’s a <a href="https://github.com/baotram153/RL-case-studies/blob/main/Tabular-Methods/Classical-RL-Algos.ipynb" target="_blank">notebook</a> that contains the code for all the algorithms mentioned above. In these experiments, we used 4 toy environments (Frozen Lake - deterministic, Frozen Lake - slippery, Cliff Walking and Taxi) provided by <code>gymnasium</code> - a library that provides API for lots of useful Reinforcement Learning environments. <code>Frozen Lake</code> and <code>Cliff Walking</code> are environments where the agent has to reach the goal (a present/cookie) without breaking the ice/falling off the grid, while <code>Taxi</code> is an environment where our agent (a taxi) navigates to passengers, picks them up and delivers them to the building.</p>
<p>If there’s a mistake that I’ve made, or there is something in the notebook that confuses you, feel free to reach out to me!</p>
<p>There are a few things that you should notice:</p>
<h3 id="1-sarsa-vs-q-learning-in-cliff-walking">1. Sarsa vs Q-learning in Cliff Walking<a class="headerlink" href="#1-sarsa-vs-q-learning-in-cliff-walking" title="Permanent link">&para;</a></h3>
<p>Sarsa chose the safer path than Q-learning. This happens because the state-action values calculated by Sarsa take into account all possible states and actions from the policy and the environment dynamics, this results in this algorithm concerning about both the reward of reaching the goal and the penalty of falling off the cliff, while the state-action values calculated by Q-learning only takes into account the actions that maximize the reward, hence the agent chose the shortest path to get to the goal. In other words, agent learned with Sarsa algorithm chose the path that maximized expected return, while agent learned with Q-learning algorithm chose the path that maximized greatest possible return.</p>
<figure>
<!-- ![Figure 10: The optimal policy found by SARSA in Cliff Walking environment.](images/sarsa.mp4) -->
<!-- <video width="700" controls loop="" muted="" autoplay="">
  <source src="https://www.youtube.com/watch?v=2Z1lsVqk3nc">
</video> -->
<iframe allowfullscreen="allowfullscreen" frameborder="0" height="300" src="https://www.youtube.com/embed/2Z1lsVqk3nc" width="650"></iframe>
<figcaption>
<p>Figure 10: The optimal policy found by SARSA in Cliff Walking environment.
 </p>
</figcaption>
</figcaption>
</figure>
<figure>
<!-- ![Figure 11: The optimal policy found by Q-learning in Cliff Walking environment.](images/q_learning.mp4) -->
<iframe allowfullscreen="allowfullscreen" frameborder="0" height="300" src="https://www.youtube.com/embed/-UTGXvIQEyE" width="650"></iframe>
<figcaption>
<p>Figure 11: The optimal policy found by Q-learning in Cliff Walking environment.</p>
</figcaption>
</figure>
<h3 id="2-value-iteration-vs-policy-iteration">2. Value Iteration vs Policy Iteration<a class="headerlink" href="#2-value-iteration-vs-policy-iteration" title="Permanent link">&para;</a></h3>
<p>It’s easy to observe that Value Iteration yields better result than Policy Iteration, since it cancels out a lot of redundant iterative steps of approaching the truth value of states following a suboptimal policy.</p>
<figure>
<p><img alt="Figure 12: Number of steps using Value Iteration and Policy Iteration, average over 5 experiments" src="../research/rl-series-part-1/images/image.png" /></p>
<figcaption>
<p>Figure 12: Number of steps using Value Iteration and Policy Iteration, average over 5 experiments</p>
</figcaption>
</figure>
<figure>
<p><img alt="Figure 13: The Frozen Lake environment and the corresponding Q-table learned by the Value Iteration algorithm. The direction of the arrow and the color of each state in the above graph represent the optimal action (action which has the highest value) and its corresponding value in each state." src="../research/rl-series-part-1/images/image1.png" /></p>
<figcaption>
<p>Figure 13: The Frozen Lake environment and the corresponding Q-table learned by the Value Iteration algorithm. The direction of the arrow and the color of each state in the above graph represent the optimal action (action which has the highest value) and its corresponding value in each state.
 </p>
</figcaption>
</figure>
<h3 id="3-how-many-steps-are-optimal-for-n-step-bootstrapping-algorithms">3. How many steps are optimal for n-step bootstrapping algorithms?<a class="headerlink" href="#3-how-many-steps-are-optimal-for-n-step-bootstrapping-algorithms" title="Permanent link">&para;</a></h3>
<p>The below plot shows the performance of n-step SARSA and n-step Tree Backup algorithms with different number of bootstrap steps. We can see that the step n set at 4 or 8 is optimal for both algorithms, in the Cliff Walking environment</p>
<figure>
<p><img alt="Figure 14: Performance of n-step Tree Backup with n = [2, 4, 8, 16] in Cliff Walking environment." src="../research/rl-series-part-1/images/image2.png" /></p>
<figcaption>
<p>Figure 14: Performance of n-step Tree Backup with n = [2, 4, 8, 16] in Cliff Walking environment.
 </p>
</figcaption>
</figure>
<figure>
<p><img alt="Figure 15: Performance of n-step Sarsa with n = [2, 4, 8, 16] in Cliff Walking environment." src="../research/rl-series-part-1/images/image3.png" /></p>
<figcaption>
<p>Figure 15: Performance of n-step Sarsa with n = [2, 4, 8, 16] in Cliff Walking environment.
 </p>
</figcaption>
</figure>
<h3 id="4-the-best-algorithm-for-each-environment-not-include-n-step-bootstrapping-algorithms">4. The best algorithm for each environment (not include n-step bootstrapping algorithms)<a class="headerlink" href="#4-the-best-algorithm-for-each-environment-not-include-n-step-bootstrapping-algorithms" title="Permanent link">&para;</a></h3>
<p>We can see that all the algorithms have comparative performance to one another in our toy environments such as Cliff Walking or Taxi, except for Monte Carlo. We can see from the graph “Steps until terminated” that Q-Learning is the fastest to converge, and Monte Carlo is the slowest. However, in different hyperparameter and environment settings, this may not remain true. Double Q-Learning algorithm may converge faster in more complex environment since it addresses the issue of positive bias made by Q-Learning. But in easy environments, it performs worse since there are two Q tables to be updated.</p>
<figure>
<p><img alt="Figure 16: Comparison between different Reinforcement Learning algorithms in Cliff Walking environment." src="../research/rl-series-part-1/images/image4.png" /></p>
<figcaption>
<p>Figure 16: Comparison between different Reinforcement Learning algorithms in Cliff Walking environment.
 </p>
</figcaption>
</figure>
<figure>
<p><img alt="Figure 17: Comparison between different Reinforcement Learning algorithms in the Taxi environment." src="../research/rl-series-part-1/images/image5.png" /></p>
<figcaption>
<p>Figure 17: Comparison between different Reinforcement Learning algorithms in the Taxi environment.
 </p>
</figcaption>
</figure>
<h3 id="5-the-strange-case-of-monte-carlos-gamma-fine-tuning">5. The strange case of Monte Carlo’s gamma fine-tuning<a class="headerlink" href="#5-the-strange-case-of-monte-carlos-gamma-fine-tuning" title="Permanent link">&para;</a></h3>
<p>While running the experiment, we’ve observed Monte Carlo’s strange behaviors: its inability to converge although the greedy hyperparameter <span class="arithmatex">\(\epsilon\)</span> was set to be very high. This happened because the discounted factor (<span class="arithmatex">\(\gamma\)</span>) was so small (<span class="arithmatex">\(0.9\)</span>) that the accumulated penalty for exploring around (the agent had greater risk of falling off the cliff, which gave a penalty of -100 in return) was much worse than standing at the same state or repeatedly visiting the same states (the maximum number of timesteps is 1000, so the accumulative penalty of standing at the same state was <span class="arithmatex">\(\frac{0.9^{1000}\times(-1) - 0.9^0 \times (-1)}{0.9 - 1} \approx -10\)</span>), which was approximately the same as wandering around about 60 timesteps, and then entering the terminal state?!  To address this problem, we had to increase the gamma (<span class="arithmatex">\(\gamma\)</span>) hyperparameter to <span class="arithmatex">\(0.99\)</span> or <span class="arithmatex">\(1\)</span>, and the algorithm converged perfectly.</p>
<figure>
<p><img alt="Figure 18: Monte Carlo cannot converge in Cliff Walking environment when *discount factor* (\(\gamma\)) is set to \(0.9\)." src="../research/rl-series-part-1/images/image6.png" /></p>
<figcaption>
<p>Figure 18: Monte Carlo cannot converge in Cliff Walking environment when <em>discount factor</em> (<span class="arithmatex">\(\gamma\)</span>) is set to <span class="arithmatex">\(0.9\)</span>.
 </p>
</figcaption>
</figure>
<figure>
<p><img alt="Figure 19: Monte Carlo algorithm converges in Cliff Walking environment when *discount factor* (gamma) is set to \(1\)." src="../research/rl-series-part-1/images/image7.png" /></p>
<figcaption>
<p>Figure 19: Monte Carlo algorithm converges in Cliff Walking environment when <em>discount factor</em> (<span class="arithmatex">\(\gamma\)</span>) is set to <span class="arithmatex">\(1\)</span>.
 </p>
</figcaption>
</figure>
<p>This is also one of the biggest problems of Reinforcement Learning, that is the time-consuming task of hyperparameter tuning: just a small change in hyperparameter can make the algorithm to be extremely unstable.</p>
<h2 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h2>
<p>We’ve gone a pretty far way through different classical Reinforcement Learning algorithms, and applied them to some toy environments like Frozen Lake, Cliff Walking and Taxi. We‘ve also learned about the trade-offs between those algorithms, between on-policy and off-policy methods, and between exploration and exploitation in finding the optimal solution; about different strategies to improve the efficiency of exploration and updating of state/action values; and about the drawbacks of Reinforcement Learning algorithms in general and how to avoid (well, a part of) them. We hope that this article will invoke your enthusiasm and eagerness to explore more about the subject matter.</p>
<p>If you want to discuss about certain parts of the article, or interesting RL ideas, feel free to contact me via email <code>tramdang7907129@gmail.com</code></p>
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<p>[1] R. S. Sutton and A. Barto, <em>Reinforcement learning : An introduction</em>. London: The Mit Press, 2018.</p>
<p>[2] Gymnasium documentation. (n.d.). Retrieved November 7, 2024, from https://gymnasium.farama.org</p>







  
    
  
  
    
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 12, 2024</span>
  </span>

    
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Created">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">November 12, 2024</span>
  </span>

    
    
    
  </aside>



  



<h2 id="__comments">Comments</h2>
<script src="https://giscus.app/client.js"
        data-repo="TickLabVN/knowledge"
        data-repo-id="R_kgDOKi0muQ"
        data-category="Q&A"
        data-category-id="DIC_kwDOKi0muc4CaS0Y"
        data-mapping="pathname"
        data-strict="1"
        data-reactions-enabled="1"
        data-emit-metadata="1"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

        // Instruct Giscus to set theme
        giscus.setAttribute("data-theme", theme)
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate"
                    ? "transparent_dark"
                    : "light"

                // Instruct Giscus to change theme
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    { giscus: { setConfig: { theme } } },
                    "https://giscus.app"
                )
            }
        })
    })
</script>

        
    </article>
</div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <!-- Footer -->
<footer class="md-footer">

    <!-- Link to previous and/or next page -->
    
      
        
          
        
        <nav
          class="md-footer__inner md-grid"
          aria-label="Footer"
          
        >
  
          <!-- Link to previous page -->
          
            
            <a
              href="../colmap-a-software-for-3d-spare--dense-reconstruction/"
              class="md-footer__link md-footer__link--prev"
              aria-label="Previous: COLMAP: A software for 3D Spare &amp;amp; Dense Reconstruction"
            >
              <div class="md-footer__button md-icon">
                
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
              </div>
              <div class="md-footer__title">
                <span class="md-footer__direction">
                  Previous
                </span>
                <div class="md-ellipsis">
                  COLMAP: A software for 3D Spare &amp; Dense Reconstruction
                </div>
              </div>
            </a>
          
  
          <!-- Link to next page -->
          
        </nav>
      
    
  
    <!-- Further information -->
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-copyright">
    
    <div class="md-copyright__highlight">
        Copyright &copy; 2024 TickLab. Made by Engineering team.
    </div>
    
    
</div>
  
        <!-- Social links -->
        
          <div class="md-social">
    
    
    
    
    
    
    
    
    <a href="https://github.com/TickLabVN" target="_blank" rel="noopener" title="github.com" class="md-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
    
    
    
    
    
    
    
    
    <a href="https://www.facebook.com/ticklab.vn" target="_blank" rel="noopener" title="www.facebook.com" class="md-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M512 256C512 114.6 397.4 0 256 0S0 114.6 0 256c0 120 82.7 220.8 194.2 248.5V334.2h-52.8V256h52.8v-33.7c0-87.1 39.4-127.5 125-127.5 16.2 0 44.2 3.2 55.7 6.4V172c-6-.6-16.5-1-29.6-1-42 0-58.2 15.9-58.2 57.2V256h83.6l-14.4 78.2H287v175.9C413.8 494.8 512 386.9 512 256"/></svg>
    </a>
    
    
    
    
    
    
    
    
    <a href="https://www.youtube.com/@TickLab" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305m-317.51 213.508V175.185l142.739 81.205z"/></svg>
    </a>
    
</div>
        
      </div>
    </div>
  </footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.sections", "navigation.indexes", "navigation.top", "navigation.footer", "toc.follow", "content.code.copy", "content.code.annotate", "search.suggest", "search.highlight", "search.share"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    

      <script src="../../assets/javascripts/bundle.83f73b43.min.js"></script>
      
        <script src="../../javascripts/katex.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
      
    
<script src="../../assets/javascripts/custom.9c11c319.min.js"></script>

  </body>
</html>